{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.1 Data Processing and spliting dataset along with normaizign and standardizing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1143.000000       1143.000000  1143.000000     1143.000000   \n",
      "mean        8.311111          0.531339     0.268364        2.532152   \n",
      "std         1.747595          0.179633     0.196686        1.355917   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.392500     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.250000        2.200000   \n",
      "75%         9.100000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1143.000000          1143.000000           1143.000000  1143.000000   \n",
      "mean      0.086933            15.615486             45.914698     0.996730   \n",
      "std       0.047267            10.250486             32.782130     0.001925   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             21.000000     0.995570   \n",
      "50%       0.079000            13.000000             37.000000     0.996680   \n",
      "75%       0.090000            21.000000             61.000000     0.997845   \n",
      "max       0.611000            68.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality           Id  \n",
      "count  1143.000000  1143.000000  1143.000000  1143.000000  1143.000000  \n",
      "mean      3.311015     0.657708    10.442111     5.657043   804.969379  \n",
      "std       0.156664     0.170399     1.082196     0.805824   463.997116  \n",
      "min       2.740000     0.330000     8.400000     3.000000     0.000000  \n",
      "25%       3.205000     0.550000     9.500000     5.000000   411.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000   794.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  1209.500000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  1597.000000  \n",
      "                      fixed acidity  volatile acidity  citric acid  \\\n",
      "fixed acidity              1.000000         -0.250728     0.673157   \n",
      "volatile acidity          -0.250728          1.000000    -0.544187   \n",
      "citric acid                0.673157         -0.544187     1.000000   \n",
      "residual sugar             0.171831         -0.005751     0.175815   \n",
      "chlorides                  0.107889          0.056336     0.245312   \n",
      "free sulfur dioxide       -0.164831         -0.001962    -0.057589   \n",
      "total sulfur dioxide      -0.110628          0.077748     0.036871   \n",
      "density                    0.681501          0.016512     0.375243   \n",
      "pH                        -0.685163          0.221492    -0.546339   \n",
      "sulphates                  0.174592         -0.276079     0.331232   \n",
      "alcohol                   -0.075055         -0.203909     0.106250   \n",
      "quality                    0.121970         -0.407394     0.240821   \n",
      "Id                        -0.275826         -0.007892    -0.139011   \n",
      "\n",
      "                      residual sugar  chlorides  free sulfur dioxide  \\\n",
      "fixed acidity               0.171831   0.107889            -0.164831   \n",
      "volatile acidity           -0.005751   0.056336            -0.001962   \n",
      "citric acid                 0.175815   0.245312            -0.057589   \n",
      "residual sugar              1.000000   0.070863             0.165339   \n",
      "chlorides                   0.070863   1.000000             0.015280   \n",
      "free sulfur dioxide         0.165339   0.015280             1.000000   \n",
      "total sulfur dioxide        0.190790   0.048163             0.661093   \n",
      "density                     0.380147   0.208901            -0.054150   \n",
      "pH                         -0.116959  -0.277759             0.072804   \n",
      "sulphates                   0.017475   0.374784             0.034445   \n",
      "alcohol                     0.058421  -0.229917            -0.047095   \n",
      "quality                     0.022002  -0.124085            -0.063260   \n",
      "Id                         -0.046344  -0.088099             0.095268   \n",
      "\n",
      "                      total sulfur dioxide   density        pH  sulphates  \\\n",
      "fixed acidity                    -0.110628  0.681501 -0.685163   0.174592   \n",
      "volatile acidity                  0.077748  0.016512  0.221492  -0.276079   \n",
      "citric acid                       0.036871  0.375243 -0.546339   0.331232   \n",
      "residual sugar                    0.190790  0.380147 -0.116959   0.017475   \n",
      "chlorides                         0.048163  0.208901 -0.277759   0.374784   \n",
      "free sulfur dioxide               0.661093 -0.054150  0.072804   0.034445   \n",
      "total sulfur dioxide              1.000000  0.050175 -0.059126   0.026894   \n",
      "density                           0.050175  1.000000 -0.352775   0.143139   \n",
      "pH                               -0.059126 -0.352775  1.000000  -0.185499   \n",
      "sulphates                         0.026894  0.143139 -0.185499   1.000000   \n",
      "alcohol                          -0.188165 -0.494727  0.225322   0.094421   \n",
      "quality                          -0.183339 -0.175208 -0.052453   0.257710   \n",
      "Id                               -0.107389 -0.363926  0.132904  -0.103954   \n",
      "\n",
      "                       alcohol   quality        Id  \n",
      "fixed acidity        -0.075055  0.121970 -0.275826  \n",
      "volatile acidity     -0.203909 -0.407394 -0.007892  \n",
      "citric acid           0.106250  0.240821 -0.139011  \n",
      "residual sugar        0.058421  0.022002 -0.046344  \n",
      "chlorides            -0.229917 -0.124085 -0.088099  \n",
      "free sulfur dioxide  -0.047095 -0.063260  0.095268  \n",
      "total sulfur dioxide -0.188165 -0.183339 -0.107389  \n",
      "density              -0.494727 -0.175208 -0.363926  \n",
      "pH                    0.225322 -0.052453  0.132904  \n",
      "sulphates             0.094421  0.257710 -0.103954  \n",
      "alcohol               1.000000  0.484866  0.238087  \n",
      "quality               0.484866  1.000000  0.069708  \n",
      "Id                    0.238087  0.069708  1.000000  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('WineQT.csv')\n",
    "\n",
    "\n",
    "print(data.describe())\n",
    "\n",
    "cov_matrix = data.corr()\n",
    "print(cov_matrix)\n",
    "sns.heatmap(cov_matrix, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      quality\n",
      "0           2\n",
      "1           2\n",
      "2           2\n",
      "3           3\n",
      "4           2\n",
      "...       ...\n",
      "1138        3\n",
      "1139        3\n",
      "1140        2\n",
      "1141        3\n",
      "1142        2\n",
      "\n",
      "[1143 rows x 1 columns] [[ 7.4    0.7    0.    ...  3.51   0.56   9.4  ]\n",
      " [ 7.8    0.88   0.    ...  3.2    0.68   9.8  ]\n",
      " [ 7.8    0.76   0.04  ...  3.26   0.65   9.8  ]\n",
      " ...\n",
      " [ 6.2    0.6    0.08  ...  3.45   0.58  10.5  ]\n",
      " [ 5.9    0.55   0.1   ...  3.52   0.76  11.2  ]\n",
      " [ 5.9    0.645  0.12  ...  3.57   0.71  10.2  ]]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('WineQT.csv')\n",
    "\n",
    "labels = pd.DataFrame(data.quality-3)\n",
    "\n",
    "data = data.drop(columns=['quality', 'Id'], axis=1)\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "data = imputer.fit_transform(data)\n",
    "\n",
    "print(labels,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(data):\n",
    "    data = (data - data.min())/(data.max()-data.min())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    data = (data - data.mean())/data.std()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03987803 -0.44427449 -0.48652486 ... -0.2746694  -0.45272456\n",
      "   0.08083733]\n",
      " [-0.01573495 -0.4334101  -0.48652486 ... -0.29338028 -0.44548164\n",
      "   0.10498041]\n",
      " [-0.01573495 -0.44065302 -0.48411055 ... -0.28975882 -0.44729237\n",
      "   0.10498041]\n",
      " ...\n",
      " [-0.11230724 -0.45031025 -0.48169625 ... -0.27829086 -0.45151741\n",
      "   0.14723078]\n",
      " [-0.13041455 -0.45332814 -0.48048909 ... -0.27406583 -0.44065302\n",
      "   0.18948116]\n",
      " [-0.13041455 -0.44759416 -0.47928194 ... -0.27104794 -0.44367091\n",
      "   0.12912348]]\n"
     ]
    }
   ],
   "source": [
    "data = normalise(data) \n",
    "data = standardize(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(labels,data)\n",
    "X_train , X_test , y_train , y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality    2428\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Implementation of MLP Classification class`\n",
    "\n",
    "Built an MLP classifier class with the following specifications:\n",
    "1. Created a class where you can modify and access the learning rate, activa-\n",
    "tion function, optimisers, number of hidden layers and neurons.\n",
    "2. Implemented methods for forward propagation, backpropagation, and train-\n",
    "ing.\n",
    "3. Different activation functions introduce non-linearity to the model and\n",
    "affect the learning process. Implement the Sigmoid, Tanh, and ReLU\n",
    "activation functions and make them easily interchangeable within your\n",
    "MLP framework.\n",
    "4. Optimization techniques dictate how the neural network updates its weights\n",
    "during training. Implement methods for the Stochastic Gradient Descent\n",
    "(SGD), Batch Gradient Descent, and Mini-Batch Gradient Descent algo-\n",
    "rithms from scratch, ensuring that they can be employed within your MLP\n",
    "architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_layer_perceptron_gd():\n",
    "    def __init__(self,data, labels,  hiddenLayers=[3], OutputLayer=6, learningRate = 0.001, activation = \"sigmoid\"):\n",
    "        encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.labels = encoder.fit_transform(labels)\n",
    "        # self.labels = labels\n",
    "        self.data = data\n",
    "        self.action = activation\n",
    "        self.inputLayer = data.shape[1]\n",
    "        self.learningRate = learningRate\n",
    "        self.outputLayer = OutputLayer\n",
    "        self.hiddenLayer = hiddenLayers\n",
    "        print(self.inputLayer, self.outputLayer, self.hiddenLayer)\n",
    "        #required number of weight vector's = hiddenLayers.size()+1\n",
    "        self.we = [] \n",
    "        \n",
    "        W1 = np.random.randn(self.inputLayer, self.hiddenLayer[0])\n",
    "        self.we.append(W1)\n",
    "\n",
    "        self.numberOfHiddenLayers = len(hiddenLayers)\n",
    "\n",
    "        for i in range (1, self.numberOfHiddenLayers):\n",
    "            W1 = np.random.randn(self.hiddenLayer[i-1], self.hiddenLayer[i])\n",
    "            self.we.append(W1)\n",
    "\n",
    "        W2 = np.random.randn(self.hiddenLayer[-1], self.outputLayer)\n",
    "        self.we.append(W2)\n",
    "        print(len(self.we))\n",
    "        print(W1.shape,W2.shape)\n",
    "\n",
    "    def activation(self,z):\n",
    "        if(self.action == \"sigmoid\"):\n",
    "            return 1/(1+np.exp(-z))\n",
    "        if(self.action == \"tanh\"):\n",
    "            return np.tanh(z)\n",
    "        if(self.action == \"Relu\"):\n",
    "            # return z*(z>0)\n",
    "            return np.maximum(0,z)\n",
    "\n",
    "    def derivative(self,z):\n",
    "        if(self.action == \"sigmoid\"):\n",
    "            return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "        if(self.action == \"tanh\"):\n",
    "            return 1 - z**2\n",
    "        if(self.action == \"Relu\"):\n",
    "            # return 1*(z>0)\n",
    "            return np.where(z>0 , 1 , 0)\n",
    "    \n",
    "    \n",
    "    def soft_max_each(self,X):\n",
    "        return np.exp(X) / np.sum(np.exp(X))\n",
    "    \n",
    "    def soft_max(self, pre_score):\n",
    "   \n",
    "        for i in range(len(pre_score)):\n",
    "            pre_score[i] = self.soft_max_each(pre_score[i])\n",
    "        return pre_score\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #number z's and a's will be self.numberOfHiddenLayers+1\n",
    "        self.a = []\n",
    "        self.z = []\n",
    "        \n",
    "        self.z.append( np.dot(X, self.we[0]) )\n",
    "        self.a.append( self.activation(self.z[0] ))\n",
    "\n",
    "        for i in range(1, self.numberOfHiddenLayers+1):\n",
    "            self.z.append(  np.dot(self.a[i-1], self.we[i])  ) \n",
    "            self.a.append(  self.activation(self.z[i]) )\n",
    "\n",
    "        return self.soft_max(self.a[-1])\n",
    "    \n",
    "    def backward(self, data_batch, labels_batch):\n",
    "        djdw = []\n",
    "\n",
    "        # Forward pass\n",
    "        self.forward(data_batch)\n",
    "        self.a[-1] = self.soft_max(self.a[-1])\n",
    "        # Compute output layer error\n",
    "        delta = np.multiply(-(labels_batch - self.a[-1]), self.derivative(self.z[-1]))\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(self.numberOfHiddenLayers):\n",
    "            curr_idx = self.numberOfHiddenLayers - i - 1\n",
    "            jw = np.dot(self.a[curr_idx].T, delta)\n",
    "            djdw.insert(0, jw)\n",
    "            delta = np.dot(delta, self.we[curr_idx + 1].T) * self.derivative(self.z[curr_idx])\n",
    "\n",
    "        # Compute the gradient for the input layer\n",
    "        jw = np.dot(data_batch.T, delta)\n",
    "        djdw.insert(0, jw)\n",
    "\n",
    "        return djdw\n",
    "\n",
    "    def gradient_descent(self, djdw):\n",
    "        n = len(self.data)\n",
    "        for i in range(len(self.we)):\n",
    "            self.we[i] -= (self.learningRate) * djdw[i]\n",
    "\n",
    "    def CELoss(self):\n",
    "        #prediction set : self.a[-1]\n",
    "        #label's : self.labels\n",
    "        L = 0 # Define loss\n",
    "        for i in range(self.a[-1].shape[0]): # Iterate through X\n",
    "            L -= np.vdot( np.array(self.labels[i]),np.log(np.array(self.a[-1][i]))) # Cross Entropy Loss\n",
    "        return L\n",
    "        \n",
    "    def fit(self, batch_size=0, num_epochs=20000):\n",
    "        num_samples = len(self.data)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if batch_size == 0:  # Full Batch Gradient Descent\n",
    "                data_batch, labels_batch = self.data, self.labels\n",
    "                djdw = self.backward(data_batch, labels_batch)\n",
    "                self.gradient_descent(djdw)\n",
    "            else:  # Mini-Batch Gradient Descent or Stochastic Gradient Descent\n",
    "                indices = np.arange(num_samples)\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                for i in range(0, num_samples, batch_size):\n",
    "                    batch_indices = indices[i:i + batch_size]\n",
    "                    data_batch = self.data[batch_indices]\n",
    "                    labels_batch = self.labels[batch_indices]\n",
    "                    djdw = self.backward(data_batch, labels_batch)\n",
    "                    self.gradient_descent(djdw)\n",
    "            # print(self.a[-1].shape)      \n",
    "            error = self.CELoss()\n",
    "            wandb.log({\"Learning_Rate\": self.learningRate, \"batch_size\": batch_size})\n",
    "            wandb.log({\"CELoss\": error})\n",
    "            if(epoch%10 == 0):\n",
    "                print(\"Epoch {}: Training Loss = {}\".format(epoch, error))\n",
    "\n",
    "            # output_labels = self.forward(self.data)\n",
    "            # error = (((output_labels - self.labels) ** 2).sum())/2\n",
    "    def predict(self, X):\n",
    "\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # obj = Multi_layer_perceptron(data = X_train , labels=y_train, hiddenLayers=[3], learningRate=0.0001, activation='sigmoid', OutputLayer=6)\n",
    "# # # obj.forward(X_train)\n",
    "# # # print(obj.a[-1].shape)\n",
    "# # # print(obj.a[-1])\n",
    "# # # obj.backward()\n",
    "# # obj.fit()\n",
    "\n",
    "# obj = Multi_layer_perceptron_gd(data = X_train , labels=y_train, hiddenLayers=[5,6,7], learningRate=0.06, activation='sigmoid', OutputLayer=6)\n",
    "# obj.fit(num_epochs=100, batch_size=32)\n",
    "\n",
    "# # Assuming y_true contains the true labels and y_pred contains the predicted labels\n",
    "# # Make sure y_true and y_pred are of the same length\n",
    "# y_true =  y_test.values.T[0]\n",
    "# y_pred = obj.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_true, y_pred)\n",
    "# f1 = f1_score(y_true, y_pred, average='weighted')  # Use 'weighted' if you have multiple classes\n",
    "# precision = precision_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "# recall = recall_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(f\"F1 Score: {f1}\")\n",
    "# print(f\"Precision: {precision}\")\n",
    "# print(f\"Recall: {recall}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:4d6i26jj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr><tr><td>CELoss</td><td></td></tr><tr><td>F1 score</td><td></td></tr><tr><td>Learning_Rate</td><td></td></tr><tr><td>Precision</td><td></td></tr><tr><td>Recall</td><td></td></tr><tr><td>batch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.41921</td></tr><tr><td>CELoss</td><td>30.68195</td></tr><tr><td>F1 score</td><td>0.24766</td></tr><tr><td>Learning_Rate</td><td>0.005</td></tr><tr><td>Precision</td><td>0.17574</td></tr><tr><td>Recall</td><td>0.41921</td></tr><tr><td>batch_size</td><td>32</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">giddy-paper-4</strong> at: <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/4d6i26jj' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/4d6i26jj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231022_185638-4d6i26jj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:4d6i26jj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arya/Desktop/SAMI/Ass/Ass3/wandb/run-20231022_190011-v00csbn7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/v00csbn7' target=\"_blank\">bumbling-bush-5</a></strong> to <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/v00csbn7' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/v00csbn7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 1618.3273566984137\n",
      "Epoch 10: Training Loss = 1566.4233843656345\n",
      "Epoch 20: Training Loss = 1563.1690077063438\n",
      "Epoch 30: Training Loss = 1562.1480853935743\n",
      "Epoch 40: Training Loss = 1561.6459949441628\n",
      "Epoch 50: Training Loss = 1561.3466870659229\n",
      "Epoch 60: Training Loss = 1561.14778521245\n",
      "Epoch 70: Training Loss = 1561.0060222336456\n",
      "Epoch 80: Training Loss = 1560.8999307172774\n",
      "Epoch 90: Training Loss = 1560.8176379999427\n",
      "Epoch 100: Training Loss = 1560.7520398798142\n",
      "Epoch 110: Training Loss = 1560.6986232462275\n",
      "Epoch 120: Training Loss = 1560.6543829339691\n",
      "Epoch 130: Training Loss = 1560.6172405588572\n",
      "Epoch 140: Training Loss = 1560.5857132576468\n",
      "Epoch 150: Training Loss = 1560.5587152924502\n",
      "Epoch 160: Training Loss = 1560.5354342227618\n",
      "Epoch 170: Training Loss = 1560.5152508718472\n",
      "Epoch 180: Training Loss = 1560.4976860296736\n",
      "Epoch 190: Training Loss = 1560.4823640340476\n",
      "Epoch 200: Training Loss = 1560.4689873236114\n",
      "Epoch 210: Training Loss = 1560.4573183108175\n",
      "Epoch 220: Training Loss = 1560.4471662537712\n",
      "Epoch 230: Training Loss = 1560.4383776154568\n",
      "Epoch 240: Training Loss = 1560.4308289043347\n",
      "Epoch 250: Training Loss = 1560.424421314364\n",
      "Epoch 260: Training Loss = 1560.41907669412\n",
      "Epoch 270: Training Loss = 1560.414734516341\n",
      "Epoch 280: Training Loss = 1560.4113496153434\n",
      "Epoch 290: Training Loss = 1560.4088905264146\n",
      "Epoch 300: Training Loss = 1560.4073383079442\n",
      "Epoch 310: Training Loss = 1560.4066857602343\n",
      "Epoch 320: Training Loss = 1560.4069369783977\n",
      "Epoch 330: Training Loss = 1560.4081071930366\n",
      "Epoch 340: Training Loss = 1560.4102228631948\n",
      "Epoch 350: Training Loss = 1560.4133219912733\n",
      "Epoch 360: Training Loss = 1560.41745463015\n",
      "Epoch 370: Training Loss = 1560.4226835465615\n",
      "Epoch 380: Training Loss = 1560.4290849916629\n",
      "Epoch 390: Training Loss = 1560.4367495060706\n",
      "Epoch 400: Training Loss = 1560.4457826505832\n",
      "Epoch 410: Training Loss = 1560.456305501009\n",
      "Epoch 420: Training Loss = 1560.4684546730812\n",
      "Epoch 430: Training Loss = 1560.4823815501604\n",
      "Epoch 440: Training Loss = 1560.4982502749078\n",
      "Epoch 450: Training Loss = 1560.5162339503224\n",
      "Epoch 460: Training Loss = 1560.5365084037676\n",
      "Epoch 470: Training Loss = 1560.5592428538923\n",
      "Epoch 480: Training Loss = 1560.584586965069\n",
      "Epoch 490: Training Loss = 1560.6126541777994\n",
      "Epoch 500: Training Loss = 1560.6435019524406\n",
      "Epoch 510: Training Loss = 1560.677110662306\n",
      "Epoch 520: Training Loss = 1560.7133641509288\n",
      "Epoch 530: Training Loss = 1560.7520360156375\n",
      "Epoch 540: Training Loss = 1560.7927858946146\n",
      "Epoch 550: Training Loss = 1560.8351688727043\n",
      "Epoch 560: Training Loss = 1560.8786584931713\n",
      "Epoch 570: Training Loss = 1560.9226804302552\n",
      "Epoch 580: Training Loss = 1560.9666509315457\n",
      "Epoch 590: Training Loss = 1561.010012966157\n",
      "Epoch 600: Training Loss = 1561.0522641330417\n",
      "Epoch 610: Training Loss = 1561.092973217724\n",
      "Epoch 620: Training Loss = 1561.1317855456514\n",
      "Epoch 630: Training Loss = 1561.1684197238128\n",
      "Epoch 640: Training Loss = 1561.2026593781864\n",
      "Epoch 650: Training Loss = 1561.2343431984202\n",
      "Epoch 660: Training Loss = 1561.263355544001\n",
      "Epoch 670: Training Loss = 1561.2896186558137\n",
      "Epoch 680: Training Loss = 1561.3130865725634\n",
      "Epoch 690: Training Loss = 1561.3337403367937\n",
      "Epoch 700: Training Loss = 1561.3515839530928\n",
      "Epoch 710: Training Loss = 1561.3666406919888\n",
      "Epoch 720: Training Loss = 1561.378949560068\n",
      "Epoch 730: Training Loss = 1561.388561961716\n",
      "Epoch 740: Training Loss = 1561.3955386992545\n",
      "Epoch 750: Training Loss = 1561.3999474868106\n",
      "Epoch 760: Training Loss = 1561.4018611119504\n",
      "Epoch 770: Training Loss = 1561.4013563017907\n",
      "Epoch 780: Training Loss = 1561.3985132679743\n",
      "Epoch 790: Training Loss = 1561.3934158355703\n",
      "Epoch 800: Training Loss = 1561.3861520121163\n",
      "Epoch 810: Training Loss = 1561.3768148228767\n",
      "Epoch 820: Training Loss = 1561.3655032223082\n",
      "Epoch 830: Training Loss = 1561.3523228861302\n",
      "Epoch 840: Training Loss = 1561.337386694049\n",
      "Epoch 850: Training Loss = 1561.3208147342834\n",
      "Epoch 860: Training Loss = 1561.3027337020235\n",
      "Epoch 870: Training Loss = 1561.283275621481\n",
      "Epoch 880: Training Loss = 1561.2625758769861\n",
      "Epoch 890: Training Loss = 1561.2407705574753\n",
      "Epoch 900: Training Loss = 1561.2179930598209\n",
      "Epoch 910: Training Loss = 1561.1943697361835\n",
      "Epoch 920: Training Loss = 1561.1700141308315\n",
      "Epoch 930: Training Loss = 1561.1450191112529\n",
      "Epoch 940: Training Loss = 1561.119446073147\n",
      "Epoch 950: Training Loss = 1561.0933105205936\n",
      "Epoch 960: Training Loss = 1561.0665638824148\n",
      "Epoch 970: Training Loss = 1561.0390728852237\n",
      "Epoch 980: Training Loss = 1561.0106011992634\n",
      "Epoch 990: Training Loss = 1560.9808046748094\n",
      "Epoch 1000: Training Loss = 1560.949259994742\n",
      "Epoch 1010: Training Loss = 1560.9155473502485\n",
      "Epoch 1020: Training Loss = 1560.8793832820231\n",
      "Epoch 1030: Training Loss = 1560.8407494095375\n",
      "Epoch 1040: Training Loss = 1560.7999335358868\n",
      "Epoch 1050: Training Loss = 1560.7574463791009\n",
      "Epoch 1060: Training Loss = 1560.7138670820395\n",
      "Epoch 1070: Training Loss = 1560.6697078204018\n",
      "Epoch 1080: Training Loss = 1560.6253477191954\n",
      "Epoch 1090: Training Loss = 1560.5810300955097\n",
      "Epoch 1100: Training Loss = 1560.5368925850396\n",
      "Epoch 1110: Training Loss = 1560.493004062839\n",
      "Epoch 1120: Training Loss = 1560.4493951475918\n",
      "Epoch 1130: Training Loss = 1560.4060788314175\n",
      "Epoch 1140: Training Loss = 1560.363062465496\n",
      "Epoch 1150: Training Loss = 1560.3203536658332\n",
      "Epoch 1160: Training Loss = 1560.27796250909\n",
      "Epoch 1170: Training Loss = 1560.2359017422648\n",
      "Epoch 1180: Training Loss = 1560.1941860998231\n",
      "Epoch 1190: Training Loss = 1560.1528313502865\n",
      "Epoch 1200: Training Loss = 1560.111853385334\n",
      "Epoch 1210: Training Loss = 1560.071267481265\n",
      "Epoch 1220: Training Loss = 1560.0310877636462\n",
      "Epoch 1230: Training Loss = 1559.9913268584544\n",
      "Epoch 1240: Training Loss = 1559.9519956952154\n",
      "Epoch 1250: Training Loss = 1559.9131034246823\n",
      "Epoch 1260: Training Loss = 1559.8746574176569\n",
      "Epoch 1270: Training Loss = 1559.836663318208\n",
      "Epoch 1280: Training Loss = 1559.7991251308601\n",
      "Epoch 1290: Training Loss = 1559.7620453273419\n",
      "Epoch 1300: Training Loss = 1559.7254249627395\n",
      "Epoch 1310: Training Loss = 1559.6892637945107\n",
      "Epoch 1320: Training Loss = 1559.6535604002072\n",
      "Epoch 1330: Training Loss = 1559.6183122914304\n",
      "Epoch 1340: Training Loss = 1559.5835160227043\n",
      "Epoch 1350: Training Loss = 1559.5491672945807\n",
      "Epoch 1360: Training Loss = 1559.5152610508196\n",
      "Epoch 1370: Training Loss = 1559.4817915695903\n",
      "Epoch 1380: Training Loss = 1559.4487525488157\n",
      "Epoch 1390: Training Loss = 1559.4161371857604\n",
      "Epoch 1400: Training Loss = 1559.3839382510141\n",
      "Epoch 1410: Training Loss = 1559.3521481569237\n",
      "Epoch 1420: Training Loss = 1559.3207590206111\n",
      "Epoch 1430: Training Loss = 1559.2897627216364\n",
      "Epoch 1440: Training Loss = 1559.2591509544695\n",
      "Epoch 1450: Training Loss = 1559.2289152758801\n",
      "Epoch 1460: Training Loss = 1559.1990471475322\n",
      "Epoch 1470: Training Loss = 1559.169537974003\n",
      "Epoch 1480: Training Loss = 1559.1403791365954\n",
      "Epoch 1490: Training Loss = 1559.111562023359\n",
      "Epoch 1500: Training Loss = 1559.083078055774\n",
      "Epoch 1510: Training Loss = 1559.0549187125287\n",
      "Epoch 1520: Training Loss = 1559.027075551014\n",
      "Epoch 1530: Training Loss = 1558.9995402269287\n",
      "Epoch 1540: Training Loss = 1558.972304512584\n",
      "Epoch 1550: Training Loss = 1558.9453603143147\n",
      "Epoch 1560: Training Loss = 1558.918699689423\n",
      "Epoch 1570: Training Loss = 1558.892314863023\n",
      "Epoch 1580: Training Loss = 1558.8661982449905\n",
      "Epoch 1590: Training Loss = 1558.8403424472292\n",
      "Epoch 1600: Training Loss = 1558.8147403012701\n",
      "Epoch 1610: Training Loss = 1558.7893848761387\n",
      "Epoch 1620: Training Loss = 1558.7642694962615\n",
      "Epoch 1630: Training Loss = 1558.7393877590105\n",
      "Epoch 1640: Training Loss = 1558.7147335514153\n",
      "Epoch 1650: Training Loss = 1558.6903010652852\n",
      "Epoch 1660: Training Loss = 1558.6660848099377\n",
      "Epoch 1670: Training Loss = 1558.642079621542\n",
      "Epoch 1680: Training Loss = 1558.6182806680488\n",
      "Epoch 1690: Training Loss = 1558.5946834485862\n",
      "Epoch 1700: Training Loss = 1558.5712837863462\n",
      "Epoch 1710: Training Loss = 1558.548077813956\n",
      "Epoch 1720: Training Loss = 1558.5250619507347\n",
      "Epoch 1730: Training Loss = 1558.5022328714492\n",
      "Epoch 1740: Training Loss = 1558.4795874665433\n",
      "Epoch 1750: Training Loss = 1558.457122794377\n",
      "Epoch 1760: Training Loss = 1558.4348360263932\n",
      "Epoch 1770: Training Loss = 1558.4127243866155\n",
      "Epoch 1780: Training Loss = 1558.390785087315\n",
      "Epoch 1790: Training Loss = 1558.3690152629981\n",
      "Epoch 1800: Training Loss = 1558.3474119050675\n",
      "Epoch 1810: Training Loss = 1558.3259717996068\n",
      "Epoch 1820: Training Loss = 1558.3046914706829\n",
      "Epoch 1830: Training Loss = 1558.2835671313826\n",
      "Epoch 1840: Training Loss = 1558.2625946444464\n",
      "Epoch 1850: Training Loss = 1558.2417694941191\n",
      "Epoch 1860: Training Loss = 1558.221086770243\n",
      "Epoch 1870: Training Loss = 1558.2005411652535\n",
      "Epoch 1880: Training Loss = 1558.1801269843086\n",
      "Epoch 1890: Training Loss = 1558.1598381683918\n",
      "Epoch 1900: Training Loss = 1558.1396683298105\n",
      "Epoch 1910: Training Loss = 1558.1196107993615\n",
      "Epoch 1920: Training Loss = 1558.0996586841516\n",
      "Epoch 1930: Training Loss = 1558.0798049349771\n",
      "Epoch 1940: Training Loss = 1558.0600424220852\n",
      "Epoch 1950: Training Loss = 1558.0403640179995\n",
      "Epoch 1960: Training Loss = 1558.0207626861777\n",
      "Epoch 1970: Training Loss = 1558.00123157403\n",
      "Epoch 1980: Training Loss = 1557.9817641089098\n",
      "Epoch 1990: Training Loss = 1557.9623540952423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 32.16402781261048\n",
      "Epoch 10: Training Loss = 30.920485579198534\n",
      "Epoch 20: Training Loss = 30.799149435572897\n",
      "Epoch 30: Training Loss = 30.77657604475292\n",
      "Epoch 40: Training Loss = 30.752895367999415\n",
      "Epoch 50: Training Loss = 30.756491930331496\n",
      "Epoch 60: Training Loss = 30.7188607590752\n",
      "Epoch 70: Training Loss = 30.706715469358635\n",
      "Epoch 80: Training Loss = 30.71559184472193\n",
      "Epoch 90: Training Loss = 30.701086449660195\n",
      "Epoch 100: Training Loss = 30.703771431259458\n",
      "Epoch 110: Training Loss = 30.705430828802065\n",
      "Epoch 120: Training Loss = 30.69489067626112\n",
      "Epoch 130: Training Loss = 30.68978303999126\n",
      "Epoch 140: Training Loss = 30.703512932795537\n",
      "Epoch 150: Training Loss = 30.690797188424856\n",
      "Epoch 160: Training Loss = 30.694048943736743\n",
      "Epoch 170: Training Loss = 30.69010748557584\n",
      "Epoch 180: Training Loss = 30.69060223033717\n",
      "Epoch 190: Training Loss = 30.69508813761336\n",
      "Epoch 200: Training Loss = 30.690638801781365\n",
      "Epoch 210: Training Loss = 30.691119031601197\n",
      "Epoch 220: Training Loss = 30.690984688258858\n",
      "Epoch 230: Training Loss = 30.69824196231444\n",
      "Epoch 240: Training Loss = 30.683420544197517\n",
      "Epoch 250: Training Loss = 30.692126572233555\n",
      "Epoch 260: Training Loss = 30.693262410076553\n",
      "Epoch 270: Training Loss = 30.6978557298372\n",
      "Epoch 280: Training Loss = 30.686083915407117\n",
      "Epoch 290: Training Loss = 30.691528360852846\n",
      "Epoch 300: Training Loss = 30.691795391739575\n",
      "Epoch 310: Training Loss = 30.685385729950013\n",
      "Epoch 320: Training Loss = 30.69054551261938\n",
      "Epoch 330: Training Loss = 30.689707345252113\n",
      "Epoch 340: Training Loss = 30.690948127477704\n",
      "Epoch 350: Training Loss = 30.68727961150295\n",
      "Epoch 360: Training Loss = 30.688785979397764\n",
      "Epoch 370: Training Loss = 30.689345994615987\n",
      "Epoch 380: Training Loss = 30.68910543710032\n",
      "Epoch 390: Training Loss = 30.685492554303465\n",
      "Epoch 400: Training Loss = 30.686512924929552\n",
      "Epoch 410: Training Loss = 30.682575244134885\n",
      "Epoch 420: Training Loss = 30.684882288541722\n",
      "Epoch 430: Training Loss = 30.68672013804341\n",
      "Epoch 440: Training Loss = 30.682351098364816\n",
      "Epoch 450: Training Loss = 30.686904592442257\n",
      "Epoch 460: Training Loss = 30.686507555978796\n",
      "Epoch 470: Training Loss = 30.68228793115817\n",
      "Epoch 480: Training Loss = 30.68403218142267\n",
      "Epoch 490: Training Loss = 30.685955564492062\n",
      "Epoch 500: Training Loss = 30.686918002450078\n",
      "Epoch 510: Training Loss = 30.685614802179472\n",
      "Epoch 520: Training Loss = 30.687083765909414\n",
      "Epoch 530: Training Loss = 30.685945998470682\n",
      "Epoch 540: Training Loss = 30.68911231291449\n",
      "Epoch 550: Training Loss = 30.687874627356504\n",
      "Epoch 560: Training Loss = 30.68528544382292\n",
      "Epoch 570: Training Loss = 30.684661593355838\n",
      "Epoch 580: Training Loss = 30.684314042592884\n",
      "Epoch 590: Training Loss = 30.68626333241086\n",
      "Epoch 600: Training Loss = 30.684666155424203\n",
      "Epoch 610: Training Loss = 30.686509687709716\n",
      "Epoch 620: Training Loss = 30.686325518249575\n",
      "Epoch 630: Training Loss = 30.68522715626316\n",
      "Epoch 640: Training Loss = 30.6834899576403\n",
      "Epoch 650: Training Loss = 30.68350777622795\n",
      "Epoch 660: Training Loss = 30.686326714314678\n",
      "Epoch 670: Training Loss = 30.684081338497307\n",
      "Epoch 680: Training Loss = 30.683604128767616\n",
      "Epoch 690: Training Loss = 30.683854367244585\n",
      "Epoch 700: Training Loss = 30.684749080666258\n",
      "Epoch 710: Training Loss = 30.68134541092141\n",
      "Epoch 720: Training Loss = 30.68341566845942\n",
      "Epoch 730: Training Loss = 30.684339223348257\n",
      "Epoch 740: Training Loss = 30.6852536747008\n",
      "Epoch 750: Training Loss = 30.68749214488019\n",
      "Epoch 760: Training Loss = 30.685968680928625\n",
      "Epoch 770: Training Loss = 30.684997971696166\n",
      "Epoch 780: Training Loss = 30.683760556745764\n",
      "Epoch 790: Training Loss = 30.68313403521752\n",
      "Epoch 800: Training Loss = 30.683776749375717\n",
      "Epoch 810: Training Loss = 30.68227714985005\n",
      "Epoch 820: Training Loss = 30.682709062706703\n",
      "Epoch 830: Training Loss = 30.682879326254323\n",
      "Epoch 840: Training Loss = 30.68143198341863\n",
      "Epoch 850: Training Loss = 30.682398107570055\n",
      "Epoch 860: Training Loss = 30.68323705829282\n",
      "Epoch 870: Training Loss = 30.682630192424313\n",
      "Epoch 880: Training Loss = 30.68250914670767\n",
      "Epoch 890: Training Loss = 30.68097792422787\n",
      "Epoch 900: Training Loss = 30.685846182554613\n",
      "Epoch 910: Training Loss = 30.68434229447346\n",
      "Epoch 920: Training Loss = 30.683552623159986\n",
      "Epoch 930: Training Loss = 30.68352621484539\n",
      "Epoch 940: Training Loss = 30.682032473665544\n",
      "Epoch 950: Training Loss = 30.682137687142614\n",
      "Epoch 960: Training Loss = 30.68128298522352\n",
      "Epoch 970: Training Loss = 30.683427164832565\n",
      "Epoch 980: Training Loss = 30.68410620267335\n",
      "Epoch 990: Training Loss = 30.68256649002193\n",
      "Epoch 1000: Training Loss = 30.682899523250995\n",
      "Epoch 1010: Training Loss = 30.68389320999138\n",
      "Epoch 1020: Training Loss = 30.683511967580205\n",
      "Epoch 1030: Training Loss = 30.684458353813643\n",
      "Epoch 1040: Training Loss = 30.6813288529119\n",
      "Epoch 1050: Training Loss = 30.682874991352346\n",
      "Epoch 1060: Training Loss = 30.683966875679086\n",
      "Epoch 1070: Training Loss = 30.68313892856142\n",
      "Epoch 1080: Training Loss = 30.683829458880254\n",
      "Epoch 1090: Training Loss = 30.682956804835122\n",
      "Epoch 1100: Training Loss = 30.682368409827536\n",
      "Epoch 1110: Training Loss = 30.685050206204323\n",
      "Epoch 1120: Training Loss = 30.682452295035258\n",
      "Epoch 1130: Training Loss = 30.683095951259908\n",
      "Epoch 1140: Training Loss = 30.68338675105209\n",
      "Epoch 1150: Training Loss = 30.68324141364014\n",
      "Epoch 1160: Training Loss = 30.683723209591655\n",
      "Epoch 1170: Training Loss = 30.6823754525508\n",
      "Epoch 1180: Training Loss = 30.682608104391733\n",
      "Epoch 1190: Training Loss = 30.68063878241101\n",
      "Epoch 1200: Training Loss = 30.681973436867885\n",
      "Epoch 1210: Training Loss = 30.682377798616578\n",
      "Epoch 1220: Training Loss = 30.682817253790244\n",
      "Epoch 1230: Training Loss = 30.682086058746574\n",
      "Epoch 1240: Training Loss = 30.68297283618116\n",
      "Epoch 1250: Training Loss = 30.684690773332438\n",
      "Epoch 1260: Training Loss = 30.682994594263736\n",
      "Epoch 1270: Training Loss = 30.68331331861024\n",
      "Epoch 1280: Training Loss = 30.681182078930853\n",
      "Epoch 1290: Training Loss = 30.68181943994994\n",
      "Epoch 1300: Training Loss = 30.68328513417546\n",
      "Epoch 1310: Training Loss = 30.68312598244596\n",
      "Epoch 1320: Training Loss = 30.682416492325927\n",
      "Epoch 1330: Training Loss = 30.6834078592147\n",
      "Epoch 1340: Training Loss = 30.683216271542804\n",
      "Epoch 1350: Training Loss = 30.681929404896703\n",
      "Epoch 1360: Training Loss = 30.68146424741481\n",
      "Epoch 1370: Training Loss = 30.683471216679404\n",
      "Epoch 1380: Training Loss = 30.681811830719234\n",
      "Epoch 1390: Training Loss = 30.682190349669177\n",
      "Epoch 1400: Training Loss = 30.683700528547924\n",
      "Epoch 1410: Training Loss = 30.681665291559533\n",
      "Epoch 1420: Training Loss = 30.68311551692275\n",
      "Epoch 1430: Training Loss = 30.681568109863953\n",
      "Epoch 1440: Training Loss = 30.682518710763972\n",
      "Epoch 1450: Training Loss = 30.682321842636473\n",
      "Epoch 1460: Training Loss = 30.682566202305747\n",
      "Epoch 1470: Training Loss = 30.683065332367118\n",
      "Epoch 1480: Training Loss = 30.68219478337907\n",
      "Epoch 1490: Training Loss = 30.682408388815364\n",
      "Epoch 1500: Training Loss = 30.68220279012405\n",
      "Epoch 1510: Training Loss = 30.682798271181685\n",
      "Epoch 1520: Training Loss = 30.682273538158274\n",
      "Epoch 1530: Training Loss = 30.682803820135906\n",
      "Epoch 1540: Training Loss = 30.682160661704454\n",
      "Epoch 1550: Training Loss = 30.683343292833634\n",
      "Epoch 1560: Training Loss = 30.681847888092864\n",
      "Epoch 1570: Training Loss = 30.682878101340755\n",
      "Epoch 1580: Training Loss = 30.683506815187155\n",
      "Epoch 1590: Training Loss = 30.681833432905357\n",
      "Epoch 1600: Training Loss = 30.681512991124524\n",
      "Epoch 1610: Training Loss = 30.68165041340537\n",
      "Epoch 1620: Training Loss = 30.68178902925024\n",
      "Epoch 1630: Training Loss = 30.68232458393874\n",
      "Epoch 1640: Training Loss = 30.681677376345156\n",
      "Epoch 1650: Training Loss = 30.681282795898078\n",
      "Epoch 1660: Training Loss = 30.68333465675094\n",
      "Epoch 1670: Training Loss = 30.68279716684756\n",
      "Epoch 1680: Training Loss = 30.683495912465368\n",
      "Epoch 1690: Training Loss = 30.680551927932086\n",
      "Epoch 1700: Training Loss = 30.682530274630736\n",
      "Epoch 1710: Training Loss = 30.682601579885798\n",
      "Epoch 1720: Training Loss = 30.680690606299887\n",
      "Epoch 1730: Training Loss = 30.680457109357945\n",
      "Epoch 1740: Training Loss = 30.68192710377937\n",
      "Epoch 1750: Training Loss = 30.68158742318454\n",
      "Epoch 1760: Training Loss = 30.68208493519558\n",
      "Epoch 1770: Training Loss = 30.681835383783977\n",
      "Epoch 1780: Training Loss = 30.681744822649545\n",
      "Epoch 1790: Training Loss = 30.68185685067683\n",
      "Epoch 1800: Training Loss = 30.681282807800436\n",
      "Epoch 1810: Training Loss = 30.683153487704026\n",
      "Epoch 1820: Training Loss = 30.68365213997233\n",
      "Epoch 1830: Training Loss = 30.681020648771504\n",
      "Epoch 1840: Training Loss = 30.681083887466094\n",
      "Epoch 1850: Training Loss = 30.68158610216222\n",
      "Epoch 1860: Training Loss = 30.682472739050137\n",
      "Epoch 1870: Training Loss = 30.683705068494945\n",
      "Epoch 1880: Training Loss = 30.682769299708443\n",
      "Epoch 1890: Training Loss = 30.681589240404158\n",
      "Epoch 1900: Training Loss = 30.68137578134108\n",
      "Epoch 1910: Training Loss = 30.68202257114529\n",
      "Epoch 1920: Training Loss = 30.68261487366506\n",
      "Epoch 1930: Training Loss = 30.682026471932566\n",
      "Epoch 1940: Training Loss = 30.681530702332772\n",
      "Epoch 1950: Training Loss = 30.681663379605627\n",
      "Epoch 1960: Training Loss = 30.681982458845507\n",
      "Epoch 1970: Training Loss = 30.68066892871653\n",
      "Epoch 1980: Training Loss = 30.682555702928383\n",
      "Epoch 1990: Training Loss = 30.682497344046183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 25.00304677702638\n",
      "Epoch 10: Training Loss = 24.09695502947397\n",
      "Epoch 20: Training Loss = 24.03032166189511\n",
      "Epoch 30: Training Loss = 24.019413590967037\n",
      "Epoch 40: Training Loss = 24.01286025241284\n",
      "Epoch 50: Training Loss = 24.01164350273335\n",
      "Epoch 60: Training Loss = 23.99891053586056\n",
      "Epoch 70: Training Loss = 24.000404694097703\n",
      "Epoch 80: Training Loss = 23.99958931797317\n",
      "Epoch 90: Training Loss = 23.995176755944595\n",
      "Epoch 100: Training Loss = 23.989925839864565\n",
      "Epoch 110: Training Loss = 23.988558816340415\n",
      "Epoch 120: Training Loss = 23.99035672318988\n",
      "Epoch 130: Training Loss = 23.992123326455665\n",
      "Epoch 140: Training Loss = 23.99243259304601\n",
      "Epoch 150: Training Loss = 23.991226729826497\n",
      "Epoch 160: Training Loss = 23.988015006242343\n",
      "Epoch 170: Training Loss = 23.98946358009333\n",
      "Epoch 180: Training Loss = 23.990785122509266\n",
      "Epoch 190: Training Loss = 23.989583873779527\n",
      "Epoch 200: Training Loss = 23.988217411985996\n",
      "Epoch 210: Training Loss = 23.988356819065125\n",
      "Epoch 220: Training Loss = 23.986704431195864\n",
      "Epoch 230: Training Loss = 23.98855108388036\n",
      "Epoch 240: Training Loss = 23.988214097612737\n",
      "Epoch 250: Training Loss = 23.98797695438918\n",
      "Epoch 260: Training Loss = 23.987563458011042\n",
      "Epoch 270: Training Loss = 23.987958275637226\n",
      "Epoch 280: Training Loss = 23.98710555020653\n",
      "Epoch 290: Training Loss = 23.98610056956631\n",
      "Epoch 300: Training Loss = 23.984957478387397\n",
      "Epoch 310: Training Loss = 23.986544713984223\n",
      "Epoch 320: Training Loss = 23.987759499044998\n",
      "Epoch 330: Training Loss = 23.98682958596622\n",
      "Epoch 340: Training Loss = 23.98638550791285\n",
      "Epoch 350: Training Loss = 23.987844733425433\n",
      "Epoch 360: Training Loss = 23.986336016029313\n",
      "Epoch 370: Training Loss = 23.987647683165328\n",
      "Epoch 380: Training Loss = 23.98647134890133\n",
      "Epoch 390: Training Loss = 23.98705113700415\n",
      "Epoch 400: Training Loss = 23.986155706489733\n",
      "Epoch 410: Training Loss = 23.98757569065543\n",
      "Epoch 420: Training Loss = 23.98624273323388\n",
      "Epoch 430: Training Loss = 23.98610561992322\n",
      "Epoch 440: Training Loss = 23.985736252195807\n",
      "Epoch 450: Training Loss = 23.985811826962735\n",
      "Epoch 460: Training Loss = 23.98480656990823\n",
      "Epoch 470: Training Loss = 23.98483443818652\n",
      "Epoch 480: Training Loss = 23.98613750808222\n",
      "Epoch 490: Training Loss = 23.986862090043672\n",
      "Epoch 500: Training Loss = 23.98547781169446\n",
      "Epoch 510: Training Loss = 23.98630527083588\n",
      "Epoch 520: Training Loss = 23.985896533744654\n",
      "Epoch 530: Training Loss = 23.984695088260317\n",
      "Epoch 540: Training Loss = 23.985160692390128\n",
      "Epoch 550: Training Loss = 23.986740170185517\n",
      "Epoch 560: Training Loss = 23.985823194153106\n",
      "Epoch 570: Training Loss = 23.985834622458665\n",
      "Epoch 580: Training Loss = 23.98545895578751\n",
      "Epoch 590: Training Loss = 23.986077501541583\n",
      "Epoch 600: Training Loss = 23.985583934218695\n",
      "Epoch 610: Training Loss = 23.985748066558198\n",
      "Epoch 620: Training Loss = 23.985641877474748\n",
      "Epoch 630: Training Loss = 23.984622030787232\n",
      "Epoch 640: Training Loss = 23.985216271184655\n",
      "Epoch 650: Training Loss = 23.985932759372123\n",
      "Epoch 660: Training Loss = 23.985631598478523\n",
      "Epoch 670: Training Loss = 23.98489077423868\n",
      "Epoch 680: Training Loss = 23.98491479880544\n",
      "Epoch 690: Training Loss = 23.9851822987918\n",
      "Epoch 700: Training Loss = 23.98537341599368\n",
      "Epoch 710: Training Loss = 23.985289728955877\n",
      "Epoch 720: Training Loss = 23.985897664904105\n",
      "Epoch 730: Training Loss = 23.985833397366555\n",
      "Epoch 740: Training Loss = 23.984287805090325\n",
      "Epoch 750: Training Loss = 23.9853722906207\n",
      "Epoch 760: Training Loss = 23.985168901135992\n",
      "Epoch 770: Training Loss = 23.985586969729034\n",
      "Epoch 780: Training Loss = 23.98534888572355\n",
      "Epoch 790: Training Loss = 23.98544277624942\n",
      "Epoch 800: Training Loss = 23.984340031393423\n",
      "Epoch 810: Training Loss = 23.984891677144635\n",
      "Epoch 820: Training Loss = 23.986001804476206\n",
      "Epoch 830: Training Loss = 23.985029010044485\n",
      "Epoch 840: Training Loss = 23.98497854262206\n",
      "Epoch 850: Training Loss = 23.984980360206368\n",
      "Epoch 860: Training Loss = 23.98538375801174\n",
      "Epoch 870: Training Loss = 23.985341995547586\n",
      "Epoch 880: Training Loss = 23.98534563270066\n",
      "Epoch 890: Training Loss = 23.984816858451754\n",
      "Epoch 900: Training Loss = 23.98492725642688\n",
      "Epoch 910: Training Loss = 23.985321491185786\n",
      "Epoch 920: Training Loss = 23.984651380464488\n",
      "Epoch 930: Training Loss = 23.985107452942334\n",
      "Epoch 940: Training Loss = 23.984620145785044\n",
      "Epoch 950: Training Loss = 23.985124121966198\n",
      "Epoch 960: Training Loss = 23.985602146449892\n",
      "Epoch 970: Training Loss = 23.985258758406463\n",
      "Epoch 980: Training Loss = 23.985281303122356\n",
      "Epoch 990: Training Loss = 23.985460669598396\n",
      "Epoch 1000: Training Loss = 23.984994108451023\n",
      "Epoch 1010: Training Loss = 23.984806114696575\n",
      "Epoch 1020: Training Loss = 23.984766650832448\n",
      "Epoch 1030: Training Loss = 23.984952613313336\n",
      "Epoch 1040: Training Loss = 23.985243331221863\n",
      "Epoch 1050: Training Loss = 23.984945101493615\n",
      "Epoch 1060: Training Loss = 23.984720098064095\n",
      "Epoch 1070: Training Loss = 23.984959157953586\n",
      "Epoch 1080: Training Loss = 23.985018303536897\n",
      "Epoch 1090: Training Loss = 23.98490422702307\n",
      "Epoch 1100: Training Loss = 23.985188436308746\n",
      "Epoch 1110: Training Loss = 23.985197027720332\n",
      "Epoch 1120: Training Loss = 23.984819117669876\n",
      "Epoch 1130: Training Loss = 23.984957561243064\n",
      "Epoch 1140: Training Loss = 23.985603832733972\n",
      "Epoch 1150: Training Loss = 23.98462863766931\n",
      "Epoch 1160: Training Loss = 23.98475875783692\n",
      "Epoch 1170: Training Loss = 23.984794273983304\n",
      "Epoch 1180: Training Loss = 23.985323828224562\n",
      "Epoch 1190: Training Loss = 23.984736313876507\n",
      "Epoch 1200: Training Loss = 23.984372642110692\n",
      "Epoch 1210: Training Loss = 23.984584180973204\n",
      "Epoch 1220: Training Loss = 23.984776418223174\n",
      "Epoch 1230: Training Loss = 23.98402811403807\n",
      "Epoch 1240: Training Loss = 23.98471755169181\n",
      "Epoch 1250: Training Loss = 23.984559283550727\n",
      "Epoch 1260: Training Loss = 23.9850601247878\n",
      "Epoch 1270: Training Loss = 23.98491546799648\n",
      "Epoch 1280: Training Loss = 23.984862564315176\n",
      "Epoch 1290: Training Loss = 23.984108786380947\n",
      "Epoch 1300: Training Loss = 23.98517503061056\n",
      "Epoch 1310: Training Loss = 23.984987588520468\n",
      "Epoch 1320: Training Loss = 23.984488826195268\n",
      "Epoch 1330: Training Loss = 23.98456480436703\n",
      "Epoch 1340: Training Loss = 23.983959236687575\n",
      "Epoch 1350: Training Loss = 23.985088740121626\n",
      "Epoch 1360: Training Loss = 23.984748613577995\n",
      "Epoch 1370: Training Loss = 23.984774282809962\n",
      "Epoch 1380: Training Loss = 23.984729429194477\n",
      "Epoch 1390: Training Loss = 23.984760637029463\n",
      "Epoch 1400: Training Loss = 23.984238934433897\n",
      "Epoch 1410: Training Loss = 23.98474031285378\n",
      "Epoch 1420: Training Loss = 23.984445760907278\n",
      "Epoch 1430: Training Loss = 23.984321521124542\n",
      "Epoch 1440: Training Loss = 23.984544404788\n",
      "Epoch 1450: Training Loss = 23.984750145177088\n",
      "Epoch 1460: Training Loss = 23.9846160265516\n",
      "Epoch 1470: Training Loss = 23.984668205476268\n",
      "Epoch 1480: Training Loss = 23.984754257104626\n",
      "Epoch 1490: Training Loss = 23.984823159209967\n",
      "Epoch 1500: Training Loss = 23.984664391990417\n",
      "Epoch 1510: Training Loss = 23.984601372270593\n",
      "Epoch 1520: Training Loss = 23.98470587741587\n",
      "Epoch 1530: Training Loss = 23.98483275143939\n",
      "Epoch 1540: Training Loss = 23.985100823515996\n",
      "Epoch 1550: Training Loss = 23.98464905189913\n",
      "Epoch 1560: Training Loss = 23.984328301312125\n",
      "Epoch 1570: Training Loss = 23.984119129648146\n",
      "Epoch 1580: Training Loss = 23.983954249080156\n",
      "Epoch 1590: Training Loss = 23.984340132746013\n",
      "Epoch 1600: Training Loss = 23.984482689789537\n",
      "Epoch 1610: Training Loss = 23.98501540808572\n",
      "Epoch 1620: Training Loss = 23.984623967294446\n",
      "Epoch 1630: Training Loss = 23.98463179160202\n",
      "Epoch 1640: Training Loss = 23.984691982100223\n",
      "Epoch 1650: Training Loss = 23.984762556213372\n",
      "Epoch 1660: Training Loss = 23.985149007701523\n",
      "Epoch 1670: Training Loss = 23.984594346217918\n",
      "Epoch 1680: Training Loss = 23.984571149685657\n",
      "Epoch 1690: Training Loss = 23.984418237715143\n",
      "Epoch 1700: Training Loss = 23.984731317275564\n",
      "Epoch 1710: Training Loss = 23.98407490027957\n",
      "Epoch 1720: Training Loss = 23.984688309141006\n",
      "Epoch 1730: Training Loss = 23.984240625625134\n",
      "Epoch 1740: Training Loss = 23.98454538728232\n",
      "Epoch 1750: Training Loss = 23.98417628593709\n",
      "Epoch 1760: Training Loss = 23.98469243003206\n",
      "Epoch 1770: Training Loss = 23.984625782506917\n",
      "Epoch 1780: Training Loss = 23.984328145565293\n",
      "Epoch 1790: Training Loss = 23.984496370359242\n",
      "Epoch 1800: Training Loss = 23.984442509221285\n",
      "Epoch 1810: Training Loss = 23.984555398426306\n",
      "Epoch 1820: Training Loss = 23.984396904273524\n",
      "Epoch 1830: Training Loss = 23.984185802551742\n",
      "Epoch 1840: Training Loss = 23.98482374736754\n",
      "Epoch 1850: Training Loss = 23.984525058782012\n",
      "Epoch 1860: Training Loss = 23.98453133927969\n",
      "Epoch 1870: Training Loss = 23.984494081391272\n",
      "Epoch 1880: Training Loss = 23.984275971245584\n",
      "Epoch 1890: Training Loss = 23.984320124918415\n",
      "Epoch 1900: Training Loss = 23.98473971180868\n",
      "Epoch 1910: Training Loss = 23.984484790262556\n",
      "Epoch 1920: Training Loss = 23.984366604310697\n",
      "Epoch 1930: Training Loss = 23.984538739261183\n",
      "Epoch 1940: Training Loss = 23.984821067330216\n",
      "Epoch 1950: Training Loss = 23.984865539470384\n",
      "Epoch 1960: Training Loss = 23.98444411382772\n",
      "Epoch 1970: Training Loss = 23.9844897011179\n",
      "Epoch 1980: Training Loss = 23.984247262077723\n",
      "Epoch 1990: Training Loss = 23.984408614614324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 1649.3367682718333\n",
      "Epoch 10: Training Loss = 1606.092267317214\n",
      "Epoch 20: Training Loss = 1596.4043767236599\n",
      "Epoch 30: Training Loss = 1580.760626871515\n",
      "Epoch 40: Training Loss = 1572.1653411046866\n",
      "Epoch 50: Training Loss = 1567.66106643094\n",
      "Epoch 60: Training Loss = 1564.9950791265785\n",
      "Epoch 70: Training Loss = 1563.571097033839\n",
      "Epoch 80: Training Loss = 1562.7578746771865\n",
      "Epoch 90: Training Loss = 1562.2455398160546\n",
      "Epoch 100: Training Loss = 1561.895809812276\n",
      "Epoch 110: Training Loss = 1561.642295763386\n",
      "Epoch 120: Training Loss = 1561.4500599076782\n",
      "Epoch 130: Training Loss = 1561.2991752326873\n",
      "Epoch 140: Training Loss = 1561.177500476544\n",
      "Epoch 150: Training Loss = 1561.0772301336228\n",
      "Epoch 160: Training Loss = 1560.9931206484703\n",
      "Epoch 170: Training Loss = 1560.9215189709928\n",
      "Epoch 180: Training Loss = 1560.8598012875048\n",
      "Epoch 190: Training Loss = 1560.806033646109\n",
      "Epoch 200: Training Loss = 1560.758758605634\n",
      "Epoch 210: Training Loss = 1560.7168565299544\n",
      "Epoch 220: Training Loss = 1560.6794527395025\n",
      "Epoch 230: Training Loss = 1560.6458537487101\n",
      "Epoch 240: Training Loss = 1560.6155024779687\n",
      "Epoch 250: Training Loss = 1560.5879461565482\n",
      "Epoch 260: Training Loss = 1560.5628129043496\n",
      "Epoch 270: Training Loss = 1560.5397943685014\n",
      "Epoch 280: Training Loss = 1560.5186326606556\n",
      "Epoch 290: Training Loss = 1560.4991103991354\n",
      "Epoch 300: Training Loss = 1560.48104302616\n",
      "Epoch 310: Training Loss = 1560.4642728146755\n",
      "Epoch 320: Training Loss = 1560.448664145748\n",
      "Epoch 330: Training Loss = 1560.434099752332\n",
      "Epoch 340: Training Loss = 1560.4204777058242\n",
      "Epoch 350: Training Loss = 1560.4077089791235\n",
      "Epoch 360: Training Loss = 1560.3957154612835\n",
      "Epoch 370: Training Loss = 1560.384428328754\n",
      "Epoch 380: Training Loss = 1560.373786700596\n",
      "Epoch 390: Training Loss = 1560.3637365213442\n",
      "Epoch 400: Training Loss = 1560.3542296277753\n",
      "Epoch 410: Training Loss = 1560.3452229650457\n",
      "Epoch 420: Training Loss = 1560.3366779250375\n",
      "Epoch 430: Training Loss = 1560.3285597852018\n",
      "Epoch 440: Training Loss = 1560.3208372304598\n",
      "Epoch 450: Training Loss = 1560.313481944165\n",
      "Epoch 460: Training Loss = 1560.3064682567274\n",
      "Epoch 470: Training Loss = 1560.2997728426412\n",
      "Epoch 480: Training Loss = 1560.2933744582501\n",
      "Epoch 490: Training Loss = 1560.287253714045\n",
      "Epoch 500: Training Loss = 1560.2813928762469\n",
      "Epoch 510: Training Loss = 1560.2757756933947\n",
      "Epoch 520: Training Loss = 1560.2703872443067\n",
      "Epoch 530: Training Loss = 1560.2652138044411\n",
      "Epoch 540: Training Loss = 1560.2602427280424\n",
      "Epoch 550: Training Loss = 1560.255462344005\n",
      "Epoch 560: Training Loss = 1560.2508618635595\n",
      "Epoch 570: Training Loss = 1560.2464312982993\n",
      "Epoch 580: Training Loss = 1560.242161387191\n",
      "Epoch 590: Training Loss = 1560.2380435314196\n",
      "Epoch 600: Training Loss = 1560.2340697361458\n",
      "Epoch 610: Training Loss = 1560.2302325583228\n",
      "Epoch 620: Training Loss = 1560.2265250597825\n",
      "Epoch 630: Training Loss = 1560.2229407651305\n",
      "Epoch 640: Training Loss = 1560.2194736236984\n",
      "Epoch 650: Training Loss = 1560.2161179752488\n",
      "Epoch 660: Training Loss = 1560.2128685189275\n",
      "Epoch 670: Training Loss = 1560.2097202851705\n",
      "Epoch 680: Training Loss = 1560.2066686101555\n",
      "Epoch 690: Training Loss = 1560.2037091126424\n",
      "Epoch 700: Training Loss = 1560.2008376728393\n",
      "Epoch 710: Training Loss = 1560.1980504131893\n",
      "Epoch 720: Training Loss = 1560.1953436807814\n",
      "Epoch 730: Training Loss = 1560.192714031297\n",
      "Epoch 740: Training Loss = 1560.1901582143366\n",
      "Epoch 750: Training Loss = 1560.1876731598986\n",
      "Epoch 760: Training Loss = 1560.185255966082\n",
      "Epoch 770: Training Loss = 1560.1829038876724\n",
      "Epoch 780: Training Loss = 1560.1806143257352\n",
      "Epoch 790: Training Loss = 1560.178384817955\n",
      "Epoch 800: Training Loss = 1560.1762130297927\n",
      "Epoch 810: Training Loss = 1560.174096746286\n",
      "Epoch 820: Training Loss = 1560.1720338644805\n",
      "Epoch 830: Training Loss = 1560.1700223864345\n",
      "Epoch 840: Training Loss = 1560.168060412739\n",
      "Epoch 850: Training Loss = 1560.1661461365097\n",
      "Epoch 860: Training Loss = 1560.1642778378114\n",
      "Epoch 870: Training Loss = 1560.1624538785127\n",
      "Epoch 880: Training Loss = 1560.1606726974258\n",
      "Epoch 890: Training Loss = 1560.158932805888\n",
      "Epoch 900: Training Loss = 1560.1572327835684\n",
      "Epoch 910: Training Loss = 1560.1555712745812\n",
      "Epoch 920: Training Loss = 1560.1539469838995\n",
      "Epoch 930: Training Loss = 1560.152358673944\n",
      "Epoch 940: Training Loss = 1560.1508051614544\n",
      "Epoch 950: Training Loss = 1560.1492853145198\n",
      "Epoch 960: Training Loss = 1560.1477980498378\n",
      "Epoch 970: Training Loss = 1560.1463423301163\n",
      "Epoch 980: Training Loss = 1560.1449171616655\n",
      "Epoch 990: Training Loss = 1560.1435215921347\n",
      "Epoch 1000: Training Loss = 1560.1421547083528\n",
      "Epoch 1010: Training Loss = 1560.140815634375\n",
      "Epoch 1020: Training Loss = 1560.139503529572\n",
      "Epoch 1030: Training Loss = 1560.1382175868764\n",
      "Epoch 1040: Training Loss = 1560.1369570311367\n",
      "Epoch 1050: Training Loss = 1560.135721117531\n",
      "Epoch 1060: Training Loss = 1560.134509130109\n",
      "Epoch 1070: Training Loss = 1560.1333203804052\n",
      "Epoch 1080: Training Loss = 1560.1321542061432\n",
      "Epoch 1090: Training Loss = 1560.1310099699697\n",
      "Epoch 1100: Training Loss = 1560.1298870583307\n",
      "Epoch 1110: Training Loss = 1560.128784880339\n",
      "Epoch 1120: Training Loss = 1560.1277028667598\n",
      "Epoch 1130: Training Loss = 1560.1266404690055\n",
      "Epoch 1140: Training Loss = 1560.1255971582334\n",
      "Epoch 1150: Training Loss = 1560.124572424432\n",
      "Epoch 1160: Training Loss = 1560.1235657756297\n",
      "Epoch 1170: Training Loss = 1560.12257673707\n",
      "Epoch 1180: Training Loss = 1560.1216048504766\n",
      "Epoch 1190: Training Loss = 1560.1206496733478\n",
      "Epoch 1200: Training Loss = 1560.1197107782784\n",
      "Epoch 1210: Training Loss = 1560.118787752327\n",
      "Epoch 1220: Training Loss = 1560.1178801964033\n",
      "Epoch 1230: Training Loss = 1560.116987724687\n",
      "Epoch 1240: Training Loss = 1560.116109964089\n",
      "Epoch 1250: Training Loss = 1560.11524655374\n",
      "Epoch 1260: Training Loss = 1560.1143971444703\n",
      "Epoch 1270: Training Loss = 1560.113561398355\n",
      "Epoch 1280: Training Loss = 1560.112738988257\n",
      "Epoch 1290: Training Loss = 1560.1119295974108\n",
      "Epoch 1300: Training Loss = 1560.1111329190032\n",
      "Epoch 1310: Training Loss = 1560.1103486557956\n",
      "Epoch 1320: Training Loss = 1560.1095765197401\n",
      "Epoch 1330: Training Loss = 1560.1088162316323\n",
      "Epoch 1340: Training Loss = 1560.1080675207907\n",
      "Epoch 1350: Training Loss = 1560.1073301247013\n",
      "Epoch 1360: Training Loss = 1560.1066037887406\n",
      "Epoch 1370: Training Loss = 1560.1058882658742\n",
      "Epoch 1380: Training Loss = 1560.105183316358\n",
      "Epoch 1390: Training Loss = 1560.1044887074981\n",
      "Epoch 1400: Training Loss = 1560.1038042133791\n",
      "Epoch 1410: Training Loss = 1560.1031296146168\n",
      "Epoch 1420: Training Loss = 1560.1024646981252\n",
      "Epoch 1430: Training Loss = 1560.1018092569077\n",
      "Epoch 1440: Training Loss = 1560.101163089807\n",
      "Epoch 1450: Training Loss = 1560.1005260013414\n",
      "Epoch 1460: Training Loss = 1560.099897801482\n",
      "Epoch 1470: Training Loss = 1560.0992783054562\n",
      "Epoch 1480: Training Loss = 1560.098667333586\n",
      "Epoch 1490: Training Loss = 1560.098064711113\n",
      "Epoch 1500: Training Loss = 1560.097470268018\n",
      "Epoch 1510: Training Loss = 1560.0968838388671\n",
      "Epoch 1520: Training Loss = 1560.0963052626596\n",
      "Epoch 1530: Training Loss = 1560.0957343826826\n",
      "Epoch 1540: Training Loss = 1560.0951710463644\n",
      "Epoch 1550: Training Loss = 1560.094615105138\n",
      "Epoch 1560: Training Loss = 1560.0940664143172\n",
      "Epoch 1570: Training Loss = 1560.093524832965\n",
      "Epoch 1580: Training Loss = 1560.0929902237747\n",
      "Epoch 1590: Training Loss = 1560.0924624529532\n",
      "Epoch 1600: Training Loss = 1560.0919413901092\n",
      "Epoch 1610: Training Loss = 1560.0914269081477\n",
      "Epoch 1620: Training Loss = 1560.0909188831622\n",
      "Epoch 1630: Training Loss = 1560.0904171943405\n",
      "Epoch 1640: Training Loss = 1560.089921723867\n",
      "Epoch 1650: Training Loss = 1560.089432356824\n",
      "Epoch 1660: Training Loss = 1560.0889489811163\n",
      "Epoch 1670: Training Loss = 1560.0884714873657\n",
      "Epoch 1680: Training Loss = 1560.0879997688598\n",
      "Epoch 1690: Training Loss = 1560.0875337214334\n",
      "Epoch 1700: Training Loss = 1560.0870732434225\n",
      "Epoch 1710: Training Loss = 1560.0866182355755\n",
      "Epoch 1720: Training Loss = 1560.0861686009869\n",
      "Epoch 1730: Training Loss = 1560.0857242450256\n",
      "Epoch 1740: Training Loss = 1560.085285075271\n",
      "Epoch 1750: Training Loss = 1560.0848510014487\n",
      "Epoch 1760: Training Loss = 1560.0844219353653\n",
      "Epoch 1770: Training Loss = 1560.0839977908497\n",
      "Epoch 1780: Training Loss = 1560.0835784837072\n",
      "Epoch 1790: Training Loss = 1560.0831639316414\n",
      "Epoch 1800: Training Loss = 1560.0827540542123\n",
      "Epoch 1810: Training Loss = 1560.0823487727923\n",
      "Epoch 1820: Training Loss = 1560.081948010505\n",
      "Epoch 1830: Training Loss = 1560.0815516921766\n",
      "Epoch 1840: Training Loss = 1560.0811597442907\n",
      "Epoch 1850: Training Loss = 1560.0807720949474\n",
      "Epoch 1860: Training Loss = 1560.0803886738183\n",
      "Epoch 1870: Training Loss = 1560.0800094120882\n",
      "Epoch 1880: Training Loss = 1560.079634242446\n",
      "Epoch 1890: Training Loss = 1560.0792630990054\n",
      "Epoch 1900: Training Loss = 1560.0788959173033\n",
      "Epoch 1910: Training Loss = 1560.0785326342373\n",
      "Epoch 1920: Training Loss = 1560.0781731880422\n",
      "Epoch 1930: Training Loss = 1560.077817518247\n",
      "Epoch 1940: Training Loss = 1560.0774655656444\n",
      "Epoch 1950: Training Loss = 1560.0771172722648\n",
      "Epoch 1960: Training Loss = 1560.0767725813319\n",
      "Epoch 1970: Training Loss = 1560.0764314372436\n",
      "Epoch 1980: Training Loss = 1560.076093785527\n",
      "Epoch 1990: Training Loss = 1560.075759572828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 31.95355421665968\n",
      "Epoch 10: Training Loss = 30.995070363869118\n",
      "Epoch 20: Training Loss = 30.806821858402152\n",
      "Epoch 30: Training Loss = 30.759234972684038\n",
      "Epoch 40: Training Loss = 30.73900352008674\n",
      "Epoch 50: Training Loss = 30.726829561106623\n",
      "Epoch 60: Training Loss = 30.719267027473325\n",
      "Epoch 70: Training Loss = 30.71375767041542\n",
      "Epoch 80: Training Loss = 30.70953693555089\n",
      "Epoch 90: Training Loss = 30.70662768659862\n",
      "Epoch 100: Training Loss = 30.7044984708775\n",
      "Epoch 110: Training Loss = 30.7021376911319\n",
      "Epoch 120: Training Loss = 30.70057361918582\n",
      "Epoch 130: Training Loss = 30.69957540962609\n",
      "Epoch 140: Training Loss = 30.697701790326512\n",
      "Epoch 150: Training Loss = 30.696665601628094\n",
      "Epoch 160: Training Loss = 30.695944386420216\n",
      "Epoch 170: Training Loss = 30.695161702412612\n",
      "Epoch 180: Training Loss = 30.694417683867997\n",
      "Epoch 190: Training Loss = 30.693559140756243\n",
      "Epoch 200: Training Loss = 30.692629558965674\n",
      "Epoch 210: Training Loss = 30.692532610873013\n",
      "Epoch 220: Training Loss = 30.691868611177345\n",
      "Epoch 230: Training Loss = 30.69141606917151\n",
      "Epoch 240: Training Loss = 30.69099773933937\n",
      "Epoch 250: Training Loss = 30.690440938250326\n",
      "Epoch 260: Training Loss = 30.690108966027513\n",
      "Epoch 270: Training Loss = 30.689905029336067\n",
      "Epoch 280: Training Loss = 30.689499452500243\n",
      "Epoch 290: Training Loss = 30.689319117606573\n",
      "Epoch 300: Training Loss = 30.68892518932116\n",
      "Epoch 310: Training Loss = 30.688648631569485\n",
      "Epoch 320: Training Loss = 30.688478462083637\n",
      "Epoch 330: Training Loss = 30.688243002637265\n",
      "Epoch 340: Training Loss = 30.68809365448421\n",
      "Epoch 350: Training Loss = 30.687862663680026\n",
      "Epoch 360: Training Loss = 30.687614554190446\n",
      "Epoch 370: Training Loss = 30.68747560937407\n",
      "Epoch 380: Training Loss = 30.687346759311378\n",
      "Epoch 390: Training Loss = 30.68692467686153\n",
      "Epoch 400: Training Loss = 30.6869354629357\n",
      "Epoch 410: Training Loss = 30.68680103549425\n",
      "Epoch 420: Training Loss = 30.68658806471919\n",
      "Epoch 430: Training Loss = 30.68635311663054\n",
      "Epoch 440: Training Loss = 30.68632594947679\n",
      "Epoch 450: Training Loss = 30.686201713803552\n",
      "Epoch 460: Training Loss = 30.686121843033547\n",
      "Epoch 470: Training Loss = 30.68597631876187\n",
      "Epoch 480: Training Loss = 30.686033516965516\n",
      "Epoch 490: Training Loss = 30.685793192883132\n",
      "Epoch 500: Training Loss = 30.685708511993987\n",
      "Epoch 510: Training Loss = 30.685595632630257\n",
      "Epoch 520: Training Loss = 30.685508614565038\n",
      "Epoch 530: Training Loss = 30.685402027099176\n",
      "Epoch 540: Training Loss = 30.685300470715703\n",
      "Epoch 550: Training Loss = 30.685291906852356\n",
      "Epoch 560: Training Loss = 30.685235433723644\n",
      "Epoch 570: Training Loss = 30.684976554995195\n",
      "Epoch 580: Training Loss = 30.684976353795665\n",
      "Epoch 590: Training Loss = 30.68500295453857\n",
      "Epoch 600: Training Loss = 30.684868610882067\n",
      "Epoch 610: Training Loss = 30.68470413958446\n",
      "Epoch 620: Training Loss = 30.684779130097777\n",
      "Epoch 630: Training Loss = 30.684658230537618\n",
      "Epoch 640: Training Loss = 30.684559863797816\n",
      "Epoch 650: Training Loss = 30.6844828131132\n",
      "Epoch 660: Training Loss = 30.68452301722756\n",
      "Epoch 670: Training Loss = 30.684471367215316\n",
      "Epoch 680: Training Loss = 30.684329380995855\n",
      "Epoch 690: Training Loss = 30.68425984926216\n",
      "Epoch 700: Training Loss = 30.684190639912224\n",
      "Epoch 710: Training Loss = 30.684178763152254\n",
      "Epoch 720: Training Loss = 30.68424528679043\n",
      "Epoch 730: Training Loss = 30.684104091073625\n",
      "Epoch 740: Training Loss = 30.68405851715351\n",
      "Epoch 750: Training Loss = 30.683969154333205\n",
      "Epoch 760: Training Loss = 30.68392885087366\n",
      "Epoch 770: Training Loss = 30.683867098372605\n",
      "Epoch 780: Training Loss = 30.683836253816242\n",
      "Epoch 790: Training Loss = 30.683771744957124\n",
      "Epoch 800: Training Loss = 30.683803773381623\n",
      "Epoch 810: Training Loss = 30.683724455062677\n",
      "Epoch 820: Training Loss = 30.683677231691167\n",
      "Epoch 830: Training Loss = 30.683744468240633\n",
      "Epoch 840: Training Loss = 30.683610063941998\n",
      "Epoch 850: Training Loss = 30.683634690937527\n",
      "Epoch 860: Training Loss = 30.683532524042445\n",
      "Epoch 870: Training Loss = 30.683434543190767\n",
      "Epoch 880: Training Loss = 30.683505489346008\n",
      "Epoch 890: Training Loss = 30.683450307061833\n",
      "Epoch 900: Training Loss = 30.68347433736718\n",
      "Epoch 910: Training Loss = 30.683356677525403\n",
      "Epoch 920: Training Loss = 30.683327055954365\n",
      "Epoch 930: Training Loss = 30.683363584634282\n",
      "Epoch 940: Training Loss = 30.683385489338416\n",
      "Epoch 950: Training Loss = 30.68328651466912\n",
      "Epoch 960: Training Loss = 30.68323077911961\n",
      "Epoch 970: Training Loss = 30.683160057118553\n",
      "Epoch 980: Training Loss = 30.683159762212938\n",
      "Epoch 990: Training Loss = 30.683161904788896\n",
      "Epoch 1000: Training Loss = 30.68316856548605\n",
      "Epoch 1010: Training Loss = 30.68312261741236\n",
      "Epoch 1020: Training Loss = 30.68311011920564\n",
      "Epoch 1030: Training Loss = 30.6830712641046\n",
      "Epoch 1040: Training Loss = 30.683049789851932\n",
      "Epoch 1050: Training Loss = 30.682988685809384\n",
      "Epoch 1060: Training Loss = 30.682983717549224\n",
      "Epoch 1070: Training Loss = 30.682951858982545\n",
      "Epoch 1080: Training Loss = 30.682932226069347\n",
      "Epoch 1090: Training Loss = 30.68288846322942\n",
      "Epoch 1100: Training Loss = 30.68295673355972\n",
      "Epoch 1110: Training Loss = 30.682848639008547\n",
      "Epoch 1120: Training Loss = 30.682855590017816\n",
      "Epoch 1130: Training Loss = 30.682805085856746\n",
      "Epoch 1140: Training Loss = 30.68281107167489\n",
      "Epoch 1150: Training Loss = 30.682809397142076\n",
      "Epoch 1160: Training Loss = 30.68279826324192\n",
      "Epoch 1170: Training Loss = 30.68270879494175\n",
      "Epoch 1180: Training Loss = 30.682725743625895\n",
      "Epoch 1190: Training Loss = 30.682717587838248\n",
      "Epoch 1200: Training Loss = 30.68270541672389\n",
      "Epoch 1210: Training Loss = 30.6826800065898\n",
      "Epoch 1220: Training Loss = 30.682640505453385\n",
      "Epoch 1230: Training Loss = 30.682629203353464\n",
      "Epoch 1240: Training Loss = 30.682669609537317\n",
      "Epoch 1250: Training Loss = 30.682572078463572\n",
      "Epoch 1260: Training Loss = 30.682546797500297\n",
      "Epoch 1270: Training Loss = 30.68256107699155\n",
      "Epoch 1280: Training Loss = 30.682568659791787\n",
      "Epoch 1290: Training Loss = 30.682576673067487\n",
      "Epoch 1300: Training Loss = 30.682525377865304\n",
      "Epoch 1310: Training Loss = 30.68253247367628\n",
      "Epoch 1320: Training Loss = 30.68252319221618\n",
      "Epoch 1330: Training Loss = 30.682470170930568\n",
      "Epoch 1340: Training Loss = 30.682503809664972\n",
      "Epoch 1350: Training Loss = 30.682445495368846\n",
      "Epoch 1360: Training Loss = 30.682472667563644\n",
      "Epoch 1370: Training Loss = 30.68245890817179\n",
      "Epoch 1380: Training Loss = 30.68245714553445\n",
      "Epoch 1390: Training Loss = 30.682392567011995\n",
      "Epoch 1400: Training Loss = 30.6823369492126\n",
      "Epoch 1410: Training Loss = 30.68233654276708\n",
      "Epoch 1420: Training Loss = 30.682354616003632\n",
      "Epoch 1430: Training Loss = 30.682328877291837\n",
      "Epoch 1440: Training Loss = 30.682316629830954\n",
      "Epoch 1450: Training Loss = 30.682338660417948\n",
      "Epoch 1460: Training Loss = 30.682332519273174\n",
      "Epoch 1470: Training Loss = 30.682289406560916\n",
      "Epoch 1480: Training Loss = 30.68230515600877\n",
      "Epoch 1490: Training Loss = 30.682270345912613\n",
      "Epoch 1500: Training Loss = 30.682213237255723\n",
      "Epoch 1510: Training Loss = 30.68223178336696\n",
      "Epoch 1520: Training Loss = 30.682227780321764\n",
      "Epoch 1530: Training Loss = 30.68221511359851\n",
      "Epoch 1540: Training Loss = 30.682212649395815\n",
      "Epoch 1550: Training Loss = 30.682175016878702\n",
      "Epoch 1560: Training Loss = 30.682200770159263\n",
      "Epoch 1570: Training Loss = 30.68217100217312\n",
      "Epoch 1580: Training Loss = 30.68218007245025\n",
      "Epoch 1590: Training Loss = 30.682193790998664\n",
      "Epoch 1600: Training Loss = 30.682145323654147\n",
      "Epoch 1610: Training Loss = 30.682132966258962\n",
      "Epoch 1620: Training Loss = 30.68214676287393\n",
      "Epoch 1630: Training Loss = 30.682146888213644\n",
      "Epoch 1640: Training Loss = 30.6821380890638\n",
      "Epoch 1650: Training Loss = 30.682113472622273\n",
      "Epoch 1660: Training Loss = 30.68206644548634\n",
      "Epoch 1670: Training Loss = 30.682063464638784\n",
      "Epoch 1680: Training Loss = 30.68205324219147\n",
      "Epoch 1690: Training Loss = 30.68207422330011\n",
      "Epoch 1700: Training Loss = 30.682073321981786\n",
      "Epoch 1710: Training Loss = 30.68204909945003\n",
      "Epoch 1720: Training Loss = 30.682029253105043\n",
      "Epoch 1730: Training Loss = 30.681993086841253\n",
      "Epoch 1740: Training Loss = 30.681993024240136\n",
      "Epoch 1750: Training Loss = 30.681992036737373\n",
      "Epoch 1760: Training Loss = 30.681984589092142\n",
      "Epoch 1770: Training Loss = 30.68197759173524\n",
      "Epoch 1780: Training Loss = 30.681998132967138\n",
      "Epoch 1790: Training Loss = 30.681964255569145\n",
      "Epoch 1800: Training Loss = 30.681932953188724\n",
      "Epoch 1810: Training Loss = 30.68197553148503\n",
      "Epoch 1820: Training Loss = 30.681940773989723\n",
      "Epoch 1830: Training Loss = 30.681940970082557\n",
      "Epoch 1840: Training Loss = 30.681966907220854\n",
      "Epoch 1850: Training Loss = 30.681904810907163\n",
      "Epoch 1860: Training Loss = 30.681922348872966\n",
      "Epoch 1870: Training Loss = 30.681930531983994\n",
      "Epoch 1880: Training Loss = 30.68190684362248\n",
      "Epoch 1890: Training Loss = 30.681870275572244\n",
      "Epoch 1900: Training Loss = 30.68189899912344\n",
      "Epoch 1910: Training Loss = 30.681881004765312\n",
      "Epoch 1920: Training Loss = 30.681893944909113\n",
      "Epoch 1930: Training Loss = 30.681873351459384\n",
      "Epoch 1940: Training Loss = 30.68186172109367\n",
      "Epoch 1950: Training Loss = 30.68183451020974\n",
      "Epoch 1960: Training Loss = 30.68182775644441\n",
      "Epoch 1970: Training Loss = 30.68183587391178\n",
      "Epoch 1980: Training Loss = 30.681823908937208\n",
      "Epoch 1990: Training Loss = 30.681824855180885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 24.900948821201254\n",
      "Epoch 10: Training Loss = 24.172946186689487\n",
      "Epoch 20: Training Loss = 24.07083132932765\n",
      "Epoch 30: Training Loss = 24.04048431950799\n",
      "Epoch 40: Training Loss = 24.02613487982595\n",
      "Epoch 50: Training Loss = 24.017345106627392\n",
      "Epoch 60: Training Loss = 24.012067428641718\n",
      "Epoch 70: Training Loss = 24.008130609549376\n",
      "Epoch 80: Training Loss = 24.005115018836776\n",
      "Epoch 90: Training Loss = 24.00274252611286\n",
      "Epoch 100: Training Loss = 24.00117227097984\n",
      "Epoch 110: Training Loss = 23.99957852690908\n",
      "Epoch 120: Training Loss = 23.998030915126773\n",
      "Epoch 130: Training Loss = 23.99709143675588\n",
      "Epoch 140: Training Loss = 23.99607150435024\n",
      "Epoch 150: Training Loss = 23.995280898420333\n",
      "Epoch 160: Training Loss = 23.99465667245607\n",
      "Epoch 170: Training Loss = 23.993936767649284\n",
      "Epoch 180: Training Loss = 23.993634187832743\n",
      "Epoch 190: Training Loss = 23.993048474340807\n",
      "Epoch 200: Training Loss = 23.99262581777466\n",
      "Epoch 210: Training Loss = 23.992216068159127\n",
      "Epoch 220: Training Loss = 23.991834973396102\n",
      "Epoch 230: Training Loss = 23.99154209245582\n",
      "Epoch 240: Training Loss = 23.991305597482025\n",
      "Epoch 250: Training Loss = 23.990865253505678\n",
      "Epoch 260: Training Loss = 23.99064507471208\n",
      "Epoch 270: Training Loss = 23.990422881414805\n",
      "Epoch 280: Training Loss = 23.990318803363845\n",
      "Epoch 290: Training Loss = 23.98998528312378\n",
      "Epoch 300: Training Loss = 23.98976140917334\n",
      "Epoch 310: Training Loss = 23.989617994504805\n",
      "Epoch 320: Training Loss = 23.989486251460008\n",
      "Epoch 330: Training Loss = 23.989218891446797\n",
      "Epoch 340: Training Loss = 23.989051966601494\n",
      "Epoch 350: Training Loss = 23.98897087771281\n",
      "Epoch 360: Training Loss = 23.98880621063527\n",
      "Epoch 370: Training Loss = 23.98866871422124\n",
      "Epoch 380: Training Loss = 23.98861938108527\n",
      "Epoch 390: Training Loss = 23.98841557866732\n",
      "Epoch 400: Training Loss = 23.988264424575945\n",
      "Epoch 410: Training Loss = 23.988245073273276\n",
      "Epoch 420: Training Loss = 23.988137845985683\n",
      "Epoch 430: Training Loss = 23.988015823526954\n",
      "Epoch 440: Training Loss = 23.98794708499586\n",
      "Epoch 450: Training Loss = 23.9878609384645\n",
      "Epoch 460: Training Loss = 23.987749372483698\n",
      "Epoch 470: Training Loss = 23.987644648652857\n",
      "Epoch 480: Training Loss = 23.987592503883636\n",
      "Epoch 490: Training Loss = 23.9875978581961\n",
      "Epoch 500: Training Loss = 23.98743789658033\n",
      "Epoch 510: Training Loss = 23.987382075183184\n",
      "Epoch 520: Training Loss = 23.98728114559058\n",
      "Epoch 530: Training Loss = 23.987258810005684\n",
      "Epoch 540: Training Loss = 23.98720326964884\n",
      "Epoch 550: Training Loss = 23.987145904627635\n",
      "Epoch 560: Training Loss = 23.987084482076046\n",
      "Epoch 570: Training Loss = 23.986974889954038\n",
      "Epoch 580: Training Loss = 23.9869552506292\n",
      "Epoch 590: Training Loss = 23.986938173745024\n",
      "Epoch 600: Training Loss = 23.9868615692757\n",
      "Epoch 610: Training Loss = 23.98685872619689\n",
      "Epoch 620: Training Loss = 23.98678493292742\n",
      "Epoch 630: Training Loss = 23.98670759928748\n",
      "Epoch 640: Training Loss = 23.98667167406616\n",
      "Epoch 650: Training Loss = 23.9866865470651\n",
      "Epoch 660: Training Loss = 23.986616570655745\n",
      "Epoch 670: Training Loss = 23.98658967091032\n",
      "Epoch 680: Training Loss = 23.98657819485651\n",
      "Epoch 690: Training Loss = 23.986492743366437\n",
      "Epoch 700: Training Loss = 23.986502928541576\n",
      "Epoch 710: Training Loss = 23.986409470759977\n",
      "Epoch 720: Training Loss = 23.98643372464811\n",
      "Epoch 730: Training Loss = 23.986411175247458\n",
      "Epoch 740: Training Loss = 23.986362917590807\n",
      "Epoch 750: Training Loss = 23.986314379146595\n",
      "Epoch 760: Training Loss = 23.986246403609012\n",
      "Epoch 770: Training Loss = 23.986247472544832\n",
      "Epoch 780: Training Loss = 23.9862211402197\n",
      "Epoch 790: Training Loss = 23.986214926602482\n",
      "Epoch 800: Training Loss = 23.986154639174114\n",
      "Epoch 810: Training Loss = 23.986126754538084\n",
      "Epoch 820: Training Loss = 23.986117588341145\n",
      "Epoch 830: Training Loss = 23.986070110303263\n",
      "Epoch 840: Training Loss = 23.986072448442226\n",
      "Epoch 850: Training Loss = 23.986016112739765\n",
      "Epoch 860: Training Loss = 23.986008491159122\n",
      "Epoch 870: Training Loss = 23.985965922777105\n",
      "Epoch 880: Training Loss = 23.985945038481965\n",
      "Epoch 890: Training Loss = 23.98591988887426\n",
      "Epoch 900: Training Loss = 23.98590227030569\n",
      "Epoch 910: Training Loss = 23.985929479783913\n",
      "Epoch 920: Training Loss = 23.98589967480138\n",
      "Epoch 930: Training Loss = 23.98586397512466\n",
      "Epoch 940: Training Loss = 23.985840794061957\n",
      "Epoch 950: Training Loss = 23.985841869262096\n",
      "Epoch 960: Training Loss = 23.985774148082353\n",
      "Epoch 970: Training Loss = 23.98574772088947\n",
      "Epoch 980: Training Loss = 23.985795686027227\n",
      "Epoch 990: Training Loss = 23.985709673819226\n",
      "Epoch 1000: Training Loss = 23.9857109250063\n",
      "Epoch 1010: Training Loss = 23.985702653686342\n",
      "Epoch 1020: Training Loss = 23.985679874584157\n",
      "Epoch 1030: Training Loss = 23.985682624869487\n",
      "Epoch 1040: Training Loss = 23.98567170061328\n",
      "Epoch 1050: Training Loss = 23.985661998472164\n",
      "Epoch 1060: Training Loss = 23.985656188252154\n",
      "Epoch 1070: Training Loss = 23.98563951664633\n",
      "Epoch 1080: Training Loss = 23.98562693908742\n",
      "Epoch 1090: Training Loss = 23.98556558494584\n",
      "Epoch 1100: Training Loss = 23.98556244354244\n",
      "Epoch 1110: Training Loss = 23.985568343709218\n",
      "Epoch 1120: Training Loss = 23.985557141994388\n",
      "Epoch 1130: Training Loss = 23.985507282948934\n",
      "Epoch 1140: Training Loss = 23.9855252369737\n",
      "Epoch 1150: Training Loss = 23.98550033230661\n",
      "Epoch 1160: Training Loss = 23.985503295471975\n",
      "Epoch 1170: Training Loss = 23.985463700539867\n",
      "Epoch 1180: Training Loss = 23.985462441354713\n",
      "Epoch 1190: Training Loss = 23.985437712103597\n",
      "Epoch 1200: Training Loss = 23.9854260898133\n",
      "Epoch 1210: Training Loss = 23.985410476485626\n",
      "Epoch 1220: Training Loss = 23.985427183290795\n",
      "Epoch 1230: Training Loss = 23.985399821191947\n",
      "Epoch 1240: Training Loss = 23.98537060731774\n",
      "Epoch 1250: Training Loss = 23.98537756930516\n",
      "Epoch 1260: Training Loss = 23.98536702176356\n",
      "Epoch 1270: Training Loss = 23.98536396311357\n",
      "Epoch 1280: Training Loss = 23.985365091709227\n",
      "Epoch 1290: Training Loss = 23.985346288537272\n",
      "Epoch 1300: Training Loss = 23.985318014508703\n",
      "Epoch 1310: Training Loss = 23.985311506407143\n",
      "Epoch 1320: Training Loss = 23.985294350847838\n",
      "Epoch 1330: Training Loss = 23.985277988437037\n",
      "Epoch 1340: Training Loss = 23.98528519412891\n",
      "Epoch 1350: Training Loss = 23.985270603087542\n",
      "Epoch 1360: Training Loss = 23.985254538012757\n",
      "Epoch 1370: Training Loss = 23.985263767852913\n",
      "Epoch 1380: Training Loss = 23.98523807477455\n",
      "Epoch 1390: Training Loss = 23.98522991519419\n",
      "Epoch 1400: Training Loss = 23.98520614374243\n",
      "Epoch 1410: Training Loss = 23.985214802395618\n",
      "Epoch 1420: Training Loss = 23.985219716412285\n",
      "Epoch 1430: Training Loss = 23.98520564441106\n",
      "Epoch 1440: Training Loss = 23.98518181844422\n",
      "Epoch 1450: Training Loss = 23.98518778557815\n",
      "Epoch 1460: Training Loss = 23.985164917006397\n",
      "Epoch 1470: Training Loss = 23.98516314117419\n",
      "Epoch 1480: Training Loss = 23.98517167621606\n",
      "Epoch 1490: Training Loss = 23.98514875778\n",
      "Epoch 1500: Training Loss = 23.98512656322288\n",
      "Epoch 1510: Training Loss = 23.985118066643608\n",
      "Epoch 1520: Training Loss = 23.985131107300656\n",
      "Epoch 1530: Training Loss = 23.985117382734934\n",
      "Epoch 1540: Training Loss = 23.98512246800327\n",
      "Epoch 1550: Training Loss = 23.98509604543411\n",
      "Epoch 1560: Training Loss = 23.985115194710627\n",
      "Epoch 1570: Training Loss = 23.985087236213204\n",
      "Epoch 1580: Training Loss = 23.98508686722846\n",
      "Epoch 1590: Training Loss = 23.98509267239337\n",
      "Epoch 1600: Training Loss = 23.985086248145866\n",
      "Epoch 1610: Training Loss = 23.985076502770763\n",
      "Epoch 1620: Training Loss = 23.985061169528475\n",
      "Epoch 1630: Training Loss = 23.98507648814165\n",
      "Epoch 1640: Training Loss = 23.98506693857417\n",
      "Epoch 1650: Training Loss = 23.985046248379987\n",
      "Epoch 1660: Training Loss = 23.98505097957034\n",
      "Epoch 1670: Training Loss = 23.985034770524795\n",
      "Epoch 1680: Training Loss = 23.98502315325504\n",
      "Epoch 1690: Training Loss = 23.984988911264438\n",
      "Epoch 1700: Training Loss = 23.985037423657076\n",
      "Epoch 1710: Training Loss = 23.98501799376613\n",
      "Epoch 1720: Training Loss = 23.98499532466131\n",
      "Epoch 1730: Training Loss = 23.984992776807616\n",
      "Epoch 1740: Training Loss = 23.984984708878674\n",
      "Epoch 1750: Training Loss = 23.984975069822053\n",
      "Epoch 1760: Training Loss = 23.98497370397374\n",
      "Epoch 1770: Training Loss = 23.984965219443595\n",
      "Epoch 1780: Training Loss = 23.98496451655593\n",
      "Epoch 1790: Training Loss = 23.984973144859527\n",
      "Epoch 1800: Training Loss = 23.98494979677298\n",
      "Epoch 1810: Training Loss = 23.98495515316045\n",
      "Epoch 1820: Training Loss = 23.984944749149683\n",
      "Epoch 1830: Training Loss = 23.984918454128177\n",
      "Epoch 1840: Training Loss = 23.984942464417568\n",
      "Epoch 1850: Training Loss = 23.984933798574108\n",
      "Epoch 1860: Training Loss = 23.98493209297292\n",
      "Epoch 1870: Training Loss = 23.984905032112692\n",
      "Epoch 1880: Training Loss = 23.984905814670736\n",
      "Epoch 1890: Training Loss = 23.984912015436453\n",
      "Epoch 1900: Training Loss = 23.984885312062246\n",
      "Epoch 1910: Training Loss = 23.98490529474967\n",
      "Epoch 1920: Training Loss = 23.98489021646818\n",
      "Epoch 1930: Training Loss = 23.984897604377164\n",
      "Epoch 1940: Training Loss = 23.984880487175452\n",
      "Epoch 1950: Training Loss = 23.984878033071784\n",
      "Epoch 1960: Training Loss = 23.984869449234587\n",
      "Epoch 1970: Training Loss = 23.9848644718193\n",
      "Epoch 1980: Training Loss = 23.98485240287754\n",
      "Epoch 1990: Training Loss = 23.98486165596353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 1646.7074771254324\n",
      "Epoch 10: Training Loss = 1569.116859819037\n",
      "Epoch 20: Training Loss = 1563.687092753385\n",
      "Epoch 30: Training Loss = 1562.2209445045237\n",
      "Epoch 40: Training Loss = 1561.5714299692088\n",
      "Epoch 50: Training Loss = 1561.2103893992894\n",
      "Epoch 60: Training Loss = 1560.982075457717\n",
      "Epoch 70: Training Loss = 1560.8252249391155\n",
      "Epoch 80: Training Loss = 1560.71107879667\n",
      "Epoch 90: Training Loss = 1560.6244162994549\n",
      "Epoch 100: Training Loss = 1560.5564509952494\n",
      "Epoch 110: Training Loss = 1560.5017654273317\n",
      "Epoch 120: Training Loss = 1560.456842131994\n",
      "Epoch 130: Training Loss = 1560.4193000853807\n",
      "Epoch 140: Training Loss = 1560.3874712600318\n",
      "Epoch 150: Training Loss = 1560.3601530238834\n",
      "Epoch 160: Training Loss = 1560.3364568274167\n",
      "Epoch 170: Training Loss = 1560.3157122139078\n",
      "Epoch 180: Training Loss = 1560.297403942301\n",
      "Epoch 190: Training Loss = 1560.281129638969\n",
      "Epoch 200: Training Loss = 1560.266570572727\n",
      "Epoch 210: Training Loss = 1560.2534710477944\n",
      "Epoch 220: Training Loss = 1560.2416235926157\n",
      "Epoch 230: Training Loss = 1560.230858130273\n",
      "Epoch 240: Training Loss = 1560.2210339369474\n",
      "Epoch 250: Training Loss = 1560.2120335864736\n",
      "Epoch 260: Training Loss = 1560.2037583320857\n",
      "Epoch 270: Training Loss = 1560.196124542998\n",
      "Epoch 280: Training Loss = 1560.1890609254351\n",
      "Epoch 290: Training Loss = 1560.1825063339716\n",
      "Epoch 300: Training Loss = 1560.176408032089\n",
      "Epoch 310: Training Loss = 1560.170720298073\n",
      "Epoch 320: Training Loss = 1560.16540329892\n",
      "Epoch 330: Training Loss = 1560.1604221740474\n",
      "Epoch 340: Training Loss = 1560.1557462846934\n",
      "Epoch 350: Training Loss = 1560.1513485950447\n",
      "Epoch 360: Training Loss = 1560.1472051589865\n",
      "Epoch 370: Training Loss = 1560.1432946920004\n",
      "Epoch 380: Training Loss = 1560.139598212211\n",
      "Epoch 390: Training Loss = 1560.1360987379205\n",
      "Epoch 400: Training Loss = 1560.132781031506\n",
      "Epoch 410: Training Loss = 1560.129631381634\n",
      "Epoch 420: Training Loss = 1560.1266374172574\n",
      "Epoch 430: Training Loss = 1560.1237879481237\n",
      "Epoch 440: Training Loss = 1560.1210728275125\n",
      "Epoch 450: Training Loss = 1560.1184828336166\n",
      "Epoch 460: Training Loss = 1560.1160095667592\n",
      "Epoch 470: Training Loss = 1560.1136453599593\n",
      "Epoch 480: Training Loss = 1560.1113832009446\n",
      "Epoch 490: Training Loss = 1560.1092166638746\n",
      "Epoch 500: Training Loss = 1560.1071398494362\n",
      "Epoch 510: Training Loss = 1560.1051473321\n",
      "Epoch 520: Training Loss = 1560.1032341136192\n",
      "Epoch 530: Training Loss = 1560.1013955818596\n",
      "Epoch 540: Training Loss = 1560.0996274743118\n",
      "Epoch 550: Training Loss = 1560.0979258456405\n",
      "Epoch 560: Training Loss = 1560.0962870388084\n",
      "Epoch 570: Training Loss = 1560.0947076592429\n",
      "Epoch 580: Training Loss = 1560.0931845517832\n",
      "Epoch 590: Training Loss = 1560.0917147799719\n",
      "Epoch 600: Training Loss = 1560.0902956074688\n",
      "Epoch 610: Training Loss = 1560.088924481335\n",
      "Epoch 620: Training Loss = 1560.0875990169484\n",
      "Epoch 630: Training Loss = 1560.0863169843876\n",
      "Epoch 640: Training Loss = 1560.0850762961084\n",
      "Epoch 650: Training Loss = 1560.083874995804\n",
      "Epoch 660: Training Loss = 1560.0827112482405\n",
      "Epoch 670: Training Loss = 1560.081583330093\n",
      "Epoch 680: Training Loss = 1560.0804896215254\n",
      "Epoch 690: Training Loss = 1560.0794285985744\n",
      "Epoch 700: Training Loss = 1560.078398826174\n",
      "Epoch 710: Training Loss = 1560.0773989517736\n",
      "Epoch 720: Training Loss = 1560.0764276995094\n",
      "Epoch 730: Training Loss = 1560.075483864849\n",
      "Epoch 740: Training Loss = 1560.0745663096961\n",
      "Epoch 750: Training Loss = 1560.0736739578783\n",
      "Epoch 760: Training Loss = 1560.0728057909985\n",
      "Epoch 770: Training Loss = 1560.071960844615\n",
      "Epoch 780: Training Loss = 1560.0711382047346\n",
      "Epoch 790: Training Loss = 1560.0703370045428\n",
      "Epoch 800: Training Loss = 1560.0695564214323\n",
      "Epoch 810: Training Loss = 1560.0687956741965\n",
      "Epoch 820: Training Loss = 1560.0680540204912\n",
      "Epoch 830: Training Loss = 1560.0673307544278\n",
      "Epoch 840: Training Loss = 1560.0666252043752\n",
      "Epoch 850: Training Loss = 1560.0659367309308\n",
      "Epoch 860: Training Loss = 1560.0652647249797\n",
      "Epoch 870: Training Loss = 1560.0646086059614\n",
      "Epoch 880: Training Loss = 1560.0639678201792\n",
      "Epoch 890: Training Loss = 1560.0633418393131\n",
      "Epoch 900: Training Loss = 1560.062730158945\n",
      "Epoch 910: Training Loss = 1560.0621322972488\n",
      "Epoch 920: Training Loss = 1560.0615477937351\n",
      "Epoch 930: Training Loss = 1560.0609762080885\n",
      "Epoch 940: Training Loss = 1560.0604171190678\n",
      "Epoch 950: Training Loss = 1560.0598701234999\n",
      "Epoch 960: Training Loss = 1560.0593348353154\n",
      "Epoch 970: Training Loss = 1560.058810884644\n",
      "Epoch 980: Training Loss = 1560.0582979169978\n",
      "Epoch 990: Training Loss = 1560.0577955924641\n",
      "Epoch 1000: Training Loss = 1560.0573035849714\n",
      "Epoch 1010: Training Loss = 1560.0568215815888\n",
      "Epoch 1020: Training Loss = 1560.0563492818817\n",
      "Epoch 1030: Training Loss = 1560.0558863972822\n",
      "Epoch 1040: Training Loss = 1560.0554326505185\n",
      "Epoch 1050: Training Loss = 1560.0549877750695\n",
      "Epoch 1060: Training Loss = 1560.0545515146523\n",
      "Epoch 1070: Training Loss = 1560.0541236227166\n",
      "Epoch 1080: Training Loss = 1560.0537038620234\n",
      "Epoch 1090: Training Loss = 1560.0532920041744\n",
      "Epoch 1100: Training Loss = 1560.052887829221\n",
      "Epoch 1110: Training Loss = 1560.0524911252867\n",
      "Epoch 1120: Training Loss = 1560.052101688184\n",
      "Epoch 1130: Training Loss = 1560.0517193210771\n",
      "Epoch 1140: Training Loss = 1560.0513438341534\n",
      "Epoch 1150: Training Loss = 1560.0509750443202\n",
      "Epoch 1160: Training Loss = 1560.0506127748995\n",
      "Epoch 1170: Training Loss = 1560.0502568553595\n",
      "Epoch 1180: Training Loss = 1560.049907121039\n",
      "Epoch 1190: Training Loss = 1560.049563412915\n",
      "Epoch 1200: Training Loss = 1560.0492255773422\n",
      "Epoch 1210: Training Loss = 1560.0488934658454\n",
      "Epoch 1220: Training Loss = 1560.048566934902\n",
      "Epoch 1230: Training Loss = 1560.0482458457195\n",
      "Epoch 1240: Training Loss = 1560.047930064078\n",
      "Epoch 1250: Training Loss = 1560.0476194601033\n",
      "Epoch 1260: Training Loss = 1560.0473139081187\n",
      "Epoch 1270: Training Loss = 1560.0470132864734\n",
      "Epoch 1280: Training Loss = 1560.0467174773821\n",
      "Epoch 1290: Training Loss = 1560.0464263667582\n",
      "Epoch 1300: Training Loss = 1560.0461398440968\n",
      "Epoch 1310: Training Loss = 1560.0458578023124\n",
      "Epoch 1320: Training Loss = 1560.0455801376236\n",
      "Epoch 1330: Training Loss = 1560.0453067494125\n",
      "Epoch 1340: Training Loss = 1560.0450375401172\n",
      "Epoch 1350: Training Loss = 1560.0447724151124\n",
      "Epoch 1360: Training Loss = 1560.0445112825996\n",
      "Epoch 1370: Training Loss = 1560.0442540535082\n",
      "Epoch 1380: Training Loss = 1560.0440006413796\n",
      "Epoch 1390: Training Loss = 1560.0437509622907\n",
      "Epoch 1400: Training Loss = 1560.0435049347493\n",
      "Epoch 1410: Training Loss = 1560.0432624796163\n",
      "Epoch 1420: Training Loss = 1560.043023520013\n",
      "Epoch 1430: Training Loss = 1560.0427879812414\n",
      "Epoch 1440: Training Loss = 1560.0425557907213\n",
      "Epoch 1450: Training Loss = 1560.042326877897\n",
      "Epoch 1460: Training Loss = 1560.0421011741785\n",
      "Epoch 1470: Training Loss = 1560.0418786128673\n",
      "Epoch 1480: Training Loss = 1560.041659129105\n",
      "Epoch 1490: Training Loss = 1560.0414426597881\n",
      "Epoch 1500: Training Loss = 1560.041229143528\n",
      "Epoch 1510: Training Loss = 1560.041018520596\n",
      "Epoch 1520: Training Loss = 1560.040810732843\n",
      "Epoch 1530: Training Loss = 1560.0406057236764\n",
      "Epoch 1540: Training Loss = 1560.0404034379833\n",
      "Epoch 1550: Training Loss = 1560.0402038221052\n",
      "Epoch 1560: Training Loss = 1560.0400068237823\n",
      "Epoch 1570: Training Loss = 1560.0398123920922\n",
      "Epoch 1580: Training Loss = 1560.0396204774327\n",
      "Epoch 1590: Training Loss = 1560.0394310314662\n",
      "Epoch 1600: Training Loss = 1560.03924400708\n",
      "Epoch 1610: Training Loss = 1560.0390593583465\n",
      "Epoch 1620: Training Loss = 1560.0388770404982\n",
      "Epoch 1630: Training Loss = 1560.0386970098748\n",
      "Epoch 1640: Training Loss = 1560.038519223893\n",
      "Epoch 1650: Training Loss = 1560.0383436410316\n",
      "Epoch 1660: Training Loss = 1560.0381702207676\n",
      "Epoch 1670: Training Loss = 1560.037998923579\n",
      "Epoch 1680: Training Loss = 1560.0378297108805\n",
      "Epoch 1690: Training Loss = 1560.0376625450324\n",
      "Epoch 1700: Training Loss = 1560.0374973892845\n",
      "Epoch 1710: Training Loss = 1560.0373342077517\n",
      "Epoch 1720: Training Loss = 1560.037172965409\n",
      "Epoch 1730: Training Loss = 1560.037013628048\n",
      "Epoch 1740: Training Loss = 1560.036856162249\n",
      "Epoch 1750: Training Loss = 1560.0367005353824\n",
      "Epoch 1760: Training Loss = 1560.0365467155643\n",
      "Epoch 1770: Training Loss = 1560.0363946716388\n",
      "Epoch 1780: Training Loss = 1560.0362443731613\n",
      "Epoch 1790: Training Loss = 1560.0360957903843\n",
      "Epoch 1800: Training Loss = 1560.035948894216\n",
      "Epoch 1810: Training Loss = 1560.0358036562338\n",
      "Epoch 1820: Training Loss = 1560.0356600486382\n",
      "Epoch 1830: Training Loss = 1560.0355180442489\n",
      "Epoch 1840: Training Loss = 1560.0353776164843\n",
      "Epoch 1850: Training Loss = 1560.0352387393432\n",
      "Epoch 1860: Training Loss = 1560.0351013873963\n",
      "Epoch 1870: Training Loss = 1560.0349655357657\n",
      "Epoch 1880: Training Loss = 1560.0348311601035\n",
      "Epoch 1890: Training Loss = 1560.0346982365963\n",
      "Epoch 1900: Training Loss = 1560.0345667419265\n",
      "Epoch 1910: Training Loss = 1560.0344366532784\n",
      "Epoch 1920: Training Loss = 1560.0343079483157\n",
      "Epoch 1930: Training Loss = 1560.034180605176\n",
      "Epoch 1940: Training Loss = 1560.034054602452\n",
      "Epoch 1950: Training Loss = 1560.033929919171\n",
      "Epoch 1960: Training Loss = 1560.033806534817\n",
      "Epoch 1970: Training Loss = 1560.0336844292733\n",
      "Epoch 1980: Training Loss = 1560.033563582849\n",
      "Epoch 1990: Training Loss = 1560.0334439762496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 32.554912468074015\n",
      "Epoch 10: Training Loss = 31.015217630202947\n",
      "Epoch 20: Training Loss = 30.806574244402793\n",
      "Epoch 30: Training Loss = 30.749087791111425\n",
      "Epoch 40: Training Loss = 30.724429838035363\n",
      "Epoch 50: Training Loss = 30.713287453752116\n",
      "Epoch 60: Training Loss = 30.70606740257217\n",
      "Epoch 70: Training Loss = 30.701435869137537\n",
      "Epoch 80: Training Loss = 30.69827861233061\n",
      "Epoch 90: Training Loss = 30.69584532984575\n",
      "Epoch 100: Training Loss = 30.694004202939755\n",
      "Epoch 110: Training Loss = 30.692580935509966\n",
      "Epoch 120: Training Loss = 30.691439454412357\n",
      "Epoch 130: Training Loss = 30.69038264222001\n",
      "Epoch 140: Training Loss = 30.689665919244906\n",
      "Epoch 150: Training Loss = 30.68897443158898\n",
      "Epoch 160: Training Loss = 30.688328599750132\n",
      "Epoch 170: Training Loss = 30.687823793151818\n",
      "Epoch 180: Training Loss = 30.687386825026763\n",
      "Epoch 190: Training Loss = 30.686948918421148\n",
      "Epoch 200: Training Loss = 30.686579841927713\n",
      "Epoch 210: Training Loss = 30.68631561835509\n",
      "Epoch 220: Training Loss = 30.685954779099887\n",
      "Epoch 230: Training Loss = 30.685724759377994\n",
      "Epoch 240: Training Loss = 30.685515651743636\n",
      "Epoch 250: Training Loss = 30.685261610402257\n",
      "Epoch 260: Training Loss = 30.68505381143503\n",
      "Epoch 270: Training Loss = 30.68487618367781\n",
      "Epoch 280: Training Loss = 30.684698127631062\n",
      "Epoch 290: Training Loss = 30.684558071058337\n",
      "Epoch 300: Training Loss = 30.684401199123638\n",
      "Epoch 310: Training Loss = 30.684274966275208\n",
      "Epoch 320: Training Loss = 30.684151745767828\n",
      "Epoch 330: Training Loss = 30.68406171839144\n",
      "Epoch 340: Training Loss = 30.683919920444414\n",
      "Epoch 350: Training Loss = 30.68380080660787\n",
      "Epoch 360: Training Loss = 30.68372818708072\n",
      "Epoch 370: Training Loss = 30.683604803595873\n",
      "Epoch 380: Training Loss = 30.683506152696413\n",
      "Epoch 390: Training Loss = 30.683411666949315\n",
      "Epoch 400: Training Loss = 30.683370064858998\n",
      "Epoch 410: Training Loss = 30.683276318863953\n",
      "Epoch 420: Training Loss = 30.683218254280987\n",
      "Epoch 430: Training Loss = 30.683165217068535\n",
      "Epoch 440: Training Loss = 30.68308136513834\n",
      "Epoch 450: Training Loss = 30.68301493593336\n",
      "Epoch 460: Training Loss = 30.682997197574014\n",
      "Epoch 470: Training Loss = 30.682899260798376\n",
      "Epoch 480: Training Loss = 30.682853452336037\n",
      "Epoch 490: Training Loss = 30.68279078551577\n",
      "Epoch 500: Training Loss = 30.68275979456592\n",
      "Epoch 510: Training Loss = 30.682681140054118\n",
      "Epoch 520: Training Loss = 30.682654659090808\n",
      "Epoch 530: Training Loss = 30.68262719674945\n",
      "Epoch 540: Training Loss = 30.682574785397428\n",
      "Epoch 550: Training Loss = 30.682542961532796\n",
      "Epoch 560: Training Loss = 30.68249049484267\n",
      "Epoch 570: Training Loss = 30.682450685782594\n",
      "Epoch 580: Training Loss = 30.682423841420892\n",
      "Epoch 590: Training Loss = 30.682375414178694\n",
      "Epoch 600: Training Loss = 30.682353366654578\n",
      "Epoch 610: Training Loss = 30.68231148467297\n",
      "Epoch 620: Training Loss = 30.682282415890793\n",
      "Epoch 630: Training Loss = 30.68226356872949\n",
      "Epoch 640: Training Loss = 30.682227456143114\n",
      "Epoch 650: Training Loss = 30.682222111120193\n",
      "Epoch 660: Training Loss = 30.682173272884516\n",
      "Epoch 670: Training Loss = 30.682151256140365\n",
      "Epoch 680: Training Loss = 30.68212582985377\n",
      "Epoch 690: Training Loss = 30.682099463239545\n",
      "Epoch 700: Training Loss = 30.682074354252336\n",
      "Epoch 710: Training Loss = 30.682067178536883\n",
      "Epoch 720: Training Loss = 30.682028315436586\n",
      "Epoch 730: Training Loss = 30.681997734893447\n",
      "Epoch 740: Training Loss = 30.68198144610203\n",
      "Epoch 750: Training Loss = 30.681964073968853\n",
      "Epoch 760: Training Loss = 30.68194108639658\n",
      "Epoch 770: Training Loss = 30.68192068794879\n",
      "Epoch 780: Training Loss = 30.681906271334043\n",
      "Epoch 790: Training Loss = 30.681886582540002\n",
      "Epoch 800: Training Loss = 30.681867891038365\n",
      "Epoch 810: Training Loss = 30.681856897728096\n",
      "Epoch 820: Training Loss = 30.681831540983083\n",
      "Epoch 830: Training Loss = 30.681813349147898\n",
      "Epoch 840: Training Loss = 30.68179875090232\n",
      "Epoch 850: Training Loss = 30.681783454856877\n",
      "Epoch 860: Training Loss = 30.68176912039265\n",
      "Epoch 870: Training Loss = 30.68174953886623\n",
      "Epoch 880: Training Loss = 30.68173082527726\n",
      "Epoch 890: Training Loss = 30.68171362854177\n",
      "Epoch 900: Training Loss = 30.681706095505344\n",
      "Epoch 910: Training Loss = 30.681689130342676\n",
      "Epoch 920: Training Loss = 30.68167777407115\n",
      "Epoch 930: Training Loss = 30.681673237303297\n",
      "Epoch 940: Training Loss = 30.681654763099893\n",
      "Epoch 950: Training Loss = 30.68164035589053\n",
      "Epoch 960: Training Loss = 30.681621396091856\n",
      "Epoch 970: Training Loss = 30.681611442293985\n",
      "Epoch 980: Training Loss = 30.681601405102928\n",
      "Epoch 990: Training Loss = 30.68159585737006\n",
      "Epoch 1000: Training Loss = 30.681572357847713\n",
      "Epoch 1010: Training Loss = 30.6815726865996\n",
      "Epoch 1020: Training Loss = 30.681559653268224\n",
      "Epoch 1030: Training Loss = 30.681544935084418\n",
      "Epoch 1040: Training Loss = 30.681538199818554\n",
      "Epoch 1050: Training Loss = 30.681523863049\n",
      "Epoch 1060: Training Loss = 30.68150944272257\n",
      "Epoch 1070: Training Loss = 30.68150482293224\n",
      "Epoch 1080: Training Loss = 30.681495001350587\n",
      "Epoch 1090: Training Loss = 30.68148435310842\n",
      "Epoch 1100: Training Loss = 30.68147415228116\n",
      "Epoch 1110: Training Loss = 30.681464053853574\n",
      "Epoch 1120: Training Loss = 30.681458409505957\n",
      "Epoch 1130: Training Loss = 30.681445886012998\n",
      "Epoch 1140: Training Loss = 30.681437295246752\n",
      "Epoch 1150: Training Loss = 30.681434504055012\n",
      "Epoch 1160: Training Loss = 30.681425198349093\n",
      "Epoch 1170: Training Loss = 30.681412724314256\n",
      "Epoch 1180: Training Loss = 30.681406205798712\n",
      "Epoch 1190: Training Loss = 30.68139902354696\n",
      "Epoch 1200: Training Loss = 30.68138987760226\n",
      "Epoch 1210: Training Loss = 30.681378884190114\n",
      "Epoch 1220: Training Loss = 30.68137372529387\n",
      "Epoch 1230: Training Loss = 30.681375227454904\n",
      "Epoch 1240: Training Loss = 30.681359508838476\n",
      "Epoch 1250: Training Loss = 30.68135026113172\n",
      "Epoch 1260: Training Loss = 30.681346091213992\n",
      "Epoch 1270: Training Loss = 30.681338465494644\n",
      "Epoch 1280: Training Loss = 30.681332474627567\n",
      "Epoch 1290: Training Loss = 30.681323173141728\n",
      "Epoch 1300: Training Loss = 30.681319728211736\n",
      "Epoch 1310: Training Loss = 30.681307830069887\n",
      "Epoch 1320: Training Loss = 30.681302470651165\n",
      "Epoch 1330: Training Loss = 30.68129644396643\n",
      "Epoch 1340: Training Loss = 30.681292436285197\n",
      "Epoch 1350: Training Loss = 30.681285420161114\n",
      "Epoch 1360: Training Loss = 30.681282034049538\n",
      "Epoch 1370: Training Loss = 30.681271674562367\n",
      "Epoch 1380: Training Loss = 30.681268267941665\n",
      "Epoch 1390: Training Loss = 30.681263091161558\n",
      "Epoch 1400: Training Loss = 30.681254626814408\n",
      "Epoch 1410: Training Loss = 30.681251955174265\n",
      "Epoch 1420: Training Loss = 30.681244198873273\n",
      "Epoch 1430: Training Loss = 30.681240159602424\n",
      "Epoch 1440: Training Loss = 30.681232697270424\n",
      "Epoch 1450: Training Loss = 30.681231256191648\n",
      "Epoch 1460: Training Loss = 30.681222519555227\n",
      "Epoch 1470: Training Loss = 30.681219605378907\n",
      "Epoch 1480: Training Loss = 30.68121488189479\n",
      "Epoch 1490: Training Loss = 30.681208005525743\n",
      "Epoch 1500: Training Loss = 30.681197575995803\n",
      "Epoch 1510: Training Loss = 30.681196983254843\n",
      "Epoch 1520: Training Loss = 30.68119949409147\n",
      "Epoch 1530: Training Loss = 30.681186325131357\n",
      "Epoch 1540: Training Loss = 30.68118193055702\n",
      "Epoch 1550: Training Loss = 30.681174310708627\n",
      "Epoch 1560: Training Loss = 30.681173847894886\n",
      "Epoch 1570: Training Loss = 30.68116704359851\n",
      "Epoch 1580: Training Loss = 30.68116448949105\n",
      "Epoch 1590: Training Loss = 30.681160253911614\n",
      "Epoch 1600: Training Loss = 30.68115345985483\n",
      "Epoch 1610: Training Loss = 30.681152661607243\n",
      "Epoch 1620: Training Loss = 30.68114525225564\n",
      "Epoch 1630: Training Loss = 30.68114457402837\n",
      "Epoch 1640: Training Loss = 30.68113789870344\n",
      "Epoch 1650: Training Loss = 30.68113265416849\n",
      "Epoch 1660: Training Loss = 30.681129559717164\n",
      "Epoch 1670: Training Loss = 30.681126670581563\n",
      "Epoch 1680: Training Loss = 30.681121994909326\n",
      "Epoch 1690: Training Loss = 30.681122746562277\n",
      "Epoch 1700: Training Loss = 30.681116505012337\n",
      "Epoch 1710: Training Loss = 30.681107551403112\n",
      "Epoch 1720: Training Loss = 30.68110535886804\n",
      "Epoch 1730: Training Loss = 30.681104304470715\n",
      "Epoch 1740: Training Loss = 30.68109982710647\n",
      "Epoch 1750: Training Loss = 30.681096102824498\n",
      "Epoch 1760: Training Loss = 30.68109199363602\n",
      "Epoch 1770: Training Loss = 30.681093111166064\n",
      "Epoch 1780: Training Loss = 30.681089845158443\n",
      "Epoch 1790: Training Loss = 30.681085006297565\n",
      "Epoch 1800: Training Loss = 30.681077270019678\n",
      "Epoch 1810: Training Loss = 30.681075337020395\n",
      "Epoch 1820: Training Loss = 30.6810749557398\n",
      "Epoch 1830: Training Loss = 30.68106761266891\n",
      "Epoch 1840: Training Loss = 30.681062366071757\n",
      "Epoch 1850: Training Loss = 30.68106011559806\n",
      "Epoch 1860: Training Loss = 30.681057262478173\n",
      "Epoch 1870: Training Loss = 30.681055138057282\n",
      "Epoch 1880: Training Loss = 30.68105262096423\n",
      "Epoch 1890: Training Loss = 30.681048910202694\n",
      "Epoch 1900: Training Loss = 30.68104646609795\n",
      "Epoch 1910: Training Loss = 30.681039084922222\n",
      "Epoch 1920: Training Loss = 30.681040951907203\n",
      "Epoch 1930: Training Loss = 30.68103382376106\n",
      "Epoch 1940: Training Loss = 30.681033092603403\n",
      "Epoch 1950: Training Loss = 30.681028328595247\n",
      "Epoch 1960: Training Loss = 30.68102740522278\n",
      "Epoch 1970: Training Loss = 30.68102581428379\n",
      "Epoch 1980: Training Loss = 30.681021623827053\n",
      "Epoch 1990: Training Loss = 30.681016936372988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 24.4806012205395\n",
      "Epoch 10: Training Loss = 24.110222369638915\n",
      "Epoch 20: Training Loss = 24.04258103285004\n",
      "Epoch 30: Training Loss = 24.0205828442016\n",
      "Epoch 40: Training Loss = 24.01034585251334\n",
      "Epoch 50: Training Loss = 24.004428803338133\n",
      "Epoch 60: Training Loss = 24.000093475964118\n",
      "Epoch 70: Training Loss = 23.99815215829193\n",
      "Epoch 80: Training Loss = 23.995966076091484\n",
      "Epoch 90: Training Loss = 23.994332778686264\n",
      "Epoch 100: Training Loss = 23.993040269810834\n",
      "Epoch 110: Training Loss = 23.992164077512218\n",
      "Epoch 120: Training Loss = 23.991672707455788\n",
      "Epoch 130: Training Loss = 23.99076508621609\n",
      "Epoch 140: Training Loss = 23.99037741536695\n",
      "Epoch 150: Training Loss = 23.990021668590288\n",
      "Epoch 160: Training Loss = 23.9893855305255\n",
      "Epoch 170: Training Loss = 23.98913909979632\n",
      "Epoch 180: Training Loss = 23.988835906079316\n",
      "Epoch 190: Training Loss = 23.988518015027648\n",
      "Epoch 200: Training Loss = 23.988389591934695\n",
      "Epoch 210: Training Loss = 23.988199256436737\n",
      "Epoch 220: Training Loss = 23.987975345540118\n",
      "Epoch 230: Training Loss = 23.987765076201796\n",
      "Epoch 240: Training Loss = 23.987620271859996\n",
      "Epoch 250: Training Loss = 23.987477006687396\n",
      "Epoch 260: Training Loss = 23.98727467507525\n",
      "Epoch 270: Training Loss = 23.987117716402416\n",
      "Epoch 280: Training Loss = 23.987085354534738\n",
      "Epoch 290: Training Loss = 23.986880196669514\n",
      "Epoch 300: Training Loss = 23.986856863043066\n",
      "Epoch 310: Training Loss = 23.986721638647722\n",
      "Epoch 320: Training Loss = 23.986562916144376\n",
      "Epoch 330: Training Loss = 23.98659313319174\n",
      "Epoch 340: Training Loss = 23.986416613591715\n",
      "Epoch 350: Training Loss = 23.98638886655615\n",
      "Epoch 360: Training Loss = 23.98631743044851\n",
      "Epoch 370: Training Loss = 23.986221100465244\n",
      "Epoch 380: Training Loss = 23.986153483802568\n",
      "Epoch 390: Training Loss = 23.98612775849222\n",
      "Epoch 400: Training Loss = 23.986048859015206\n",
      "Epoch 410: Training Loss = 23.986044118360176\n",
      "Epoch 420: Training Loss = 23.98592756317814\n",
      "Epoch 430: Training Loss = 23.985899880781037\n",
      "Epoch 440: Training Loss = 23.98588232426931\n",
      "Epoch 450: Training Loss = 23.985836764806777\n",
      "Epoch 460: Training Loss = 23.985738497446683\n",
      "Epoch 470: Training Loss = 23.98569593258904\n",
      "Epoch 480: Training Loss = 23.98568726691\n",
      "Epoch 490: Training Loss = 23.985648104374864\n",
      "Epoch 500: Training Loss = 23.98563998893379\n",
      "Epoch 510: Training Loss = 23.985606966276052\n",
      "Epoch 520: Training Loss = 23.985567736787083\n",
      "Epoch 530: Training Loss = 23.985535120046716\n",
      "Epoch 540: Training Loss = 23.985508339848128\n",
      "Epoch 550: Training Loss = 23.98547694742336\n",
      "Epoch 560: Training Loss = 23.985450289192304\n",
      "Epoch 570: Training Loss = 23.985380785006633\n",
      "Epoch 580: Training Loss = 23.985385984258215\n",
      "Epoch 590: Training Loss = 23.985386339977094\n",
      "Epoch 600: Training Loss = 23.985328948659927\n",
      "Epoch 610: Training Loss = 23.985300838328037\n",
      "Epoch 620: Training Loss = 23.985271349267194\n",
      "Epoch 630: Training Loss = 23.98527104477027\n",
      "Epoch 640: Training Loss = 23.985263418079402\n",
      "Epoch 650: Training Loss = 23.985233164739213\n",
      "Epoch 660: Training Loss = 23.985213916157466\n",
      "Epoch 670: Training Loss = 23.98516702128984\n",
      "Epoch 680: Training Loss = 23.985156094716775\n",
      "Epoch 690: Training Loss = 23.985159997499025\n",
      "Epoch 700: Training Loss = 23.985132173210346\n",
      "Epoch 710: Training Loss = 23.985130404596713\n",
      "Epoch 720: Training Loss = 23.985099898482073\n",
      "Epoch 730: Training Loss = 23.98510108353701\n",
      "Epoch 740: Training Loss = 23.98506080971302\n",
      "Epoch 750: Training Loss = 23.985060266450446\n",
      "Epoch 760: Training Loss = 23.98503481400807\n",
      "Epoch 770: Training Loss = 23.985029054388146\n",
      "Epoch 780: Training Loss = 23.984998019122145\n",
      "Epoch 790: Training Loss = 23.985006544452713\n",
      "Epoch 800: Training Loss = 23.98498237849916\n",
      "Epoch 810: Training Loss = 23.98494921367728\n",
      "Epoch 820: Training Loss = 23.9849465026766\n",
      "Epoch 830: Training Loss = 23.984934982893087\n",
      "Epoch 840: Training Loss = 23.98492359506271\n",
      "Epoch 850: Training Loss = 23.984915294157734\n",
      "Epoch 860: Training Loss = 23.984896421495467\n",
      "Epoch 870: Training Loss = 23.984888197179643\n",
      "Epoch 880: Training Loss = 23.984864507555514\n",
      "Epoch 890: Training Loss = 23.98487705989989\n",
      "Epoch 900: Training Loss = 23.984857244403997\n",
      "Epoch 910: Training Loss = 23.984852144808208\n",
      "Epoch 920: Training Loss = 23.984839684673602\n",
      "Epoch 930: Training Loss = 23.984834677094042\n",
      "Epoch 940: Training Loss = 23.984814623605367\n",
      "Epoch 950: Training Loss = 23.98482186234449\n",
      "Epoch 960: Training Loss = 23.98480480905921\n",
      "Epoch 970: Training Loss = 23.9848081635663\n",
      "Epoch 980: Training Loss = 23.984809257794197\n",
      "Epoch 990: Training Loss = 23.98479414676144\n",
      "Epoch 1000: Training Loss = 23.984770747999498\n",
      "Epoch 1010: Training Loss = 23.98477203900434\n",
      "Epoch 1020: Training Loss = 23.98475963860094\n",
      "Epoch 1030: Training Loss = 23.98475459865174\n",
      "Epoch 1040: Training Loss = 23.984732581242394\n",
      "Epoch 1050: Training Loss = 23.984721073685787\n",
      "Epoch 1060: Training Loss = 23.98471701129775\n",
      "Epoch 1070: Training Loss = 23.98472253446945\n",
      "Epoch 1080: Training Loss = 23.984706642057724\n",
      "Epoch 1090: Training Loss = 23.98471710873536\n",
      "Epoch 1100: Training Loss = 23.984692012282604\n",
      "Epoch 1110: Training Loss = 23.98468668592666\n",
      "Epoch 1120: Training Loss = 23.98467724384907\n",
      "Epoch 1130: Training Loss = 23.98464636603001\n",
      "Epoch 1140: Training Loss = 23.984680293181114\n",
      "Epoch 1150: Training Loss = 23.98465468851807\n",
      "Epoch 1160: Training Loss = 23.98465293515794\n",
      "Epoch 1170: Training Loss = 23.98466572559509\n",
      "Epoch 1180: Training Loss = 23.98464856060763\n",
      "Epoch 1190: Training Loss = 23.98463302075649\n",
      "Epoch 1200: Training Loss = 23.98463269337765\n",
      "Epoch 1210: Training Loss = 23.98464087217458\n",
      "Epoch 1220: Training Loss = 23.984617586742015\n",
      "Epoch 1230: Training Loss = 23.984634790460017\n",
      "Epoch 1240: Training Loss = 23.98461391941236\n",
      "Epoch 1250: Training Loss = 23.984627566477897\n",
      "Epoch 1260: Training Loss = 23.984597157196518\n",
      "Epoch 1270: Training Loss = 23.98459087540821\n",
      "Epoch 1280: Training Loss = 23.984602457486105\n",
      "Epoch 1290: Training Loss = 23.984588271249105\n",
      "Epoch 1300: Training Loss = 23.98457880290861\n",
      "Epoch 1310: Training Loss = 23.984575725874148\n",
      "Epoch 1320: Training Loss = 23.98457829225382\n",
      "Epoch 1330: Training Loss = 23.98456367189302\n",
      "Epoch 1340: Training Loss = 23.98455785335074\n",
      "Epoch 1350: Training Loss = 23.984567737098924\n",
      "Epoch 1360: Training Loss = 23.984550194265115\n",
      "Epoch 1370: Training Loss = 23.984544493342046\n",
      "Epoch 1380: Training Loss = 23.984552108889574\n",
      "Epoch 1390: Training Loss = 23.984554725411822\n",
      "Epoch 1400: Training Loss = 23.984539798069232\n",
      "Epoch 1410: Training Loss = 23.98454319180812\n",
      "Epoch 1420: Training Loss = 23.984528573463777\n",
      "Epoch 1430: Training Loss = 23.984533677744228\n",
      "Epoch 1440: Training Loss = 23.98451570287337\n",
      "Epoch 1450: Training Loss = 23.984512618648317\n",
      "Epoch 1460: Training Loss = 23.984507002707243\n",
      "Epoch 1470: Training Loss = 23.984503186296465\n",
      "Epoch 1480: Training Loss = 23.984499133126718\n",
      "Epoch 1490: Training Loss = 23.984506625981304\n",
      "Epoch 1500: Training Loss = 23.98449306646124\n",
      "Epoch 1510: Training Loss = 23.984483729215057\n",
      "Epoch 1520: Training Loss = 23.984493408902914\n",
      "Epoch 1530: Training Loss = 23.984489754665145\n",
      "Epoch 1540: Training Loss = 23.984487997052177\n",
      "Epoch 1550: Training Loss = 23.984486944935192\n",
      "Epoch 1560: Training Loss = 23.984481477683318\n",
      "Epoch 1570: Training Loss = 23.98446478652092\n",
      "Epoch 1580: Training Loss = 23.984471190671815\n",
      "Epoch 1590: Training Loss = 23.984471669562705\n",
      "Epoch 1600: Training Loss = 23.98446370219553\n",
      "Epoch 1610: Training Loss = 23.98447162165071\n",
      "Epoch 1620: Training Loss = 23.98446127788827\n",
      "Epoch 1630: Training Loss = 23.98445719001577\n",
      "Epoch 1640: Training Loss = 23.98445819266826\n",
      "Epoch 1650: Training Loss = 23.98444604643542\n",
      "Epoch 1660: Training Loss = 23.984438401244667\n",
      "Epoch 1670: Training Loss = 23.984437669544636\n",
      "Epoch 1680: Training Loss = 23.984439824170074\n",
      "Epoch 1690: Training Loss = 23.98443623863585\n",
      "Epoch 1700: Training Loss = 23.984435564750413\n",
      "Epoch 1710: Training Loss = 23.984429947699123\n",
      "Epoch 1720: Training Loss = 23.984429668342585\n",
      "Epoch 1730: Training Loss = 23.984421076124825\n",
      "Epoch 1740: Training Loss = 23.984413390888527\n",
      "Epoch 1750: Training Loss = 23.984427083492385\n",
      "Epoch 1760: Training Loss = 23.98442427833967\n",
      "Epoch 1770: Training Loss = 23.984414546365283\n",
      "Epoch 1780: Training Loss = 23.984409862208697\n",
      "Epoch 1790: Training Loss = 23.984410892554777\n",
      "Epoch 1800: Training Loss = 23.98441036008236\n",
      "Epoch 1810: Training Loss = 23.98440298711424\n",
      "Epoch 1820: Training Loss = 23.984400493791576\n",
      "Epoch 1830: Training Loss = 23.984399660064128\n",
      "Epoch 1840: Training Loss = 23.98440031701206\n",
      "Epoch 1850: Training Loss = 23.98440216506489\n",
      "Epoch 1860: Training Loss = 23.9843925440567\n",
      "Epoch 1870: Training Loss = 23.984390066975855\n",
      "Epoch 1880: Training Loss = 23.98439017680372\n",
      "Epoch 1890: Training Loss = 23.98438945065125\n",
      "Epoch 1900: Training Loss = 23.984395720637366\n",
      "Epoch 1910: Training Loss = 23.984394764567874\n",
      "Epoch 1920: Training Loss = 23.98437328886331\n",
      "Epoch 1930: Training Loss = 23.984376269323114\n",
      "Epoch 1940: Training Loss = 23.984367095744894\n",
      "Epoch 1950: Training Loss = 23.984376190234574\n",
      "Epoch 1960: Training Loss = 23.984376921788975\n",
      "Epoch 1970: Training Loss = 23.984369470940727\n",
      "Epoch 1980: Training Loss = 23.98436335975115\n",
      "Epoch 1990: Training Loss = 23.984369433045224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 1621.7634576753453\n",
      "Epoch 10: Training Loss = 1599.124217806743\n",
      "Epoch 20: Training Loss = 1582.5513483634659\n",
      "Epoch 30: Training Loss = 1574.6289533944725\n",
      "Epoch 40: Training Loss = 1570.6602412598959\n",
      "Epoch 50: Training Loss = 1568.3696939376605\n",
      "Epoch 60: Training Loss = 1566.8934727671403\n",
      "Epoch 70: Training Loss = 1565.8648450592816\n",
      "Epoch 80: Training Loss = 1565.1064342282796\n",
      "Epoch 90: Training Loss = 1564.5231242226414\n",
      "Epoch 100: Training Loss = 1564.059669211223\n",
      "Epoch 110: Training Loss = 1563.6818948672058\n",
      "Epoch 120: Training Loss = 1563.3675589138022\n",
      "Epoch 130: Training Loss = 1563.1015655164397\n",
      "Epoch 140: Training Loss = 1562.8733030999672\n",
      "Epoch 150: Training Loss = 1562.6750873245064\n",
      "Epoch 160: Training Loss = 1562.501211020394\n",
      "Epoch 170: Training Loss = 1562.3473427669326\n",
      "Epoch 180: Training Loss = 1562.2101336008936\n",
      "Epoch 190: Training Loss = 1562.0869523132505\n",
      "Epoch 200: Training Loss = 1561.9757027111511\n",
      "Epoch 210: Training Loss = 1561.874694616889\n",
      "Epoch 220: Training Loss = 1561.7825509725578\n",
      "Epoch 230: Training Loss = 1561.6981397051072\n",
      "Epoch 240: Training Loss = 1561.620522840282\n",
      "Epoch 250: Training Loss = 1561.5489177600575\n",
      "Epoch 260: Training Loss = 1561.4826670603773\n",
      "Epoch 270: Training Loss = 1561.421214535732\n",
      "Epoch 280: Training Loss = 1561.3640856116238\n",
      "Epoch 290: Training Loss = 1561.310871185928\n",
      "Epoch 300: Training Loss = 1561.2612143605606\n",
      "Epoch 310: Training Loss = 1561.2147999024683\n",
      "Epoch 320: Training Loss = 1561.1713463960848\n",
      "Epoch 330: Training Loss = 1561.130600917171\n",
      "Epoch 340: Training Loss = 1561.0923357648628\n",
      "Epoch 350: Training Loss = 1561.056346518012\n",
      "Epoch 360: Training Loss = 1561.0224506022732\n",
      "Epoch 370: Training Loss = 1560.9904857186843\n",
      "Epoch 380: Training Loss = 1560.96030781557\n",
      "Epoch 390: Training Loss = 1560.9317886347437\n",
      "Epoch 400: Training Loss = 1560.9048130964425\n",
      "Epoch 410: Training Loss = 1560.87927684788\n",
      "Epoch 420: Training Loss = 1560.8550842183472\n",
      "Epoch 430: Training Loss = 1560.832146681154\n",
      "Epoch 440: Training Loss = 1560.810381795173\n",
      "Epoch 450: Training Loss = 1560.7897125249979\n",
      "Epoch 460: Training Loss = 1560.77006681816\n",
      "Epoch 470: Training Loss = 1560.7513773322714\n",
      "Epoch 480: Training Loss = 1560.7335812332506\n",
      "Epoch 490: Training Loss = 1560.7166200150577\n",
      "Epoch 500: Training Loss = 1560.7004393146215\n",
      "Epoch 510: Training Loss = 1560.684988711566\n",
      "Epoch 520: Training Loss = 1560.6702215120158\n",
      "Epoch 530: Training Loss = 1560.6560945204626\n",
      "Epoch 540: Training Loss = 1560.6425678056341\n",
      "Epoch 550: Training Loss = 1560.6296044661556\n",
      "Epoch 560: Training Loss = 1560.6171704011372\n",
      "Epoch 570: Training Loss = 1560.6052340893862\n",
      "Epoch 580: Training Loss = 1560.59376638014\n",
      "Epoch 590: Training Loss = 1560.582740296746\n",
      "Epoch 600: Training Loss = 1560.5721308542686\n",
      "Epoch 610: Training Loss = 1560.5619148911337\n",
      "Epoch 620: Training Loss = 1560.5520709145403\n",
      "Epoch 630: Training Loss = 1560.5425789591363\n",
      "Epoch 640: Training Loss = 1560.5334204581375\n",
      "Epoch 650: Training Loss = 1560.524578126044\n",
      "Epoch 660: Training Loss = 1560.5160358520868\n",
      "Epoch 670: Training Loss = 1560.5077786034617\n",
      "Epoch 680: Training Loss = 1560.4997923375165\n",
      "Epoch 690: Training Loss = 1560.492063922022\n",
      "Epoch 700: Training Loss = 1560.484581062809\n",
      "Epoch 710: Training Loss = 1560.47733223801\n",
      "Epoch 720: Training Loss = 1560.470306638305\n",
      "Epoch 730: Training Loss = 1560.463494112519\n",
      "Epoch 740: Training Loss = 1560.456885118125\n",
      "Epoch 750: Training Loss = 1560.4504706760836\n",
      "Epoch 760: Training Loss = 1560.444242329654\n",
      "Epoch 770: Training Loss = 1560.4381921067384\n",
      "Epoch 780: Training Loss = 1560.4323124854645\n",
      "Epoch 790: Training Loss = 1560.4265963626597\n",
      "Epoch 800: Training Loss = 1560.4210370249225\n",
      "Epoch 810: Training Loss = 1560.4156281221083\n",
      "Epoch 820: Training Loss = 1560.4103636429377\n",
      "Epoch 830: Training Loss = 1560.4052378925624\n",
      "Epoch 840: Training Loss = 1560.4002454718875\n",
      "Epoch 850: Training Loss = 1560.3953812585405\n",
      "Epoch 860: Training Loss = 1560.3906403892663\n",
      "Epoch 870: Training Loss = 1560.386018243666\n",
      "Epoch 880: Training Loss = 1560.381510429157\n",
      "Epoch 890: Training Loss = 1560.3771127670636\n",
      "Epoch 900: Training Loss = 1560.3728212796816\n",
      "Epoch 910: Training Loss = 1560.368632178311\n",
      "Epoch 920: Training Loss = 1560.3645418521266\n",
      "Epoch 930: Training Loss = 1560.360546857823\n",
      "Epoch 940: Training Loss = 1560.3566439099948\n",
      "Epoch 950: Training Loss = 1560.352829872159\n",
      "Epoch 960: Training Loss = 1560.3491017484214\n",
      "Epoch 970: Training Loss = 1560.3454566756302\n",
      "Epoch 980: Training Loss = 1560.3418919161427\n",
      "Epoch 990: Training Loss = 1560.3384048509715\n",
      "Epoch 1000: Training Loss = 1560.3349929734566\n",
      "Epoch 1010: Training Loss = 1560.3316538832817\n",
      "Epoch 1020: Training Loss = 1560.328385280905\n",
      "Epoch 1030: Training Loss = 1560.3251849623143\n",
      "Epoch 1040: Training Loss = 1560.3220508141212\n",
      "Epoch 1050: Training Loss = 1560.3189808089573\n",
      "Epoch 1060: Training Loss = 1560.3159730011469\n",
      "Epoch 1070: Training Loss = 1560.3130255226297\n",
      "Epoch 1080: Training Loss = 1560.310136579125\n",
      "Epoch 1090: Training Loss = 1560.3073044465445\n",
      "Epoch 1100: Training Loss = 1560.3045274676056\n",
      "Epoch 1110: Training Loss = 1560.301804048593\n",
      "Epoch 1120: Training Loss = 1560.2991326563824\n",
      "Epoch 1130: Training Loss = 1560.2965118155832\n",
      "Epoch 1140: Training Loss = 1560.2939401058447\n",
      "Epoch 1150: Training Loss = 1560.2914161593217\n",
      "Epoch 1160: Training Loss = 1560.2889386582729\n",
      "Epoch 1170: Training Loss = 1560.2865063328074\n",
      "Epoch 1180: Training Loss = 1560.2841179587108\n",
      "Epoch 1190: Training Loss = 1560.2817723554483\n",
      "Epoch 1200: Training Loss = 1560.2794683842121\n",
      "Epoch 1210: Training Loss = 1560.2772049461073\n",
      "Epoch 1220: Training Loss = 1560.2749809804202\n",
      "Epoch 1230: Training Loss = 1560.2727954629904\n",
      "Epoch 1240: Training Loss = 1560.2706474046438\n",
      "Epoch 1250: Training Loss = 1560.268535849718\n",
      "Epoch 1260: Training Loss = 1560.2664598746624\n",
      "Epoch 1270: Training Loss = 1560.264418586696\n",
      "Epoch 1280: Training Loss = 1560.2624111225518\n",
      "Epoch 1290: Training Loss = 1560.2604366472683\n",
      "Epoch 1300: Training Loss = 1560.2584943530371\n",
      "Epoch 1310: Training Loss = 1560.2565834581135\n",
      "Epoch 1320: Training Loss = 1560.2547032057792\n",
      "Epoch 1330: Training Loss = 1560.2528528633516\n",
      "Epoch 1340: Training Loss = 1560.251031721246\n",
      "Epoch 1350: Training Loss = 1560.2492390920622\n",
      "Epoch 1360: Training Loss = 1560.2474743097353\n",
      "Epoch 1370: Training Loss = 1560.245736728722\n",
      "Epoch 1380: Training Loss = 1560.244025723204\n",
      "Epoch 1390: Training Loss = 1560.2423406863657\n",
      "Epoch 1400: Training Loss = 1560.2406810296584\n",
      "Epoch 1410: Training Loss = 1560.2390461821333\n",
      "Epoch 1420: Training Loss = 1560.237435589781\n",
      "Epoch 1430: Training Loss = 1560.2358487149222\n",
      "Epoch 1440: Training Loss = 1560.234285035612\n",
      "Epoch 1450: Training Loss = 1560.232744045052\n",
      "Epoch 1460: Training Loss = 1560.2312252510667\n",
      "Epoch 1470: Training Loss = 1560.2297281755798\n",
      "Epoch 1480: Training Loss = 1560.2282523541028\n",
      "Epoch 1490: Training Loss = 1560.2267973352668\n",
      "Epoch 1500: Training Loss = 1560.2253626803556\n",
      "Epoch 1510: Training Loss = 1560.2239479628845\n",
      "Epoch 1520: Training Loss = 1560.2225527681605\n",
      "Epoch 1530: Training Loss = 1560.2211766928842\n",
      "Epoch 1540: Training Loss = 1560.2198193447653\n",
      "Epoch 1550: Training Loss = 1560.21848034215\n",
      "Epoch 1560: Training Loss = 1560.2171593136638\n",
      "Epoch 1570: Training Loss = 1560.2158558978629\n",
      "Epoch 1580: Training Loss = 1560.2145697429212\n",
      "Epoch 1590: Training Loss = 1560.213300506288\n",
      "Epoch 1600: Training Loss = 1560.2120478544168\n",
      "Epoch 1610: Training Loss = 1560.2108114624516\n",
      "Epoch 1620: Training Loss = 1560.2095910139449\n",
      "Epoch 1630: Training Loss = 1560.2083862005986\n",
      "Epoch 1640: Training Loss = 1560.207196721998\n",
      "Epoch 1650: Training Loss = 1560.206022285375\n",
      "Epoch 1660: Training Loss = 1560.2048626053413\n",
      "Epoch 1670: Training Loss = 1560.2037174036873\n",
      "Epoch 1680: Training Loss = 1560.202586409123\n",
      "Epoch 1690: Training Loss = 1560.201469357118\n",
      "Epoch 1700: Training Loss = 1560.2003659896338\n",
      "Epoch 1710: Training Loss = 1560.1992760549645\n",
      "Epoch 1720: Training Loss = 1560.1981993075376\n",
      "Epoch 1730: Training Loss = 1560.197135507709\n",
      "Epoch 1740: Training Loss = 1560.1960844216273\n",
      "Epoch 1750: Training Loss = 1560.1950458210113\n",
      "Epoch 1760: Training Loss = 1560.1940194830195\n",
      "Epoch 1770: Training Loss = 1560.1930051900765\n",
      "Epoch 1780: Training Loss = 1560.1920027297174\n",
      "Epoch 1790: Training Loss = 1560.1910118944588\n",
      "Epoch 1800: Training Loss = 1560.1900324816095\n",
      "Epoch 1810: Training Loss = 1560.189064293189\n",
      "Epoch 1820: Training Loss = 1560.1881071357511\n",
      "Epoch 1830: Training Loss = 1560.187160820273\n",
      "Epoch 1840: Training Loss = 1560.1862251620175\n",
      "Epoch 1850: Training Loss = 1560.185299980429\n",
      "Epoch 1860: Training Loss = 1560.1843850990113\n",
      "Epoch 1870: Training Loss = 1560.1834803452023\n",
      "Epoch 1880: Training Loss = 1560.1825855502714\n",
      "Epoch 1890: Training Loss = 1560.181700549226\n",
      "Epoch 1900: Training Loss = 1560.180825180693\n",
      "Epoch 1910: Training Loss = 1560.1799592868258\n",
      "Epoch 1920: Training Loss = 1560.179102713217\n",
      "Epoch 1930: Training Loss = 1560.1782553087876\n",
      "Epoch 1940: Training Loss = 1560.1774169257108\n",
      "Epoch 1950: Training Loss = 1560.1765874193281\n",
      "Epoch 1960: Training Loss = 1560.1757666480535\n",
      "Epoch 1970: Training Loss = 1560.174954473304\n",
      "Epoch 1980: Training Loss = 1560.1741507594097\n",
      "Epoch 1990: Training Loss = 1560.173355373547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 32.01541988420611\n",
      "Epoch 10: Training Loss = 31.79814421673139\n",
      "Epoch 20: Training Loss = 31.577572918029585\n",
      "Epoch 30: Training Loss = 31.39383304973608\n",
      "Epoch 40: Training Loss = 31.310213401067333\n",
      "Epoch 50: Training Loss = 31.245818796308416\n",
      "Epoch 60: Training Loss = 31.201040600642365\n",
      "Epoch 70: Training Loss = 31.178682668540574\n",
      "Epoch 80: Training Loss = 31.16268870649545\n",
      "Epoch 90: Training Loss = 31.14925279063954\n",
      "Epoch 100: Training Loss = 31.13059162208029\n",
      "Epoch 110: Training Loss = 31.12484933296793\n",
      "Epoch 120: Training Loss = 31.116500934277024\n",
      "Epoch 130: Training Loss = 31.10605769048081\n",
      "Epoch 140: Training Loss = 31.099910092973353\n",
      "Epoch 150: Training Loss = 31.093493407678636\n",
      "Epoch 160: Training Loss = 31.09066514492013\n",
      "Epoch 170: Training Loss = 31.083991450576992\n",
      "Epoch 180: Training Loss = 31.079215784939567\n",
      "Epoch 190: Training Loss = 31.07666677327015\n",
      "Epoch 200: Training Loss = 31.07101378512319\n",
      "Epoch 210: Training Loss = 31.066245081268445\n",
      "Epoch 220: Training Loss = 31.06239606328374\n",
      "Epoch 230: Training Loss = 31.058025105669564\n",
      "Epoch 240: Training Loss = 31.05281138419933\n",
      "Epoch 250: Training Loss = 31.050358560829594\n",
      "Epoch 260: Training Loss = 31.041727188383994\n",
      "Epoch 270: Training Loss = 31.037123265547695\n",
      "Epoch 280: Training Loss = 31.031180683235622\n",
      "Epoch 290: Training Loss = 31.025043700066483\n",
      "Epoch 300: Training Loss = 31.016980992574283\n",
      "Epoch 310: Training Loss = 31.00739292147827\n",
      "Epoch 320: Training Loss = 30.998368197938298\n",
      "Epoch 330: Training Loss = 30.98690669666588\n",
      "Epoch 340: Training Loss = 30.97519789047818\n",
      "Epoch 350: Training Loss = 30.96172885032795\n",
      "Epoch 360: Training Loss = 30.95116709314755\n",
      "Epoch 370: Training Loss = 30.933551104455702\n",
      "Epoch 380: Training Loss = 30.91511234324667\n",
      "Epoch 390: Training Loss = 30.89483697100971\n",
      "Epoch 400: Training Loss = 30.883149377013762\n",
      "Epoch 410: Training Loss = 30.863779844383146\n",
      "Epoch 420: Training Loss = 30.84217632969431\n",
      "Epoch 430: Training Loss = 30.83316833792264\n",
      "Epoch 440: Training Loss = 30.815988648821932\n",
      "Epoch 450: Training Loss = 30.800611248904893\n",
      "Epoch 460: Training Loss = 30.798595645602866\n",
      "Epoch 470: Training Loss = 30.784307996395313\n",
      "Epoch 480: Training Loss = 30.778481604548997\n",
      "Epoch 490: Training Loss = 30.76541576845446\n",
      "Epoch 500: Training Loss = 30.7651192455336\n",
      "Epoch 510: Training Loss = 30.754231627952013\n",
      "Epoch 520: Training Loss = 30.74736309805451\n",
      "Epoch 530: Training Loss = 30.74757956474086\n",
      "Epoch 540: Training Loss = 30.746830776287254\n",
      "Epoch 550: Training Loss = 30.74101464052097\n",
      "Epoch 560: Training Loss = 30.73486637416126\n",
      "Epoch 570: Training Loss = 30.736435287895013\n",
      "Epoch 580: Training Loss = 30.731214757801276\n",
      "Epoch 590: Training Loss = 30.73513603927081\n",
      "Epoch 600: Training Loss = 30.72723397315022\n",
      "Epoch 610: Training Loss = 30.73334472901098\n",
      "Epoch 620: Training Loss = 30.719023681492743\n",
      "Epoch 630: Training Loss = 30.724484565689004\n",
      "Epoch 640: Training Loss = 30.725655891355284\n",
      "Epoch 650: Training Loss = 30.723902785265047\n",
      "Epoch 660: Training Loss = 30.72264780904416\n",
      "Epoch 670: Training Loss = 30.719563935039666\n",
      "Epoch 680: Training Loss = 30.71850054078747\n",
      "Epoch 690: Training Loss = 30.716214745776963\n",
      "Epoch 700: Training Loss = 30.710959656737916\n",
      "Epoch 710: Training Loss = 30.71522947927991\n",
      "Epoch 720: Training Loss = 30.71281662529706\n",
      "Epoch 730: Training Loss = 30.715199824620317\n",
      "Epoch 740: Training Loss = 30.711679466594575\n",
      "Epoch 750: Training Loss = 30.711117473657644\n",
      "Epoch 760: Training Loss = 30.70979026714556\n",
      "Epoch 770: Training Loss = 30.709407946102562\n",
      "Epoch 780: Training Loss = 30.70965726182212\n",
      "Epoch 790: Training Loss = 30.70697654065232\n",
      "Epoch 800: Training Loss = 30.70919578400588\n",
      "Epoch 810: Training Loss = 30.70923407238549\n",
      "Epoch 820: Training Loss = 30.70633194654725\n",
      "Epoch 830: Training Loss = 30.705653675255036\n",
      "Epoch 840: Training Loss = 30.70604091612228\n",
      "Epoch 850: Training Loss = 30.704045180129786\n",
      "Epoch 860: Training Loss = 30.705643318555133\n",
      "Epoch 870: Training Loss = 30.703550415387713\n",
      "Epoch 880: Training Loss = 30.705559330902645\n",
      "Epoch 890: Training Loss = 30.703303137204436\n",
      "Epoch 900: Training Loss = 30.702336525060854\n",
      "Epoch 910: Training Loss = 30.70480705033165\n",
      "Epoch 920: Training Loss = 30.703753547851846\n",
      "Epoch 930: Training Loss = 30.702282466977394\n",
      "Epoch 940: Training Loss = 30.700175053516514\n",
      "Epoch 950: Training Loss = 30.70007822718419\n",
      "Epoch 960: Training Loss = 30.700748921770558\n",
      "Epoch 970: Training Loss = 30.69922905309753\n",
      "Epoch 980: Training Loss = 30.702584934846044\n",
      "Epoch 990: Training Loss = 30.702041396020967\n",
      "Epoch 1000: Training Loss = 30.700035710822263\n",
      "Epoch 1010: Training Loss = 30.700542177800703\n",
      "Epoch 1020: Training Loss = 30.696866991847465\n",
      "Epoch 1030: Training Loss = 30.698782562768262\n",
      "Epoch 1040: Training Loss = 30.698629305048772\n",
      "Epoch 1050: Training Loss = 30.696988590216016\n",
      "Epoch 1060: Training Loss = 30.696690226854667\n",
      "Epoch 1070: Training Loss = 30.697544824824515\n",
      "Epoch 1080: Training Loss = 30.698012402733234\n",
      "Epoch 1090: Training Loss = 30.69587859745767\n",
      "Epoch 1100: Training Loss = 30.696741899704303\n",
      "Epoch 1110: Training Loss = 30.69796879859453\n",
      "Epoch 1120: Training Loss = 30.69760491686499\n",
      "Epoch 1130: Training Loss = 30.695606068683173\n",
      "Epoch 1140: Training Loss = 30.697506898562025\n",
      "Epoch 1150: Training Loss = 30.696383691855672\n",
      "Epoch 1160: Training Loss = 30.69631757331415\n",
      "Epoch 1170: Training Loss = 30.69725161523744\n",
      "Epoch 1180: Training Loss = 30.696823105738158\n",
      "Epoch 1190: Training Loss = 30.693797615943076\n",
      "Epoch 1200: Training Loss = 30.69486900220224\n",
      "Epoch 1210: Training Loss = 30.695292229558845\n",
      "Epoch 1220: Training Loss = 30.694890016256824\n",
      "Epoch 1230: Training Loss = 30.694816737826034\n",
      "Epoch 1240: Training Loss = 30.694982309587566\n",
      "Epoch 1250: Training Loss = 30.69706451589499\n",
      "Epoch 1260: Training Loss = 30.69319433316745\n",
      "Epoch 1270: Training Loss = 30.6967697455516\n",
      "Epoch 1280: Training Loss = 30.694483557045032\n",
      "Epoch 1290: Training Loss = 30.69373554174765\n",
      "Epoch 1300: Training Loss = 30.694546734027412\n",
      "Epoch 1310: Training Loss = 30.694563379511777\n",
      "Epoch 1320: Training Loss = 30.69370457386558\n",
      "Epoch 1330: Training Loss = 30.693946222348778\n",
      "Epoch 1340: Training Loss = 30.694740692516877\n",
      "Epoch 1350: Training Loss = 30.69246811395343\n",
      "Epoch 1360: Training Loss = 30.692739628205164\n",
      "Epoch 1370: Training Loss = 30.69233406821834\n",
      "Epoch 1380: Training Loss = 30.69226862049266\n",
      "Epoch 1390: Training Loss = 30.693143714246286\n",
      "Epoch 1400: Training Loss = 30.694397668599745\n",
      "Epoch 1410: Training Loss = 30.691963628327386\n",
      "Epoch 1420: Training Loss = 30.692872634532126\n",
      "Epoch 1430: Training Loss = 30.6935753481481\n",
      "Epoch 1440: Training Loss = 30.693526947863916\n",
      "Epoch 1450: Training Loss = 30.693417132777377\n",
      "Epoch 1460: Training Loss = 30.691902510141315\n",
      "Epoch 1470: Training Loss = 30.69192961846646\n",
      "Epoch 1480: Training Loss = 30.691959949601163\n",
      "Epoch 1490: Training Loss = 30.692031600300204\n",
      "Epoch 1500: Training Loss = 30.69230181100036\n",
      "Epoch 1510: Training Loss = 30.690965153915784\n",
      "Epoch 1520: Training Loss = 30.69166758125456\n",
      "Epoch 1530: Training Loss = 30.691790599755173\n",
      "Epoch 1540: Training Loss = 30.690359257602314\n",
      "Epoch 1550: Training Loss = 30.690608729284644\n",
      "Epoch 1560: Training Loss = 30.69119744696078\n",
      "Epoch 1570: Training Loss = 30.691701832665306\n",
      "Epoch 1580: Training Loss = 30.69125630050419\n",
      "Epoch 1590: Training Loss = 30.69023299222284\n",
      "Epoch 1600: Training Loss = 30.691006305097826\n",
      "Epoch 1610: Training Loss = 30.691055603537507\n",
      "Epoch 1620: Training Loss = 30.690507025698746\n",
      "Epoch 1630: Training Loss = 30.690601931089155\n",
      "Epoch 1640: Training Loss = 30.690277227061593\n",
      "Epoch 1650: Training Loss = 30.68995828703564\n",
      "Epoch 1660: Training Loss = 30.690929221779534\n",
      "Epoch 1670: Training Loss = 30.690127577445587\n",
      "Epoch 1680: Training Loss = 30.68993423521006\n",
      "Epoch 1690: Training Loss = 30.690493061269464\n",
      "Epoch 1700: Training Loss = 30.691074580797835\n",
      "Epoch 1710: Training Loss = 30.690216275493416\n",
      "Epoch 1720: Training Loss = 30.69058479453741\n",
      "Epoch 1730: Training Loss = 30.690764251412716\n",
      "Epoch 1740: Training Loss = 30.691093461517536\n",
      "Epoch 1750: Training Loss = 30.691190472913817\n",
      "Epoch 1760: Training Loss = 30.688030541206125\n",
      "Epoch 1770: Training Loss = 30.6903959016683\n",
      "Epoch 1780: Training Loss = 30.690530350492548\n",
      "Epoch 1790: Training Loss = 30.689848604862856\n",
      "Epoch 1800: Training Loss = 30.68928575291816\n",
      "Epoch 1810: Training Loss = 30.689524528539618\n",
      "Epoch 1820: Training Loss = 30.689198709538456\n",
      "Epoch 1830: Training Loss = 30.69002863106666\n",
      "Epoch 1840: Training Loss = 30.688903429915918\n",
      "Epoch 1850: Training Loss = 30.689229138622718\n",
      "Epoch 1860: Training Loss = 30.688907080773998\n",
      "Epoch 1870: Training Loss = 30.68931271496621\n",
      "Epoch 1880: Training Loss = 30.688978150765355\n",
      "Epoch 1890: Training Loss = 30.690338816993737\n",
      "Epoch 1900: Training Loss = 30.68928329011653\n",
      "Epoch 1910: Training Loss = 30.68953365652033\n",
      "Epoch 1920: Training Loss = 30.689548314195815\n",
      "Epoch 1930: Training Loss = 30.687226548533264\n",
      "Epoch 1940: Training Loss = 30.68932219888851\n",
      "Epoch 1950: Training Loss = 30.68939712845617\n",
      "Epoch 1960: Training Loss = 30.689073964919157\n",
      "Epoch 1970: Training Loss = 30.689001924063078\n",
      "Epoch 1980: Training Loss = 30.687386498928717\n",
      "Epoch 1990: Training Loss = 30.687421701471802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6]\n",
      "2\n",
      "(11, 6) (6, 6)\n",
      "Epoch 0: Training Loss = 25.24063372921671\n",
      "Epoch 10: Training Loss = 24.752366135283022\n",
      "Epoch 20: Training Loss = 24.362426453930247\n",
      "Epoch 30: Training Loss = 24.278306690411654\n",
      "Epoch 40: Training Loss = 24.191775793194015\n",
      "Epoch 50: Training Loss = 24.154932338766535\n",
      "Epoch 60: Training Loss = 24.123589521848885\n",
      "Epoch 70: Training Loss = 24.096354497488694\n",
      "Epoch 80: Training Loss = 24.061488583883158\n",
      "Epoch 90: Training Loss = 24.06957435797346\n",
      "Epoch 100: Training Loss = 24.063986543819126\n",
      "Epoch 110: Training Loss = 24.041220057708124\n",
      "Epoch 120: Training Loss = 24.046251719845255\n",
      "Epoch 130: Training Loss = 24.03605369211873\n",
      "Epoch 140: Training Loss = 24.02925027581021\n",
      "Epoch 150: Training Loss = 24.028046705205384\n",
      "Epoch 160: Training Loss = 24.016997531402566\n",
      "Epoch 170: Training Loss = 24.0129284293151\n",
      "Epoch 180: Training Loss = 24.02173087426787\n",
      "Epoch 190: Training Loss = 24.018514336963268\n",
      "Epoch 200: Training Loss = 24.017609212650562\n",
      "Epoch 210: Training Loss = 24.024159814204435\n",
      "Epoch 220: Training Loss = 24.01540834875035\n",
      "Epoch 230: Training Loss = 24.009253485911238\n",
      "Epoch 240: Training Loss = 24.00761747946378\n",
      "Epoch 250: Training Loss = 24.012313266063916\n",
      "Epoch 260: Training Loss = 24.007419931752285\n",
      "Epoch 270: Training Loss = 24.005374403851594\n",
      "Epoch 280: Training Loss = 24.009846269986774\n",
      "Epoch 290: Training Loss = 24.010031965097827\n",
      "Epoch 300: Training Loss = 24.00391260736071\n",
      "Epoch 310: Training Loss = 24.002307947212877\n",
      "Epoch 320: Training Loss = 24.008532117182863\n",
      "Epoch 330: Training Loss = 24.00217874374671\n",
      "Epoch 340: Training Loss = 23.9996256289786\n",
      "Epoch 350: Training Loss = 24.00715173335112\n",
      "Epoch 360: Training Loss = 24.003294144518875\n",
      "Epoch 370: Training Loss = 24.001786524493433\n",
      "Epoch 380: Training Loss = 23.996859924251627\n",
      "Epoch 390: Training Loss = 23.999510306122126\n",
      "Epoch 400: Training Loss = 24.001055313680375\n",
      "Epoch 410: Training Loss = 23.998216621771324\n",
      "Epoch 420: Training Loss = 23.99863276905389\n",
      "Epoch 430: Training Loss = 24.002848886668136\n",
      "Epoch 440: Training Loss = 23.994845827761637\n",
      "Epoch 450: Training Loss = 23.998406126839203\n",
      "Epoch 460: Training Loss = 23.998672684743212\n",
      "Epoch 470: Training Loss = 23.99747618634743\n",
      "Epoch 480: Training Loss = 23.99717154004999\n",
      "Epoch 490: Training Loss = 23.997504593900164\n",
      "Epoch 500: Training Loss = 23.999683059026598\n",
      "Epoch 510: Training Loss = 23.993610241614782\n",
      "Epoch 520: Training Loss = 23.99627036503753\n",
      "Epoch 530: Training Loss = 23.994212854054304\n",
      "Epoch 540: Training Loss = 23.99616021442416\n",
      "Epoch 550: Training Loss = 23.993188464145096\n",
      "Epoch 560: Training Loss = 23.9952233946952\n",
      "Epoch 570: Training Loss = 23.99612936477989\n",
      "Epoch 580: Training Loss = 23.995180388718722\n",
      "Epoch 590: Training Loss = 23.992921885032754\n",
      "Epoch 600: Training Loss = 23.991316554581847\n",
      "Epoch 610: Training Loss = 23.995702973482015\n",
      "Epoch 620: Training Loss = 23.992772385136668\n",
      "Epoch 630: Training Loss = 23.991396471765313\n",
      "Epoch 640: Training Loss = 23.998276712078905\n",
      "Epoch 650: Training Loss = 23.99164495763657\n",
      "Epoch 660: Training Loss = 23.99134857590561\n",
      "Epoch 670: Training Loss = 23.994642010360927\n",
      "Epoch 680: Training Loss = 23.992854688899357\n",
      "Epoch 690: Training Loss = 23.993758840414916\n",
      "Epoch 700: Training Loss = 23.992179702319095\n",
      "Epoch 710: Training Loss = 23.987715290889263\n",
      "Epoch 720: Training Loss = 23.992105226075612\n",
      "Epoch 730: Training Loss = 23.99121597162444\n",
      "Epoch 740: Training Loss = 23.9899788055939\n",
      "Epoch 750: Training Loss = 23.993912432866498\n",
      "Epoch 760: Training Loss = 23.99303727197061\n",
      "Epoch 770: Training Loss = 23.993284883863723\n",
      "Epoch 780: Training Loss = 23.98972153549222\n",
      "Epoch 790: Training Loss = 23.99398381282098\n",
      "Epoch 800: Training Loss = 23.993182645109663\n",
      "Epoch 810: Training Loss = 23.992845055582027\n",
      "Epoch 820: Training Loss = 23.990106015118435\n",
      "Epoch 830: Training Loss = 23.995888698219044\n",
      "Epoch 840: Training Loss = 23.989383278776547\n",
      "Epoch 850: Training Loss = 23.989948597655296\n",
      "Epoch 860: Training Loss = 23.992052564288272\n",
      "Epoch 870: Training Loss = 23.99084390143931\n",
      "Epoch 880: Training Loss = 23.992485710005248\n",
      "Epoch 890: Training Loss = 23.99285845489266\n",
      "Epoch 900: Training Loss = 23.987942936977195\n",
      "Epoch 910: Training Loss = 23.985553832861537\n",
      "Epoch 920: Training Loss = 23.99007514132943\n",
      "Epoch 930: Training Loss = 23.99166747637196\n",
      "Epoch 940: Training Loss = 23.993084777356568\n",
      "Epoch 950: Training Loss = 23.992657513588647\n",
      "Epoch 960: Training Loss = 23.98753927407096\n",
      "Epoch 970: Training Loss = 23.989919901755915\n",
      "Epoch 980: Training Loss = 23.99134380314814\n",
      "Epoch 990: Training Loss = 23.98731229101942\n",
      "Epoch 1000: Training Loss = 23.993569010527644\n",
      "Epoch 1010: Training Loss = 23.98913529533358\n",
      "Epoch 1020: Training Loss = 23.992811133878064\n",
      "Epoch 1030: Training Loss = 23.99052437566731\n",
      "Epoch 1040: Training Loss = 23.99087371227731\n",
      "Epoch 1050: Training Loss = 23.98707579051827\n",
      "Epoch 1060: Training Loss = 23.98850786512238\n",
      "Epoch 1070: Training Loss = 23.986263769377473\n",
      "Epoch 1080: Training Loss = 23.990221586958043\n",
      "Epoch 1090: Training Loss = 23.98769123936386\n",
      "Epoch 1100: Training Loss = 23.986367609072257\n",
      "Epoch 1110: Training Loss = 23.990911578709277\n",
      "Epoch 1120: Training Loss = 23.989710956244313\n",
      "Epoch 1130: Training Loss = 23.989316562752187\n",
      "Epoch 1140: Training Loss = 23.98999153563237\n",
      "Epoch 1150: Training Loss = 23.98676564131016\n",
      "Epoch 1160: Training Loss = 23.991666095236724\n",
      "Epoch 1170: Training Loss = 23.988464650939395\n",
      "Epoch 1180: Training Loss = 23.99053092951847\n",
      "Epoch 1190: Training Loss = 23.989045605342174\n",
      "Epoch 1200: Training Loss = 23.99127519465634\n",
      "Epoch 1210: Training Loss = 23.988494640904328\n",
      "Epoch 1220: Training Loss = 23.990758144939022\n",
      "Epoch 1230: Training Loss = 23.98705464336666\n",
      "Epoch 1240: Training Loss = 23.988295269898114\n",
      "Epoch 1250: Training Loss = 23.990615872955594\n",
      "Epoch 1260: Training Loss = 23.98915646842273\n",
      "Epoch 1270: Training Loss = 23.98521668143075\n",
      "Epoch 1280: Training Loss = 23.989974849952034\n",
      "Epoch 1290: Training Loss = 23.98768196878549\n",
      "Epoch 1300: Training Loss = 23.986452263365116\n",
      "Epoch 1310: Training Loss = 23.9885589456054\n",
      "Epoch 1320: Training Loss = 23.98854339335838\n",
      "Epoch 1330: Training Loss = 23.987266347266832\n",
      "Epoch 1340: Training Loss = 23.98615981282285\n",
      "Epoch 1350: Training Loss = 23.987713720766926\n",
      "Epoch 1360: Training Loss = 23.98789813651771\n",
      "Epoch 1370: Training Loss = 23.985609077143163\n",
      "Epoch 1380: Training Loss = 23.9878896509095\n",
      "Epoch 1390: Training Loss = 23.988617846995705\n",
      "Epoch 1400: Training Loss = 23.988252412298444\n",
      "Epoch 1410: Training Loss = 23.988614593682883\n",
      "Epoch 1420: Training Loss = 23.988115024009787\n",
      "Epoch 1430: Training Loss = 23.990051040106483\n",
      "Epoch 1440: Training Loss = 23.986773759908395\n",
      "Epoch 1450: Training Loss = 23.987651198366734\n",
      "Epoch 1460: Training Loss = 23.98690808517196\n",
      "Epoch 1470: Training Loss = 23.984772786717816\n",
      "Epoch 1480: Training Loss = 23.989514810919104\n",
      "Epoch 1490: Training Loss = 23.986995079231896\n",
      "Epoch 1500: Training Loss = 23.9868632356562\n",
      "Epoch 1510: Training Loss = 23.989172692597112\n",
      "Epoch 1520: Training Loss = 23.9870316178564\n",
      "Epoch 1530: Training Loss = 23.988780564992364\n",
      "Epoch 1540: Training Loss = 23.98727580033195\n",
      "Epoch 1550: Training Loss = 23.9881162560941\n",
      "Epoch 1560: Training Loss = 23.98782304986987\n",
      "Epoch 1570: Training Loss = 23.98757047787962\n",
      "Epoch 1580: Training Loss = 23.991413769420856\n",
      "Epoch 1590: Training Loss = 23.985057161568417\n",
      "Epoch 1600: Training Loss = 23.98870204645389\n",
      "Epoch 1610: Training Loss = 23.987944452359855\n",
      "Epoch 1620: Training Loss = 23.987800465365893\n",
      "Epoch 1630: Training Loss = 23.988210888312068\n",
      "Epoch 1640: Training Loss = 23.988906200987365\n",
      "Epoch 1650: Training Loss = 23.987775573599254\n",
      "Epoch 1660: Training Loss = 23.987243371969228\n",
      "Epoch 1670: Training Loss = 23.984854834955442\n",
      "Epoch 1680: Training Loss = 23.990084644713345\n",
      "Epoch 1690: Training Loss = 23.985828285321702\n",
      "Epoch 1700: Training Loss = 23.98721064379719\n",
      "Epoch 1710: Training Loss = 23.986771072288633\n",
      "Epoch 1720: Training Loss = 23.987589270954754\n",
      "Epoch 1730: Training Loss = 23.99040813907885\n",
      "Epoch 1740: Training Loss = 23.987689260673037\n",
      "Epoch 1750: Training Loss = 23.98616154067434\n",
      "Epoch 1760: Training Loss = 23.987396433334276\n",
      "Epoch 1770: Training Loss = 23.98755806249291\n",
      "Epoch 1780: Training Loss = 23.988707477498348\n",
      "Epoch 1790: Training Loss = 23.98657192770158\n",
      "Epoch 1800: Training Loss = 23.987928688633883\n",
      "Epoch 1810: Training Loss = 23.98898750953143\n",
      "Epoch 1820: Training Loss = 23.98689816000098\n",
      "Epoch 1830: Training Loss = 23.988112623029828\n",
      "Epoch 1840: Training Loss = 23.987952648302425\n",
      "Epoch 1850: Training Loss = 23.987443427563743\n",
      "Epoch 1860: Training Loss = 23.987567480065973\n",
      "Epoch 1870: Training Loss = 23.98761176039294\n",
      "Epoch 1880: Training Loss = 23.987921168643712\n",
      "Epoch 1890: Training Loss = 23.98762459109297\n",
      "Epoch 1900: Training Loss = 23.989010740209924\n",
      "Epoch 1910: Training Loss = 23.986956943769382\n",
      "Epoch 1920: Training Loss = 23.987239773661315\n",
      "Epoch 1930: Training Loss = 23.986999468910504\n",
      "Epoch 1940: Training Loss = 23.987599447910313\n",
      "Epoch 1950: Training Loss = 23.987155367820378\n",
      "Epoch 1960: Training Loss = 23.98724815605977\n",
      "Epoch 1970: Training Loss = 23.984887980078835\n",
      "Epoch 1980: Training Loss = 23.987528195816665\n",
      "Epoch 1990: Training Loss = 23.986102252145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 1648.5277422333347\n",
      "Epoch 10: Training Loss = 1638.829984776276\n",
      "Epoch 20: Training Loss = 1629.783886803288\n",
      "Epoch 30: Training Loss = 1620.6167147274816\n",
      "Epoch 40: Training Loss = 1611.8026689168903\n",
      "Epoch 50: Training Loss = 1604.3307990282703\n",
      "Epoch 60: Training Loss = 1598.202200863554\n",
      "Epoch 70: Training Loss = 1592.6466432844707\n",
      "Epoch 80: Training Loss = 1587.6298026664213\n",
      "Epoch 90: Training Loss = 1583.635370303432\n",
      "Epoch 100: Training Loss = 1580.746134223758\n",
      "Epoch 110: Training Loss = 1578.6494376259018\n",
      "Epoch 120: Training Loss = 1577.025006087081\n",
      "Epoch 130: Training Loss = 1575.6635272620567\n",
      "Epoch 140: Training Loss = 1574.444131211677\n",
      "Epoch 150: Training Loss = 1573.3010715097257\n",
      "Epoch 160: Training Loss = 1572.203189997367\n",
      "Epoch 170: Training Loss = 1571.142214232776\n",
      "Epoch 180: Training Loss = 1570.1246198995036\n",
      "Epoch 190: Training Loss = 1569.1643717816135\n",
      "Epoch 200: Training Loss = 1568.2763132321013\n",
      "Epoch 210: Training Loss = 1567.4712957826189\n",
      "Epoch 220: Training Loss = 1566.7539405654588\n",
      "Epoch 230: Training Loss = 1566.122842591238\n",
      "Epoch 240: Training Loss = 1565.5721947470474\n",
      "Epoch 250: Training Loss = 1565.0937393981753\n",
      "Epoch 260: Training Loss = 1564.6783799009743\n",
      "Epoch 270: Training Loss = 1564.3172418976592\n",
      "Epoch 280: Training Loss = 1564.002243485091\n",
      "Epoch 290: Training Loss = 1563.7263222954614\n",
      "Epoch 300: Training Loss = 1563.4834591871677\n",
      "Epoch 310: Training Loss = 1563.2685987542568\n",
      "Epoch 320: Training Loss = 1563.0775283446283\n",
      "Epoch 330: Training Loss = 1562.9067493819769\n",
      "Epoch 340: Training Loss = 1562.753357346378\n",
      "Epoch 350: Training Loss = 1562.614936890153\n",
      "Epoch 360: Training Loss = 1562.4894734352454\n",
      "Epoch 370: Training Loss = 1562.3752802006823\n",
      "Epoch 380: Training Loss = 1562.270938679465\n",
      "Epoch 390: Training Loss = 1562.1752503909515\n",
      "Epoch 400: Training Loss = 1562.087197876685\n",
      "Epoch 410: Training Loss = 1562.005913176166\n",
      "Epoch 420: Training Loss = 1561.9306523123212\n",
      "Epoch 430: Training Loss = 1561.860774589111\n",
      "Epoch 440: Training Loss = 1561.7957257389032\n",
      "Epoch 450: Training Loss = 1561.7350241521515\n",
      "Epoch 460: Training Loss = 1561.6782495799657\n",
      "Epoch 470: Training Loss = 1561.625033826026\n",
      "Epoch 480: Training Loss = 1561.5750530443017\n",
      "Epoch 490: Training Loss = 1561.5280213376864\n",
      "Epoch 500: Training Loss = 1561.4836854148175\n",
      "Epoch 510: Training Loss = 1561.4418201109916\n",
      "Epoch 520: Training Loss = 1561.4022246177205\n",
      "Epoch 530: Training Loss = 1561.3647192958103\n",
      "Epoch 540: Training Loss = 1561.3291429709045\n",
      "Epoch 550: Training Loss = 1561.295350629596\n",
      "Epoch 560: Training Loss = 1561.2632114494215\n",
      "Epoch 570: Training Loss = 1561.2326071082166\n",
      "Epoch 580: Training Loss = 1561.203430328201\n",
      "Epoch 590: Training Loss = 1561.1755836179118\n",
      "Epoch 600: Training Loss = 1561.1489781816108\n",
      "Epoch 610: Training Loss = 1561.123532970822\n",
      "Epoch 620: Training Loss = 1561.0991738570797\n",
      "Epoch 630: Training Loss = 1561.0758329082003\n",
      "Epoch 640: Training Loss = 1561.0534477534038\n",
      "Epoch 650: Training Loss = 1561.031961024809\n",
      "Epoch 660: Training Loss = 1561.01131986491\n",
      "Epoch 670: Training Loss = 1560.9914754910453\n",
      "Epoch 680: Training Loss = 1560.9723828094104\n",
      "Epoch 690: Training Loss = 1560.9540000721217\n",
      "Epoch 700: Training Loss = 1560.9362885718895\n",
      "Epoch 710: Training Loss = 1560.9192123695195\n",
      "Epoch 720: Training Loss = 1560.9027380502614\n",
      "Epoch 730: Training Loss = 1560.886834505474\n",
      "Epoch 740: Training Loss = 1560.8714727365639\n",
      "Epoch 750: Training Loss = 1560.8566256786905\n",
      "Epoch 760: Training Loss = 1560.8422680418134\n",
      "Epoch 770: Training Loss = 1560.828376167225\n",
      "Epoch 780: Training Loss = 1560.8149278978162\n",
      "Epoch 790: Training Loss = 1560.8019024605285\n",
      "Epoch 800: Training Loss = 1560.7892803597133\n",
      "Epoch 810: Training Loss = 1560.7770432802095\n",
      "Epoch 820: Training Loss = 1560.7651739991586\n",
      "Epoch 830: Training Loss = 1560.753656305581\n",
      "Epoch 840: Training Loss = 1560.7424749270222\n",
      "Epoch 850: Training Loss = 1560.7316154624475\n",
      "Epoch 860: Training Loss = 1560.7210643208832\n",
      "Epoch 870: Training Loss = 1560.7108086651274\n",
      "Epoch 880: Training Loss = 1560.7008363601396\n",
      "Epoch 890: Training Loss = 1560.6911359255967\n",
      "Epoch 900: Training Loss = 1560.681696492268\n",
      "Epoch 910: Training Loss = 1560.6725077618316\n",
      "Epoch 920: Training Loss = 1560.6635599698366\n",
      "Epoch 930: Training Loss = 1560.654843851498\n",
      "Epoch 940: Training Loss = 1560.6463506101213\n",
      "Epoch 950: Training Loss = 1560.638071887859\n",
      "Epoch 960: Training Loss = 1560.6299997386843\n",
      "Epoch 970: Training Loss = 1560.6221266032892\n",
      "Epoch 980: Training Loss = 1560.6144452858548\n",
      "Epoch 990: Training Loss = 1560.6069489324084\n",
      "Epoch 1000: Training Loss = 1560.5996310108058\n",
      "Epoch 1010: Training Loss = 1560.5924852919995\n",
      "Epoch 1020: Training Loss = 1560.5855058326908\n",
      "Epoch 1030: Training Loss = 1560.5786869591257\n",
      "Epoch 1040: Training Loss = 1560.5720232519557\n",
      "Epoch 1050: Training Loss = 1560.5655095321754\n",
      "Epoch 1060: Training Loss = 1560.5591408479117\n",
      "Epoch 1070: Training Loss = 1560.552912462135\n",
      "Epoch 1080: Training Loss = 1560.546819841117\n",
      "Epoch 1090: Training Loss = 1560.54085864365\n",
      "Epoch 1100: Training Loss = 1560.5350247109563\n",
      "Epoch 1110: Training Loss = 1560.5293140571753\n",
      "Epoch 1120: Training Loss = 1560.5237228605072\n",
      "Epoch 1130: Training Loss = 1560.5182474548485\n",
      "Epoch 1140: Training Loss = 1560.5128843219427\n",
      "Epoch 1150: Training Loss = 1560.5076300840349\n",
      "Epoch 1160: Training Loss = 1560.502481496898\n",
      "Epoch 1170: Training Loss = 1560.4974354433277\n",
      "Epoch 1180: Training Loss = 1560.4924889270044\n",
      "Epoch 1190: Training Loss = 1560.4876390666602\n",
      "Epoch 1200: Training Loss = 1560.4828830906492\n",
      "Epoch 1210: Training Loss = 1560.4782183317748\n",
      "Epoch 1220: Training Loss = 1560.4736422224266\n",
      "Epoch 1230: Training Loss = 1560.4691522899811\n",
      "Epoch 1240: Training Loss = 1560.4647461524569\n",
      "Epoch 1250: Training Loss = 1560.4604215143972\n",
      "Epoch 1260: Training Loss = 1560.4561761630011\n",
      "Epoch 1270: Training Loss = 1560.4520079644337\n",
      "Epoch 1280: Training Loss = 1560.4479148603298\n",
      "Epoch 1290: Training Loss = 1560.443894864513\n",
      "Epoch 1300: Training Loss = 1560.4399460598454\n",
      "Epoch 1310: Training Loss = 1560.436066595281\n",
      "Epoch 1320: Training Loss = 1560.4322546830258\n",
      "Epoch 1330: Training Loss = 1560.4285085958863\n",
      "Epoch 1340: Training Loss = 1560.4248266646978\n",
      "Epoch 1350: Training Loss = 1560.4212072759535\n",
      "Epoch 1360: Training Loss = 1560.4176488694814\n",
      "Epoch 1370: Training Loss = 1560.4141499362634\n",
      "Epoch 1380: Training Loss = 1560.410709016379\n",
      "Epoch 1390: Training Loss = 1560.4073246970156\n",
      "Epoch 1400: Training Loss = 1560.403995610592\n",
      "Epoch 1410: Training Loss = 1560.4007204329641\n",
      "Epoch 1420: Training Loss = 1560.3974978817225\n",
      "Epoch 1430: Training Loss = 1560.394326714566\n",
      "Epoch 1440: Training Loss = 1560.3912057277464\n",
      "Epoch 1450: Training Loss = 1560.3881337545965\n",
      "Epoch 1460: Training Loss = 1560.3851096641017\n",
      "Epoch 1470: Training Loss = 1560.382132359565\n",
      "Epoch 1480: Training Loss = 1560.3792007773193\n",
      "Epoch 1490: Training Loss = 1560.3763138854938\n",
      "Epoch 1500: Training Loss = 1560.373470682828\n",
      "Epoch 1510: Training Loss = 1560.370670197569\n",
      "Epoch 1520: Training Loss = 1560.3679114863871\n",
      "Epoch 1530: Training Loss = 1560.3651936333274\n",
      "Epoch 1540: Training Loss = 1560.3625157488652\n",
      "Epoch 1550: Training Loss = 1560.359876968932\n",
      "Epoch 1560: Training Loss = 1560.3572764540202\n",
      "Epoch 1570: Training Loss = 1560.3547133883392\n",
      "Epoch 1580: Training Loss = 1560.3521869789543\n",
      "Epoch 1590: Training Loss = 1560.349696455019\n",
      "Epoch 1600: Training Loss = 1560.3472410670054\n",
      "Epoch 1610: Training Loss = 1560.3448200859768\n",
      "Epoch 1620: Training Loss = 1560.3424328028848\n",
      "Epoch 1630: Training Loss = 1560.3400785278927\n",
      "Epoch 1640: Training Loss = 1560.337756589753\n",
      "Epoch 1650: Training Loss = 1560.3354663351552\n",
      "Epoch 1660: Training Loss = 1560.3332071281702\n",
      "Epoch 1670: Training Loss = 1560.3309783496375\n",
      "Epoch 1680: Training Loss = 1560.3287793966574\n",
      "Epoch 1690: Training Loss = 1560.3266096820334\n",
      "Epoch 1700: Training Loss = 1560.324468633777\n",
      "Epoch 1710: Training Loss = 1560.3223556946268\n",
      "Epoch 1720: Training Loss = 1560.320270321569\n",
      "Epoch 1730: Training Loss = 1560.3182119853811\n",
      "Epoch 1740: Training Loss = 1560.3161801702317\n",
      "Epoch 1750: Training Loss = 1560.3141743732067\n",
      "Epoch 1760: Training Loss = 1560.3121941039672\n",
      "Epoch 1770: Training Loss = 1560.3102388843154\n",
      "Epoch 1780: Training Loss = 1560.30830824785\n",
      "Epoch 1790: Training Loss = 1560.3064017395848\n",
      "Epoch 1800: Training Loss = 1560.3045189156267\n",
      "Epoch 1810: Training Loss = 1560.3026593428176\n",
      "Epoch 1820: Training Loss = 1560.3008225984368\n",
      "Epoch 1830: Training Loss = 1560.29900826986\n",
      "Epoch 1840: Training Loss = 1560.2972159542942\n",
      "Epoch 1850: Training Loss = 1560.295445258464\n",
      "Epoch 1860: Training Loss = 1560.2936957983372\n",
      "Epoch 1870: Training Loss = 1560.2919671988711\n",
      "Epoch 1880: Training Loss = 1560.290259093721\n",
      "Epoch 1890: Training Loss = 1560.2885711250333\n",
      "Epoch 1900: Training Loss = 1560.2869029431615\n",
      "Epoch 1910: Training Loss = 1560.285254206458\n",
      "Epoch 1920: Training Loss = 1560.2836245810315\n",
      "Epoch 1930: Training Loss = 1560.2820137405529\n",
      "Epoch 1940: Training Loss = 1560.2804213660097\n",
      "Epoch 1950: Training Loss = 1560.2788471455394\n",
      "Epoch 1960: Training Loss = 1560.2772907741955\n",
      "Epoch 1970: Training Loss = 1560.2757519537845\n",
      "Epoch 1980: Training Loss = 1560.2742303926682\n",
      "Epoch 1990: Training Loss = 1560.272725805597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 32.62613826994531\n",
      "Epoch 10: Training Loss = 32.31547811064368\n",
      "Epoch 20: Training Loss = 32.05822066154412\n",
      "Epoch 30: Training Loss = 31.738030039237085\n",
      "Epoch 40: Training Loss = 31.37818706352431\n",
      "Epoch 50: Training Loss = 31.164127914354754\n",
      "Epoch 60: Training Loss = 31.009669594252813\n",
      "Epoch 70: Training Loss = 30.932632326873435\n",
      "Epoch 80: Training Loss = 30.875275903338473\n",
      "Epoch 90: Training Loss = 30.841702084622053\n",
      "Epoch 100: Training Loss = 30.814628887196605\n",
      "Epoch 110: Training Loss = 30.797761832883726\n",
      "Epoch 120: Training Loss = 30.78068243235546\n",
      "Epoch 130: Training Loss = 30.774947403130287\n",
      "Epoch 140: Training Loss = 30.759164323794835\n",
      "Epoch 150: Training Loss = 30.75438229993483\n",
      "Epoch 160: Training Loss = 30.748204509499526\n",
      "Epoch 170: Training Loss = 30.74289057125391\n",
      "Epoch 180: Training Loss = 30.738511479329315\n",
      "Epoch 190: Training Loss = 30.732761608517976\n",
      "Epoch 200: Training Loss = 30.730211679778005\n",
      "Epoch 210: Training Loss = 30.726644736009433\n",
      "Epoch 220: Training Loss = 30.72630324141124\n",
      "Epoch 230: Training Loss = 30.7224221111424\n",
      "Epoch 240: Training Loss = 30.718815740899228\n",
      "Epoch 250: Training Loss = 30.718632228073137\n",
      "Epoch 260: Training Loss = 30.71669105342619\n",
      "Epoch 270: Training Loss = 30.714713184951222\n",
      "Epoch 280: Training Loss = 30.71291998908814\n",
      "Epoch 290: Training Loss = 30.711808747925183\n",
      "Epoch 300: Training Loss = 30.709578114039534\n",
      "Epoch 310: Training Loss = 30.70984744537332\n",
      "Epoch 320: Training Loss = 30.707536465600548\n",
      "Epoch 330: Training Loss = 30.70762803943817\n",
      "Epoch 340: Training Loss = 30.706354638598338\n",
      "Epoch 350: Training Loss = 30.705315683380125\n",
      "Epoch 360: Training Loss = 30.704151278618074\n",
      "Epoch 370: Training Loss = 30.704509517430527\n",
      "Epoch 380: Training Loss = 30.703832636130826\n",
      "Epoch 390: Training Loss = 30.703475020314208\n",
      "Epoch 400: Training Loss = 30.70160432449361\n",
      "Epoch 410: Training Loss = 30.700931740882805\n",
      "Epoch 420: Training Loss = 30.70066115280044\n",
      "Epoch 430: Training Loss = 30.700615354840888\n",
      "Epoch 440: Training Loss = 30.700359394229018\n",
      "Epoch 450: Training Loss = 30.699493387759862\n",
      "Epoch 460: Training Loss = 30.69848401924076\n",
      "Epoch 470: Training Loss = 30.698246013495854\n",
      "Epoch 480: Training Loss = 30.69808687083009\n",
      "Epoch 490: Training Loss = 30.69760205270143\n",
      "Epoch 500: Training Loss = 30.69714404730011\n",
      "Epoch 510: Training Loss = 30.696563886350305\n",
      "Epoch 520: Training Loss = 30.696248323290625\n",
      "Epoch 530: Training Loss = 30.696231414269437\n",
      "Epoch 540: Training Loss = 30.69561665735918\n",
      "Epoch 550: Training Loss = 30.695359316299452\n",
      "Epoch 560: Training Loss = 30.695309669683834\n",
      "Epoch 570: Training Loss = 30.694967129956684\n",
      "Epoch 580: Training Loss = 30.694431770874868\n",
      "Epoch 590: Training Loss = 30.694280879516022\n",
      "Epoch 600: Training Loss = 30.694188102978316\n",
      "Epoch 610: Training Loss = 30.693855854210604\n",
      "Epoch 620: Training Loss = 30.693562731846264\n",
      "Epoch 630: Training Loss = 30.6934716105351\n",
      "Epoch 640: Training Loss = 30.693260508949567\n",
      "Epoch 650: Training Loss = 30.693241754718994\n",
      "Epoch 660: Training Loss = 30.693139553360407\n",
      "Epoch 670: Training Loss = 30.692732590295627\n",
      "Epoch 680: Training Loss = 30.692979765416997\n",
      "Epoch 690: Training Loss = 30.692492885107804\n",
      "Epoch 700: Training Loss = 30.692113940167904\n",
      "Epoch 710: Training Loss = 30.692489624766264\n",
      "Epoch 720: Training Loss = 30.691723870161486\n",
      "Epoch 730: Training Loss = 30.691704074234824\n",
      "Epoch 740: Training Loss = 30.691629692643676\n",
      "Epoch 750: Training Loss = 30.69154795992807\n",
      "Epoch 760: Training Loss = 30.691304701468923\n",
      "Epoch 770: Training Loss = 30.69116301513018\n",
      "Epoch 780: Training Loss = 30.6911660314484\n",
      "Epoch 790: Training Loss = 30.690574385488038\n",
      "Epoch 800: Training Loss = 30.690872225229477\n",
      "Epoch 810: Training Loss = 30.690267613958984\n",
      "Epoch 820: Training Loss = 30.690365148676882\n",
      "Epoch 830: Training Loss = 30.690533540611295\n",
      "Epoch 840: Training Loss = 30.690137204332228\n",
      "Epoch 850: Training Loss = 30.690113676011713\n",
      "Epoch 860: Training Loss = 30.689960712153432\n",
      "Epoch 870: Training Loss = 30.68997668770925\n",
      "Epoch 880: Training Loss = 30.68966857954476\n",
      "Epoch 890: Training Loss = 30.689428450551347\n",
      "Epoch 900: Training Loss = 30.689551427022085\n",
      "Epoch 910: Training Loss = 30.68939015547235\n",
      "Epoch 920: Training Loss = 30.689500903777308\n",
      "Epoch 930: Training Loss = 30.689240709097064\n",
      "Epoch 940: Training Loss = 30.6892903479659\n",
      "Epoch 950: Training Loss = 30.68896752230345\n",
      "Epoch 960: Training Loss = 30.689001790372636\n",
      "Epoch 970: Training Loss = 30.68884978026358\n",
      "Epoch 980: Training Loss = 30.68885820600979\n",
      "Epoch 990: Training Loss = 30.68876120179688\n",
      "Epoch 1000: Training Loss = 30.688635845936194\n",
      "Epoch 1010: Training Loss = 30.688717881120102\n",
      "Epoch 1020: Training Loss = 30.688587816147326\n",
      "Epoch 1030: Training Loss = 30.688539511215385\n",
      "Epoch 1040: Training Loss = 30.68806095597688\n",
      "Epoch 1050: Training Loss = 30.688164178006257\n",
      "Epoch 1060: Training Loss = 30.688078477149766\n",
      "Epoch 1070: Training Loss = 30.68783205581921\n",
      "Epoch 1080: Training Loss = 30.688165953736434\n",
      "Epoch 1090: Training Loss = 30.687826216948316\n",
      "Epoch 1100: Training Loss = 30.688042547559185\n",
      "Epoch 1110: Training Loss = 30.68770187523757\n",
      "Epoch 1120: Training Loss = 30.68775084741691\n",
      "Epoch 1130: Training Loss = 30.687747162578386\n",
      "Epoch 1140: Training Loss = 30.687676190787894\n",
      "Epoch 1150: Training Loss = 30.68750603230184\n",
      "Epoch 1160: Training Loss = 30.687539981388717\n",
      "Epoch 1170: Training Loss = 30.687362467822044\n",
      "Epoch 1180: Training Loss = 30.687202215593725\n",
      "Epoch 1190: Training Loss = 30.68707626936593\n",
      "Epoch 1200: Training Loss = 30.687270137708317\n",
      "Epoch 1210: Training Loss = 30.68748322157129\n",
      "Epoch 1220: Training Loss = 30.6869590370868\n",
      "Epoch 1230: Training Loss = 30.687082735185903\n",
      "Epoch 1240: Training Loss = 30.687126632868686\n",
      "Epoch 1250: Training Loss = 30.686833796931353\n",
      "Epoch 1260: Training Loss = 30.686995679113753\n",
      "Epoch 1270: Training Loss = 30.686788855690672\n",
      "Epoch 1280: Training Loss = 30.686942853240357\n",
      "Epoch 1290: Training Loss = 30.686720029808114\n",
      "Epoch 1300: Training Loss = 30.686706017022196\n",
      "Epoch 1310: Training Loss = 30.68676176646742\n",
      "Epoch 1320: Training Loss = 30.68655595114798\n",
      "Epoch 1330: Training Loss = 30.68660062730179\n",
      "Epoch 1340: Training Loss = 30.68656272793239\n",
      "Epoch 1350: Training Loss = 30.6864600859647\n",
      "Epoch 1360: Training Loss = 30.686582820227883\n",
      "Epoch 1370: Training Loss = 30.686353050083635\n",
      "Epoch 1380: Training Loss = 30.68629883762819\n",
      "Epoch 1390: Training Loss = 30.686356481438583\n",
      "Epoch 1400: Training Loss = 30.686547903341566\n",
      "Epoch 1410: Training Loss = 30.686289888895033\n",
      "Epoch 1420: Training Loss = 30.686304552166103\n",
      "Epoch 1430: Training Loss = 30.686147100311942\n",
      "Epoch 1440: Training Loss = 30.686076879438627\n",
      "Epoch 1450: Training Loss = 30.686101901875478\n",
      "Epoch 1460: Training Loss = 30.68623425586625\n",
      "Epoch 1470: Training Loss = 30.68599623783138\n",
      "Epoch 1480: Training Loss = 30.686134403281574\n",
      "Epoch 1490: Training Loss = 30.686054920688274\n",
      "Epoch 1500: Training Loss = 30.686014927985514\n",
      "Epoch 1510: Training Loss = 30.685900786931118\n",
      "Epoch 1520: Training Loss = 30.685991229211957\n",
      "Epoch 1530: Training Loss = 30.685999569203833\n",
      "Epoch 1540: Training Loss = 30.685749439213833\n",
      "Epoch 1550: Training Loss = 30.685856898791076\n",
      "Epoch 1560: Training Loss = 30.685680811835773\n",
      "Epoch 1570: Training Loss = 30.68570265681047\n",
      "Epoch 1580: Training Loss = 30.685553669982372\n",
      "Epoch 1590: Training Loss = 30.685559874588613\n",
      "Epoch 1600: Training Loss = 30.68568129243506\n",
      "Epoch 1610: Training Loss = 30.685623242919554\n",
      "Epoch 1620: Training Loss = 30.685516707597127\n",
      "Epoch 1630: Training Loss = 30.68561510024513\n",
      "Epoch 1640: Training Loss = 30.685310837922604\n",
      "Epoch 1650: Training Loss = 30.68541574283059\n",
      "Epoch 1660: Training Loss = 30.685432890768237\n",
      "Epoch 1670: Training Loss = 30.685418529101817\n",
      "Epoch 1680: Training Loss = 30.685422088752276\n",
      "Epoch 1690: Training Loss = 30.68540979310369\n",
      "Epoch 1700: Training Loss = 30.685394009098165\n",
      "Epoch 1710: Training Loss = 30.685256033276968\n",
      "Epoch 1720: Training Loss = 30.6854620841105\n",
      "Epoch 1730: Training Loss = 30.685238171736646\n",
      "Epoch 1740: Training Loss = 30.6850209359189\n",
      "Epoch 1750: Training Loss = 30.68521304154918\n",
      "Epoch 1760: Training Loss = 30.685128366290037\n",
      "Epoch 1770: Training Loss = 30.68522598362233\n",
      "Epoch 1780: Training Loss = 30.685217459960164\n",
      "Epoch 1790: Training Loss = 30.685182726213615\n",
      "Epoch 1800: Training Loss = 30.68516489341346\n",
      "Epoch 1810: Training Loss = 30.685053799701592\n",
      "Epoch 1820: Training Loss = 30.684861021403776\n",
      "Epoch 1830: Training Loss = 30.685017934199735\n",
      "Epoch 1840: Training Loss = 30.68496081218914\n",
      "Epoch 1850: Training Loss = 30.68491475000625\n",
      "Epoch 1860: Training Loss = 30.684851852957298\n",
      "Epoch 1870: Training Loss = 30.684896739559036\n",
      "Epoch 1880: Training Loss = 30.684904515923943\n",
      "Epoch 1890: Training Loss = 30.684891337219483\n",
      "Epoch 1900: Training Loss = 30.684811934750964\n",
      "Epoch 1910: Training Loss = 30.68495602568148\n",
      "Epoch 1920: Training Loss = 30.684695629846324\n",
      "Epoch 1930: Training Loss = 30.68481727848593\n",
      "Epoch 1940: Training Loss = 30.684787416863703\n",
      "Epoch 1950: Training Loss = 30.684793647776473\n",
      "Epoch 1960: Training Loss = 30.68467892484938\n",
      "Epoch 1970: Training Loss = 30.684726802091586\n",
      "Epoch 1980: Training Loss = 30.684792253902053\n",
      "Epoch 1990: Training Loss = 30.68458966207691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 3]\n",
      "3\n",
      "(6, 3) (3, 6)\n",
      "Epoch 0: Training Loss = 25.39589430306454\n",
      "Epoch 10: Training Loss = 25.223473712095632\n",
      "Epoch 20: Training Loss = 25.17315770889788\n",
      "Epoch 30: Training Loss = 25.144953196593935\n",
      "Epoch 40: Training Loss = 25.135988269427333\n",
      "Epoch 50: Training Loss = 25.12190135835264\n",
      "Epoch 60: Training Loss = 25.104986429988475\n",
      "Epoch 70: Training Loss = 25.092520774715464\n",
      "Epoch 80: Training Loss = 25.050468004789973\n",
      "Epoch 90: Training Loss = 25.02361206909638\n",
      "Epoch 100: Training Loss = 24.955077787947115\n",
      "Epoch 110: Training Loss = 24.86053461533366\n",
      "Epoch 120: Training Loss = 24.773172812990765\n",
      "Epoch 130: Training Loss = 24.61723626997929\n",
      "Epoch 140: Training Loss = 24.430647279045264\n",
      "Epoch 150: Training Loss = 24.40238478432016\n",
      "Epoch 160: Training Loss = 24.259016731990023\n",
      "Epoch 170: Training Loss = 24.221099884778152\n",
      "Epoch 180: Training Loss = 24.160267775975722\n",
      "Epoch 190: Training Loss = 24.141711773511133\n",
      "Epoch 200: Training Loss = 24.1202190498703\n",
      "Epoch 210: Training Loss = 24.092531863562\n",
      "Epoch 220: Training Loss = 24.08446332444738\n",
      "Epoch 230: Training Loss = 24.0775242710771\n",
      "Epoch 240: Training Loss = 24.063106183553778\n",
      "Epoch 250: Training Loss = 24.06605886339821\n",
      "Epoch 260: Training Loss = 24.055287232861062\n",
      "Epoch 270: Training Loss = 24.04995029315146\n",
      "Epoch 280: Training Loss = 24.040945132178432\n",
      "Epoch 290: Training Loss = 24.05109372397371\n",
      "Epoch 300: Training Loss = 24.041341468999175\n",
      "Epoch 310: Training Loss = 24.03618328563261\n",
      "Epoch 320: Training Loss = 24.02828000287629\n",
      "Epoch 330: Training Loss = 24.02899539057503\n",
      "Epoch 340: Training Loss = 24.0269337307747\n",
      "Epoch 350: Training Loss = 24.025884362818115\n",
      "Epoch 360: Training Loss = 24.026364385713155\n",
      "Epoch 370: Training Loss = 24.020259096243823\n",
      "Epoch 380: Training Loss = 24.017605058366808\n",
      "Epoch 390: Training Loss = 24.02136528549358\n",
      "Epoch 400: Training Loss = 24.01766343571514\n",
      "Epoch 410: Training Loss = 24.015034404483643\n",
      "Epoch 420: Training Loss = 24.016605382160026\n",
      "Epoch 430: Training Loss = 24.015653759088863\n",
      "Epoch 440: Training Loss = 24.01283729645108\n",
      "Epoch 450: Training Loss = 24.012938875303156\n",
      "Epoch 460: Training Loss = 24.011924512931937\n",
      "Epoch 470: Training Loss = 24.012055701362165\n",
      "Epoch 480: Training Loss = 24.00959330453352\n",
      "Epoch 490: Training Loss = 24.00900367364347\n",
      "Epoch 500: Training Loss = 24.006348120375673\n",
      "Epoch 510: Training Loss = 24.00921849330208\n",
      "Epoch 520: Training Loss = 24.008859503432454\n",
      "Epoch 530: Training Loss = 24.00747615565822\n",
      "Epoch 540: Training Loss = 24.005575011389364\n",
      "Epoch 550: Training Loss = 24.008402908832515\n",
      "Epoch 560: Training Loss = 24.005909902474464\n",
      "Epoch 570: Training Loss = 24.00471969277474\n",
      "Epoch 580: Training Loss = 24.00346360016328\n",
      "Epoch 590: Training Loss = 24.004557093081367\n",
      "Epoch 600: Training Loss = 24.002339197811132\n",
      "Epoch 610: Training Loss = 24.003361133845306\n",
      "Epoch 620: Training Loss = 24.001805681859985\n",
      "Epoch 630: Training Loss = 24.002302227747716\n",
      "Epoch 640: Training Loss = 24.001586469681964\n",
      "Epoch 650: Training Loss = 24.002500491780733\n",
      "Epoch 660: Training Loss = 24.0007948732048\n",
      "Epoch 670: Training Loss = 24.001024213059118\n",
      "Epoch 680: Training Loss = 24.00165659716674\n",
      "Epoch 690: Training Loss = 24.000341233088633\n",
      "Epoch 700: Training Loss = 24.000568667619692\n",
      "Epoch 710: Training Loss = 24.000535979403995\n",
      "Epoch 720: Training Loss = 23.998748987804284\n",
      "Epoch 730: Training Loss = 23.999311970842967\n",
      "Epoch 740: Training Loss = 24.00019982861887\n",
      "Epoch 750: Training Loss = 23.99974052971954\n",
      "Epoch 760: Training Loss = 23.9993608004643\n",
      "Epoch 770: Training Loss = 23.997707652114375\n",
      "Epoch 780: Training Loss = 23.997953027746274\n",
      "Epoch 790: Training Loss = 23.998299533498443\n",
      "Epoch 800: Training Loss = 23.99727222842522\n",
      "Epoch 810: Training Loss = 23.99966158299971\n",
      "Epoch 820: Training Loss = 23.998293143419126\n",
      "Epoch 830: Training Loss = 23.99695087021784\n",
      "Epoch 840: Training Loss = 23.99719843050799\n",
      "Epoch 850: Training Loss = 23.997183068641526\n",
      "Epoch 860: Training Loss = 23.996584192152113\n",
      "Epoch 870: Training Loss = 23.99628077408607\n",
      "Epoch 880: Training Loss = 23.996110850023072\n",
      "Epoch 890: Training Loss = 23.995830483383198\n",
      "Epoch 900: Training Loss = 23.99557688200662\n",
      "Epoch 910: Training Loss = 23.997115036581615\n",
      "Epoch 920: Training Loss = 23.99625604606931\n",
      "Epoch 930: Training Loss = 23.99476196378666\n",
      "Epoch 940: Training Loss = 23.995899048420732\n",
      "Epoch 950: Training Loss = 23.993320126643273\n",
      "Epoch 960: Training Loss = 23.995369361334053\n",
      "Epoch 970: Training Loss = 23.995114552164694\n",
      "Epoch 980: Training Loss = 23.995597011362708\n",
      "Epoch 990: Training Loss = 23.994930802631803\n",
      "Epoch 1000: Training Loss = 23.993448143121793\n",
      "Epoch 1010: Training Loss = 23.994229703316492\n",
      "Epoch 1020: Training Loss = 23.994733811516774\n",
      "Epoch 1030: Training Loss = 23.994190123197487\n",
      "Epoch 1040: Training Loss = 23.993757005820658\n",
      "Epoch 1050: Training Loss = 23.994912040254306\n",
      "Epoch 1060: Training Loss = 23.99356959555889\n",
      "Epoch 1070: Training Loss = 23.99425433460531\n",
      "Epoch 1080: Training Loss = 23.99510397686859\n",
      "Epoch 1090: Training Loss = 23.992668187422645\n",
      "Epoch 1100: Training Loss = 23.993328730953774\n",
      "Epoch 1110: Training Loss = 23.993186098404443\n",
      "Epoch 1120: Training Loss = 23.99354245458339\n",
      "Epoch 1130: Training Loss = 23.993328040878335\n",
      "Epoch 1140: Training Loss = 23.99324225705331\n",
      "Epoch 1150: Training Loss = 23.993185409598205\n",
      "Epoch 1160: Training Loss = 23.99340753490418\n",
      "Epoch 1170: Training Loss = 23.992721334230232\n",
      "Epoch 1180: Training Loss = 23.991609025618295\n",
      "Epoch 1190: Training Loss = 23.992188210678208\n",
      "Epoch 1200: Training Loss = 23.99267309204694\n",
      "Epoch 1210: Training Loss = 23.992789759203756\n",
      "Epoch 1220: Training Loss = 23.992837727700223\n",
      "Epoch 1230: Training Loss = 23.991813042117794\n",
      "Epoch 1240: Training Loss = 23.99192614289966\n",
      "Epoch 1250: Training Loss = 23.991895239061826\n",
      "Epoch 1260: Training Loss = 23.99231865865264\n",
      "Epoch 1270: Training Loss = 23.991660343971866\n",
      "Epoch 1280: Training Loss = 23.991513319653563\n",
      "Epoch 1290: Training Loss = 23.992817870041076\n",
      "Epoch 1300: Training Loss = 23.991554267832406\n",
      "Epoch 1310: Training Loss = 23.991799563062\n",
      "Epoch 1320: Training Loss = 23.99239927148362\n",
      "Epoch 1330: Training Loss = 23.991159607806384\n",
      "Epoch 1340: Training Loss = 23.99124441234197\n",
      "Epoch 1350: Training Loss = 23.99133814702985\n",
      "Epoch 1360: Training Loss = 23.990951001349135\n",
      "Epoch 1370: Training Loss = 23.991701692032102\n",
      "Epoch 1380: Training Loss = 23.991238789924427\n",
      "Epoch 1390: Training Loss = 23.991338841729597\n",
      "Epoch 1400: Training Loss = 23.991525693733106\n",
      "Epoch 1410: Training Loss = 23.991000569080924\n",
      "Epoch 1420: Training Loss = 23.991081992828086\n",
      "Epoch 1430: Training Loss = 23.991526195159057\n",
      "Epoch 1440: Training Loss = 23.99077217100294\n",
      "Epoch 1450: Training Loss = 23.99089977329241\n",
      "Epoch 1460: Training Loss = 23.99116454016131\n",
      "Epoch 1470: Training Loss = 23.990966531274555\n",
      "Epoch 1480: Training Loss = 23.990671797282875\n",
      "Epoch 1490: Training Loss = 23.99115562503028\n",
      "Epoch 1500: Training Loss = 23.991034076983404\n",
      "Epoch 1510: Training Loss = 23.99047782050839\n",
      "Epoch 1520: Training Loss = 23.99003100541498\n",
      "Epoch 1530: Training Loss = 23.990586620242073\n",
      "Epoch 1540: Training Loss = 23.99039395914157\n",
      "Epoch 1550: Training Loss = 23.990810309824205\n",
      "Epoch 1560: Training Loss = 23.99026114483686\n",
      "Epoch 1570: Training Loss = 23.99000943969383\n",
      "Epoch 1580: Training Loss = 23.990396234707447\n",
      "Epoch 1590: Training Loss = 23.9897320572131\n",
      "Epoch 1600: Training Loss = 23.990249195882846\n",
      "Epoch 1610: Training Loss = 23.990043221206456\n",
      "Epoch 1620: Training Loss = 23.98969081819377\n",
      "Epoch 1630: Training Loss = 23.990355427753407\n",
      "Epoch 1640: Training Loss = 23.98965936927687\n",
      "Epoch 1650: Training Loss = 23.990209399998573\n",
      "Epoch 1660: Training Loss = 23.989871776506167\n",
      "Epoch 1670: Training Loss = 23.98968835427517\n",
      "Epoch 1680: Training Loss = 23.989956491287163\n",
      "Epoch 1690: Training Loss = 23.989838193445074\n",
      "Epoch 1700: Training Loss = 23.990150215723535\n",
      "Epoch 1710: Training Loss = 23.98982574509831\n",
      "Epoch 1720: Training Loss = 23.98916838761742\n",
      "Epoch 1730: Training Loss = 23.989537551613687\n",
      "Epoch 1740: Training Loss = 23.989033264824332\n",
      "Epoch 1750: Training Loss = 23.989337435245435\n",
      "Epoch 1760: Training Loss = 23.989210490993244\n",
      "Epoch 1770: Training Loss = 23.989365552545674\n",
      "Epoch 1780: Training Loss = 23.989678641889906\n",
      "Epoch 1790: Training Loss = 23.98947986202701\n",
      "Epoch 1800: Training Loss = 23.98954098289589\n",
      "Epoch 1810: Training Loss = 23.989393428630688\n",
      "Epoch 1820: Training Loss = 23.98936444003623\n",
      "Epoch 1830: Training Loss = 23.989308255338994\n",
      "Epoch 1840: Training Loss = 23.98939725882171\n",
      "Epoch 1850: Training Loss = 23.98978164883214\n",
      "Epoch 1860: Training Loss = 23.989343806341832\n",
      "Epoch 1870: Training Loss = 23.989537715863115\n",
      "Epoch 1880: Training Loss = 23.989195648163346\n",
      "Epoch 1890: Training Loss = 23.989253627458087\n",
      "Epoch 1900: Training Loss = 23.98902038426763\n",
      "Epoch 1910: Training Loss = 23.98887522765101\n",
      "Epoch 1920: Training Loss = 23.989193933473576\n",
      "Epoch 1930: Training Loss = 23.989137384290768\n",
      "Epoch 1940: Training Loss = 23.989083074644313\n",
      "Epoch 1950: Training Loss = 23.98865600964315\n",
      "Epoch 1960: Training Loss = 23.988802770926863\n",
      "Epoch 1970: Training Loss = 23.989243943861215\n",
      "Epoch 1980: Training Loss = 23.988620995527924\n",
      "Epoch 1990: Training Loss = 23.98891507545767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 1640.571489200948\n",
      "Epoch 10: Training Loss = 1603.4793427656075\n",
      "Epoch 20: Training Loss = 1584.9400932995134\n",
      "Epoch 30: Training Loss = 1578.532399241379\n",
      "Epoch 40: Training Loss = 1575.0838315726774\n",
      "Epoch 50: Training Loss = 1572.3016597345731\n",
      "Epoch 60: Training Loss = 1569.8123158662904\n",
      "Epoch 70: Training Loss = 1567.7204571865698\n",
      "Epoch 80: Training Loss = 1566.1175824304955\n",
      "Epoch 90: Training Loss = 1564.9514345927137\n",
      "Epoch 100: Training Loss = 1564.1094763478316\n",
      "Epoch 110: Training Loss = 1563.4912392931724\n",
      "Epoch 120: Training Loss = 1563.0254973343299\n",
      "Epoch 130: Training Loss = 1562.6652116796135\n",
      "Epoch 140: Training Loss = 1562.3796302613573\n",
      "Epoch 150: Training Loss = 1562.1483662107753\n",
      "Epoch 160: Training Loss = 1561.957593712325\n",
      "Epoch 170: Training Loss = 1561.797699917261\n",
      "Epoch 180: Training Loss = 1561.6618364565263\n",
      "Epoch 190: Training Loss = 1561.5450124856188\n",
      "Epoch 200: Training Loss = 1561.44351484405\n",
      "Epoch 210: Training Loss = 1561.3545288801322\n",
      "Epoch 220: Training Loss = 1561.2758848763988\n",
      "Epoch 230: Training Loss = 1561.2058848055965\n",
      "Epoch 240: Training Loss = 1561.1431815775327\n",
      "Epoch 250: Training Loss = 1561.0866932989252\n",
      "Epoch 260: Training Loss = 1561.0355413457694\n",
      "Epoch 270: Training Loss = 1560.9890049270793\n",
      "Epoch 280: Training Loss = 1560.946487263654\n",
      "Epoch 290: Training Loss = 1560.90749007623\n",
      "Epoch 300: Training Loss = 1560.8715941046785\n",
      "Epoch 310: Training Loss = 1560.8384440637162\n",
      "Epoch 320: Training Loss = 1560.8077369028738\n",
      "Epoch 330: Training Loss = 1560.7792125560911\n",
      "Epoch 340: Training Loss = 1560.7526465872256\n",
      "Epoch 350: Training Loss = 1560.7278442939783\n",
      "Epoch 360: Training Loss = 1560.7046359440512\n",
      "Epoch 370: Training Loss = 1560.6828728981577\n",
      "Epoch 380: Training Loss = 1560.6624244331342\n",
      "Epoch 390: Training Loss = 1560.643175122233\n",
      "Epoch 400: Training Loss = 1560.6250226618063\n",
      "Epoch 410: Training Loss = 1560.607876058207\n",
      "Epoch 420: Training Loss = 1560.5916541071178\n",
      "Epoch 430: Training Loss = 1560.576284111773\n",
      "Epoch 440: Training Loss = 1560.56170079742\n",
      "Epoch 450: Training Loss = 1560.5478453878156\n",
      "Epoch 460: Training Loss = 1560.5346648162758\n",
      "Epoch 470: Training Loss = 1560.522111048932\n",
      "Epoch 480: Training Loss = 1560.510140501983\n",
      "Epoch 490: Training Loss = 1560.4987135381357\n",
      "Epoch 500: Training Loss = 1560.487794029848\n",
      "Epoch 510: Training Loss = 1560.4773489793583\n",
      "Epoch 520: Training Loss = 1560.4673481870266\n",
      "Epoch 530: Training Loss = 1560.4577639609477\n",
      "Epoch 540: Training Loss = 1560.448570862027\n",
      "Epoch 550: Training Loss = 1560.4397454795005\n",
      "Epoch 560: Training Loss = 1560.4312662328027\n",
      "Epoch 570: Training Loss = 1560.423113196181\n",
      "Epoch 580: Training Loss = 1560.415267943116\n",
      "Epoch 590: Training Loss = 1560.4077134079685\n",
      "Epoch 600: Training Loss = 1560.4004337626789\n",
      "Epoch 610: Training Loss = 1560.3934143065967\n",
      "Epoch 620: Training Loss = 1560.3866413679161\n",
      "Epoch 630: Training Loss = 1560.3801022152386\n",
      "Epoch 640: Training Loss = 1560.3737849781255\n",
      "Epoch 650: Training Loss = 1560.3676785755567\n",
      "Epoch 660: Training Loss = 1560.3617726514071\n",
      "Epoch 670: Training Loss = 1560.356057516165\n",
      "Epoch 680: Training Loss = 1560.3505240941538\n",
      "Epoch 690: Training Loss = 1560.3451638757097\n",
      "Epoch 700: Training Loss = 1560.3399688737613\n",
      "Epoch 710: Training Loss = 1560.3349315843332\n",
      "Epoch 720: Training Loss = 1560.3300449505991\n",
      "Epoch 730: Training Loss = 1560.325302330057\n",
      "Epoch 740: Training Loss = 1560.320697464605\n",
      "Epoch 750: Training Loss = 1560.316224453126\n",
      "Epoch 760: Training Loss = 1560.311877726424\n",
      "Epoch 770: Training Loss = 1560.3076520242043\n",
      "Epoch 780: Training Loss = 1560.303542373984\n",
      "Epoch 790: Training Loss = 1560.299544071667\n",
      "Epoch 800: Training Loss = 1560.2956526636951\n",
      "Epoch 810: Training Loss = 1560.2918639306033\n",
      "Epoch 820: Training Loss = 1560.2881738718438\n",
      "Epoch 830: Training Loss = 1560.284578691779\n",
      "Epoch 840: Training Loss = 1560.2810747867309\n",
      "Epoch 850: Training Loss = 1560.2776587330175\n",
      "Epoch 860: Training Loss = 1560.2743272758612\n",
      "Epoch 870: Training Loss = 1560.2710773190825\n",
      "Epoch 880: Training Loss = 1560.2679059155996\n",
      "Epoch 890: Training Loss = 1560.2648102585322\n",
      "Epoch 900: Training Loss = 1560.2617876729971\n",
      "Epoch 910: Training Loss = 1560.2588356084184\n",
      "Epoch 920: Training Loss = 1560.255951631417\n",
      "Epoch 930: Training Loss = 1560.253133419138\n",
      "Epoch 940: Training Loss = 1560.2503787530827\n",
      "Epoch 950: Training Loss = 1560.247685513276\n",
      "Epoch 960: Training Loss = 1560.2450516729007\n",
      "Epoch 970: Training Loss = 1560.242475293208\n",
      "Epoch 980: Training Loss = 1560.2399545188177\n",
      "Epoch 990: Training Loss = 1560.2374875732626\n",
      "Epoch 1000: Training Loss = 1560.2350727548558\n",
      "Epoch 1010: Training Loss = 1560.232708432794\n",
      "Epoch 1020: Training Loss = 1560.2303930435087\n",
      "Epoch 1030: Training Loss = 1560.2281250872395\n",
      "Epoch 1040: Training Loss = 1560.2259031248202\n",
      "Epoch 1050: Training Loss = 1560.2237257746333\n",
      "Epoch 1060: Training Loss = 1560.2215917097926\n",
      "Epoch 1070: Training Loss = 1560.2194996554294\n",
      "Epoch 1080: Training Loss = 1560.2174483862013\n",
      "Epoch 1090: Training Loss = 1560.2154367238827\n",
      "Epoch 1100: Training Loss = 1560.2134635351536\n",
      "Epoch 1110: Training Loss = 1560.2115277294533\n",
      "Epoch 1120: Training Loss = 1560.2096282570085\n",
      "Epoch 1130: Training Loss = 1560.207764106927\n",
      "Epoch 1140: Training Loss = 1560.2059343054234\n",
      "Epoch 1150: Training Loss = 1560.204137914142\n",
      "Epoch 1160: Training Loss = 1560.2023740285433\n",
      "Epoch 1170: Training Loss = 1560.2006417764098\n",
      "Epoch 1180: Training Loss = 1560.1989403164068\n",
      "Epoch 1190: Training Loss = 1560.1972688367387\n",
      "Epoch 1200: Training Loss = 1560.195626553857\n",
      "Epoch 1210: Training Loss = 1560.1940127112418\n",
      "Epoch 1220: Training Loss = 1560.1924265782568\n",
      "Epoch 1230: Training Loss = 1560.1908674490505\n",
      "Epoch 1240: Training Loss = 1560.1893346414915\n",
      "Epoch 1250: Training Loss = 1560.1878274962253\n",
      "Epoch 1260: Training Loss = 1560.186345375693\n",
      "Epoch 1270: Training Loss = 1560.1848876632628\n",
      "Epoch 1280: Training Loss = 1560.1834537623615\n",
      "Epoch 1290: Training Loss = 1560.1820430956802\n",
      "Epoch 1300: Training Loss = 1560.1806551043944\n",
      "Epoch 1310: Training Loss = 1560.1792892474411\n",
      "Epoch 1320: Training Loss = 1560.1779450008048\n",
      "Epoch 1330: Training Loss = 1560.1766218568691\n",
      "Epoch 1340: Training Loss = 1560.1753193237546\n",
      "Epoch 1350: Training Loss = 1560.174036924748\n",
      "Epoch 1360: Training Loss = 1560.1727741977006\n",
      "Epoch 1370: Training Loss = 1560.171530694471\n",
      "Epoch 1380: Training Loss = 1560.1703059804288\n",
      "Epoch 1390: Training Loss = 1560.1690996339141\n",
      "Epoch 1400: Training Loss = 1560.1679112457816\n",
      "Epoch 1410: Training Loss = 1560.1667404189259\n",
      "Epoch 1420: Training Loss = 1560.1655867678562\n",
      "Epoch 1430: Training Loss = 1560.1644499182687\n",
      "Epoch 1440: Training Loss = 1560.1633295066245\n",
      "Epoch 1450: Training Loss = 1560.1622251798112\n",
      "Epoch 1460: Training Loss = 1560.1611365947256\n",
      "Epoch 1470: Training Loss = 1560.1600634179454\n",
      "Epoch 1480: Training Loss = 1560.159005325395\n",
      "Epoch 1490: Training Loss = 1560.157962002003\n",
      "Epoch 1500: Training Loss = 1560.1569331413966\n",
      "Epoch 1510: Training Loss = 1560.155918445622\n",
      "Epoch 1520: Training Loss = 1560.154917624825\n",
      "Epoch 1530: Training Loss = 1560.1539303970135\n",
      "Epoch 1540: Training Loss = 1560.1529564877615\n",
      "Epoch 1550: Training Loss = 1560.1519956299887\n",
      "Epoch 1560: Training Loss = 1560.1510475636785\n",
      "Epoch 1570: Training Loss = 1560.150112035681\n",
      "Epoch 1580: Training Loss = 1560.1491887994646\n",
      "Epoch 1590: Training Loss = 1560.1482776149182\n",
      "Epoch 1600: Training Loss = 1560.1473782481332\n",
      "Epoch 1610: Training Loss = 1560.1464904712066\n",
      "Epoch 1620: Training Loss = 1560.1456140620521\n",
      "Epoch 1630: Training Loss = 1560.1447488042124\n",
      "Epoch 1640: Training Loss = 1560.1438944866807\n",
      "Epoch 1650: Training Loss = 1560.1430509037461\n",
      "Epoch 1660: Training Loss = 1560.1422178548025\n",
      "Epoch 1670: Training Loss = 1560.1413951442125\n",
      "Epoch 1680: Training Loss = 1560.1405825811416\n",
      "Epoch 1690: Training Loss = 1560.139779979429\n",
      "Epoch 1700: Training Loss = 1560.1389871574181\n",
      "Epoch 1710: Training Loss = 1560.1382039378516\n",
      "Epoch 1720: Training Loss = 1560.1374301477142\n",
      "Epoch 1730: Training Loss = 1560.1366656181187\n",
      "Epoch 1740: Training Loss = 1560.135910184189\n",
      "Epoch 1750: Training Loss = 1560.1351636849283\n",
      "Epoch 1760: Training Loss = 1560.1344259631144\n",
      "Epoch 1770: Training Loss = 1560.133696865188\n",
      "Epoch 1780: Training Loss = 1560.1329762411438\n",
      "Epoch 1790: Training Loss = 1560.132263944434\n",
      "Epoch 1800: Training Loss = 1560.1315598318658\n",
      "Epoch 1810: Training Loss = 1560.130863763515\n",
      "Epoch 1820: Training Loss = 1560.1301756026003\n",
      "Epoch 1830: Training Loss = 1560.1294952154487\n",
      "Epoch 1840: Training Loss = 1560.128822471363\n",
      "Epoch 1850: Training Loss = 1560.1281572425653\n",
      "Epoch 1860: Training Loss = 1560.1274994040932\n",
      "Epoch 1870: Training Loss = 1560.1268488337435\n",
      "Epoch 1880: Training Loss = 1560.1262054119966\n",
      "Epoch 1890: Training Loss = 1560.1255690219296\n",
      "Epoch 1900: Training Loss = 1560.1249395491457\n",
      "Epoch 1910: Training Loss = 1560.1243168817227\n",
      "Epoch 1920: Training Loss = 1560.123700910134\n",
      "Epoch 1930: Training Loss = 1560.1230915271874\n",
      "Epoch 1940: Training Loss = 1560.1224886279633\n",
      "Epoch 1950: Training Loss = 1560.1218921097573\n",
      "Epoch 1960: Training Loss = 1560.121301872015\n",
      "Epoch 1970: Training Loss = 1560.1207178162922\n",
      "Epoch 1980: Training Loss = 1560.1201398461735\n",
      "Epoch 1990: Training Loss = 1560.1195678672498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 31.747101304712103\n",
      "Epoch 10: Training Loss = 31.231744875582752\n",
      "Epoch 20: Training Loss = 31.028348492954805\n",
      "Epoch 30: Training Loss = 30.93095911797921\n",
      "Epoch 40: Training Loss = 30.872378769298166\n",
      "Epoch 50: Training Loss = 30.8297943766216\n",
      "Epoch 60: Training Loss = 30.807240062899588\n",
      "Epoch 70: Training Loss = 30.787496851735717\n",
      "Epoch 80: Training Loss = 30.770445586522076\n",
      "Epoch 90: Training Loss = 30.75704162721575\n",
      "Epoch 100: Training Loss = 30.7482436033766\n",
      "Epoch 110: Training Loss = 30.743715898004208\n",
      "Epoch 120: Training Loss = 30.736676412595905\n",
      "Epoch 130: Training Loss = 30.730533750502705\n",
      "Epoch 140: Training Loss = 30.72541516898238\n",
      "Epoch 150: Training Loss = 30.723166312945043\n",
      "Epoch 160: Training Loss = 30.719501024063938\n",
      "Epoch 170: Training Loss = 30.71883456638845\n",
      "Epoch 180: Training Loss = 30.71481399156758\n",
      "Epoch 190: Training Loss = 30.713388784608767\n",
      "Epoch 200: Training Loss = 30.71262212566198\n",
      "Epoch 210: Training Loss = 30.709939771124727\n",
      "Epoch 220: Training Loss = 30.707400776492204\n",
      "Epoch 230: Training Loss = 30.7070339577169\n",
      "Epoch 240: Training Loss = 30.70492104889008\n",
      "Epoch 250: Training Loss = 30.70324474443165\n",
      "Epoch 260: Training Loss = 30.70232751488523\n",
      "Epoch 270: Training Loss = 30.70203348440663\n",
      "Epoch 280: Training Loss = 30.700875412234694\n",
      "Epoch 290: Training Loss = 30.700575289901415\n",
      "Epoch 300: Training Loss = 30.69937822387489\n",
      "Epoch 310: Training Loss = 30.69880065172459\n",
      "Epoch 320: Training Loss = 30.698639827958026\n",
      "Epoch 330: Training Loss = 30.6977809367318\n",
      "Epoch 340: Training Loss = 30.69715445644588\n",
      "Epoch 350: Training Loss = 30.69594744739183\n",
      "Epoch 360: Training Loss = 30.69630371725727\n",
      "Epoch 370: Training Loss = 30.69596631696816\n",
      "Epoch 380: Training Loss = 30.69488676421218\n",
      "Epoch 390: Training Loss = 30.69530859802652\n",
      "Epoch 400: Training Loss = 30.694552664577753\n",
      "Epoch 410: Training Loss = 30.69407274529706\n",
      "Epoch 420: Training Loss = 30.69371735579553\n",
      "Epoch 430: Training Loss = 30.693398809342842\n",
      "Epoch 440: Training Loss = 30.692753369534838\n",
      "Epoch 450: Training Loss = 30.69252525545846\n",
      "Epoch 460: Training Loss = 30.69245962951014\n",
      "Epoch 470: Training Loss = 30.692365944396453\n",
      "Epoch 480: Training Loss = 30.691642032663967\n",
      "Epoch 490: Training Loss = 30.691730134575444\n",
      "Epoch 500: Training Loss = 30.691092184734885\n",
      "Epoch 510: Training Loss = 30.690997999535515\n",
      "Epoch 520: Training Loss = 30.690725473681255\n",
      "Epoch 530: Training Loss = 30.69074696717826\n",
      "Epoch 540: Training Loss = 30.690279439530883\n",
      "Epoch 550: Training Loss = 30.69025852713217\n",
      "Epoch 560: Training Loss = 30.69019833833189\n",
      "Epoch 570: Training Loss = 30.689582592929163\n",
      "Epoch 580: Training Loss = 30.689520268530135\n",
      "Epoch 590: Training Loss = 30.689105064835438\n",
      "Epoch 600: Training Loss = 30.689476945997967\n",
      "Epoch 610: Training Loss = 30.689287540001434\n",
      "Epoch 620: Training Loss = 30.689096533811224\n",
      "Epoch 630: Training Loss = 30.68879040386993\n",
      "Epoch 640: Training Loss = 30.68857914412752\n",
      "Epoch 650: Training Loss = 30.688619974127192\n",
      "Epoch 660: Training Loss = 30.688617239892974\n",
      "Epoch 670: Training Loss = 30.68819343801794\n",
      "Epoch 680: Training Loss = 30.688198497186672\n",
      "Epoch 690: Training Loss = 30.688323763420744\n",
      "Epoch 700: Training Loss = 30.68801634518234\n",
      "Epoch 710: Training Loss = 30.687975196786105\n",
      "Epoch 720: Training Loss = 30.687786920863697\n",
      "Epoch 730: Training Loss = 30.687582821896033\n",
      "Epoch 740: Training Loss = 30.687454707928552\n",
      "Epoch 750: Training Loss = 30.687215471437494\n",
      "Epoch 760: Training Loss = 30.687328741724237\n",
      "Epoch 770: Training Loss = 30.687193970693933\n",
      "Epoch 780: Training Loss = 30.687008179995033\n",
      "Epoch 790: Training Loss = 30.68701246335622\n",
      "Epoch 800: Training Loss = 30.687046785127613\n",
      "Epoch 810: Training Loss = 30.68665445640474\n",
      "Epoch 820: Training Loss = 30.686746871102827\n",
      "Epoch 830: Training Loss = 30.686834640434352\n",
      "Epoch 840: Training Loss = 30.686591558802906\n",
      "Epoch 850: Training Loss = 30.686559044103497\n",
      "Epoch 860: Training Loss = 30.686423093688315\n",
      "Epoch 870: Training Loss = 30.686341171492433\n",
      "Epoch 880: Training Loss = 30.686070743425756\n",
      "Epoch 890: Training Loss = 30.68611378582479\n",
      "Epoch 900: Training Loss = 30.685924047806218\n",
      "Epoch 910: Training Loss = 30.68611957868476\n",
      "Epoch 920: Training Loss = 30.68582592399233\n",
      "Epoch 930: Training Loss = 30.686050403807805\n",
      "Epoch 940: Training Loss = 30.686073572474136\n",
      "Epoch 950: Training Loss = 30.68583397589723\n",
      "Epoch 960: Training Loss = 30.685907165534275\n",
      "Epoch 970: Training Loss = 30.685683197931873\n",
      "Epoch 980: Training Loss = 30.68568208791605\n",
      "Epoch 990: Training Loss = 30.685498547120304\n",
      "Epoch 1000: Training Loss = 30.685568159139223\n",
      "Epoch 1010: Training Loss = 30.6854560365719\n",
      "Epoch 1020: Training Loss = 30.68549884784814\n",
      "Epoch 1030: Training Loss = 30.685222073110854\n",
      "Epoch 1040: Training Loss = 30.68518478378685\n",
      "Epoch 1050: Training Loss = 30.685377838748153\n",
      "Epoch 1060: Training Loss = 30.68513484453314\n",
      "Epoch 1070: Training Loss = 30.685184753110164\n",
      "Epoch 1080: Training Loss = 30.68499143799578\n",
      "Epoch 1090: Training Loss = 30.68509846601714\n",
      "Epoch 1100: Training Loss = 30.685028303928966\n",
      "Epoch 1110: Training Loss = 30.68480859918714\n",
      "Epoch 1120: Training Loss = 30.684978758136236\n",
      "Epoch 1130: Training Loss = 30.684907671408848\n",
      "Epoch 1140: Training Loss = 30.684910427986594\n",
      "Epoch 1150: Training Loss = 30.684807980507543\n",
      "Epoch 1160: Training Loss = 30.684764800043638\n",
      "Epoch 1170: Training Loss = 30.684663967162656\n",
      "Epoch 1180: Training Loss = 30.68459832627277\n",
      "Epoch 1190: Training Loss = 30.684513674862533\n",
      "Epoch 1200: Training Loss = 30.68458099649888\n",
      "Epoch 1210: Training Loss = 30.684507261835392\n",
      "Epoch 1220: Training Loss = 30.68448377812657\n",
      "Epoch 1230: Training Loss = 30.68446000233588\n",
      "Epoch 1240: Training Loss = 30.684503045229157\n",
      "Epoch 1250: Training Loss = 30.68439483918661\n",
      "Epoch 1260: Training Loss = 30.684418879175343\n",
      "Epoch 1270: Training Loss = 30.6843594275577\n",
      "Epoch 1280: Training Loss = 30.68435895900716\n",
      "Epoch 1290: Training Loss = 30.68436449111631\n",
      "Epoch 1300: Training Loss = 30.684289330904445\n",
      "Epoch 1310: Training Loss = 30.68433381502333\n",
      "Epoch 1320: Training Loss = 30.68427875234785\n",
      "Epoch 1330: Training Loss = 30.684279791739765\n",
      "Epoch 1340: Training Loss = 30.684124224465954\n",
      "Epoch 1350: Training Loss = 30.68407169526589\n",
      "Epoch 1360: Training Loss = 30.684045573689847\n",
      "Epoch 1370: Training Loss = 30.68400749829885\n",
      "Epoch 1380: Training Loss = 30.684015760255555\n",
      "Epoch 1390: Training Loss = 30.68397351834097\n",
      "Epoch 1400: Training Loss = 30.683964410599526\n",
      "Epoch 1410: Training Loss = 30.683924702555416\n",
      "Epoch 1420: Training Loss = 30.683948887459024\n",
      "Epoch 1430: Training Loss = 30.684031334236852\n",
      "Epoch 1440: Training Loss = 30.683855678254925\n",
      "Epoch 1450: Training Loss = 30.68377376927456\n",
      "Epoch 1460: Training Loss = 30.68379382445355\n",
      "Epoch 1470: Training Loss = 30.683745295648183\n",
      "Epoch 1480: Training Loss = 30.683716448312975\n",
      "Epoch 1490: Training Loss = 30.683744091273955\n",
      "Epoch 1500: Training Loss = 30.683742797296407\n",
      "Epoch 1510: Training Loss = 30.683703289062798\n",
      "Epoch 1520: Training Loss = 30.68352783318184\n",
      "Epoch 1530: Training Loss = 30.683663242818316\n",
      "Epoch 1540: Training Loss = 30.68362601857816\n",
      "Epoch 1550: Training Loss = 30.683577342137017\n",
      "Epoch 1560: Training Loss = 30.683582213496315\n",
      "Epoch 1570: Training Loss = 30.683577159416252\n",
      "Epoch 1580: Training Loss = 30.683615956964413\n",
      "Epoch 1590: Training Loss = 30.683513808130765\n",
      "Epoch 1600: Training Loss = 30.683571377107796\n",
      "Epoch 1610: Training Loss = 30.68354557251134\n",
      "Epoch 1620: Training Loss = 30.68348634513714\n",
      "Epoch 1630: Training Loss = 30.683476293635696\n",
      "Epoch 1640: Training Loss = 30.683449716709365\n",
      "Epoch 1650: Training Loss = 30.68341152539343\n",
      "Epoch 1660: Training Loss = 30.68339775580386\n",
      "Epoch 1670: Training Loss = 30.68340636680216\n",
      "Epoch 1680: Training Loss = 30.68334803643042\n",
      "Epoch 1690: Training Loss = 30.68324194910054\n",
      "Epoch 1700: Training Loss = 30.683362180177184\n",
      "Epoch 1710: Training Loss = 30.683325649847614\n",
      "Epoch 1720: Training Loss = 30.683397069634502\n",
      "Epoch 1730: Training Loss = 30.683250847508337\n",
      "Epoch 1740: Training Loss = 30.683175513173303\n",
      "Epoch 1750: Training Loss = 30.683199286058745\n",
      "Epoch 1760: Training Loss = 30.683239953840136\n",
      "Epoch 1770: Training Loss = 30.683243010027205\n",
      "Epoch 1780: Training Loss = 30.68317767686593\n",
      "Epoch 1790: Training Loss = 30.683218601785736\n",
      "Epoch 1800: Training Loss = 30.683179954811653\n",
      "Epoch 1810: Training Loss = 30.68315708876709\n",
      "Epoch 1820: Training Loss = 30.683093071092774\n",
      "Epoch 1830: Training Loss = 30.683138631556172\n",
      "Epoch 1840: Training Loss = 30.68308556373083\n",
      "Epoch 1850: Training Loss = 30.683123551282815\n",
      "Epoch 1860: Training Loss = 30.683030041061755\n",
      "Epoch 1870: Training Loss = 30.68312170706614\n",
      "Epoch 1880: Training Loss = 30.683008490917494\n",
      "Epoch 1890: Training Loss = 30.68307567902366\n",
      "Epoch 1900: Training Loss = 30.682992365457668\n",
      "Epoch 1910: Training Loss = 30.6830548356985\n",
      "Epoch 1920: Training Loss = 30.682919934752082\n",
      "Epoch 1930: Training Loss = 30.68298229093561\n",
      "Epoch 1940: Training Loss = 30.682929066489116\n",
      "Epoch 1950: Training Loss = 30.68291653031509\n",
      "Epoch 1960: Training Loss = 30.682975393085407\n",
      "Epoch 1970: Training Loss = 30.68288368130254\n",
      "Epoch 1980: Training Loss = 30.682867312472187\n",
      "Epoch 1990: Training Loss = 30.68292072706612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 6 [6, 5, 7]\n",
      "4\n",
      "(5, 7) (7, 6)\n",
      "Epoch 0: Training Loss = 24.612338073548916\n",
      "Epoch 10: Training Loss = 24.312140099736297\n",
      "Epoch 20: Training Loss = 24.166295514029276\n",
      "Epoch 30: Training Loss = 24.108102563927538\n",
      "Epoch 40: Training Loss = 24.07150185451202\n",
      "Epoch 50: Training Loss = 24.05523357753568\n",
      "Epoch 60: Training Loss = 24.048264929770163\n",
      "Epoch 70: Training Loss = 24.03413504063707\n",
      "Epoch 80: Training Loss = 24.027127028755395\n",
      "Epoch 90: Training Loss = 24.021087575844476\n",
      "Epoch 100: Training Loss = 24.01595088289985\n",
      "Epoch 110: Training Loss = 24.01551589887577\n",
      "Epoch 120: Training Loss = 24.011085829598592\n",
      "Epoch 130: Training Loss = 24.009165524322047\n",
      "Epoch 140: Training Loss = 24.008023103364987\n",
      "Epoch 150: Training Loss = 24.00558696356827\n",
      "Epoch 160: Training Loss = 24.00430927025148\n",
      "Epoch 170: Training Loss = 24.00243782019823\n",
      "Epoch 180: Training Loss = 24.00111970150091\n",
      "Epoch 190: Training Loss = 23.999679719731336\n",
      "Epoch 200: Training Loss = 24.000436172330758\n",
      "Epoch 210: Training Loss = 23.998571690778245\n",
      "Epoch 220: Training Loss = 23.99900328523695\n",
      "Epoch 230: Training Loss = 23.997750443281916\n",
      "Epoch 240: Training Loss = 23.99704950590719\n",
      "Epoch 250: Training Loss = 23.997836658132158\n",
      "Epoch 260: Training Loss = 23.99631720551628\n",
      "Epoch 270: Training Loss = 23.99649847735327\n",
      "Epoch 280: Training Loss = 23.99560117426902\n",
      "Epoch 290: Training Loss = 23.994697103378332\n",
      "Epoch 300: Training Loss = 23.99363710166513\n",
      "Epoch 310: Training Loss = 23.99418520286457\n",
      "Epoch 320: Training Loss = 23.993759006166645\n",
      "Epoch 330: Training Loss = 23.993083956132715\n",
      "Epoch 340: Training Loss = 23.993316025262047\n",
      "Epoch 350: Training Loss = 23.992946985507313\n",
      "Epoch 360: Training Loss = 23.99310502960549\n",
      "Epoch 370: Training Loss = 23.992688769818056\n",
      "Epoch 380: Training Loss = 23.992218628634788\n",
      "Epoch 390: Training Loss = 23.99165273650209\n",
      "Epoch 400: Training Loss = 23.992501377359503\n",
      "Epoch 410: Training Loss = 23.991806242099333\n",
      "Epoch 420: Training Loss = 23.991362290656458\n",
      "Epoch 430: Training Loss = 23.991960752127262\n",
      "Epoch 440: Training Loss = 23.99162738955474\n",
      "Epoch 450: Training Loss = 23.990585464487022\n",
      "Epoch 460: Training Loss = 23.99085211115361\n",
      "Epoch 470: Training Loss = 23.990732829485502\n",
      "Epoch 480: Training Loss = 23.99054919816342\n",
      "Epoch 490: Training Loss = 23.99054653966041\n",
      "Epoch 500: Training Loss = 23.99037549828726\n",
      "Epoch 510: Training Loss = 23.98999742773956\n",
      "Epoch 520: Training Loss = 23.98993053448152\n",
      "Epoch 530: Training Loss = 23.989365146282964\n",
      "Epoch 540: Training Loss = 23.990083143654747\n",
      "Epoch 550: Training Loss = 23.989238609765337\n",
      "Epoch 560: Training Loss = 23.989774708038304\n",
      "Epoch 570: Training Loss = 23.989688997284556\n",
      "Epoch 580: Training Loss = 23.99001539006245\n",
      "Epoch 590: Training Loss = 23.98958405355974\n",
      "Epoch 600: Training Loss = 23.989049659953498\n",
      "Epoch 610: Training Loss = 23.988747277399476\n",
      "Epoch 620: Training Loss = 23.98912403612314\n",
      "Epoch 630: Training Loss = 23.98929961452272\n",
      "Epoch 640: Training Loss = 23.989215714159982\n",
      "Epoch 650: Training Loss = 23.98875066011612\n",
      "Epoch 660: Training Loss = 23.988385470334666\n",
      "Epoch 670: Training Loss = 23.988805914407898\n",
      "Epoch 680: Training Loss = 23.988744189066765\n",
      "Epoch 690: Training Loss = 23.988064759340418\n",
      "Epoch 700: Training Loss = 23.988493569522003\n",
      "Epoch 710: Training Loss = 23.988715250060785\n",
      "Epoch 720: Training Loss = 23.988569739740043\n",
      "Epoch 730: Training Loss = 23.988162678842645\n",
      "Epoch 740: Training Loss = 23.98807155863907\n",
      "Epoch 750: Training Loss = 23.98779087145109\n",
      "Epoch 760: Training Loss = 23.9882792870149\n",
      "Epoch 770: Training Loss = 23.988350779719415\n",
      "Epoch 780: Training Loss = 23.988132893546954\n",
      "Epoch 790: Training Loss = 23.98736587990747\n",
      "Epoch 800: Training Loss = 23.987883546563502\n",
      "Epoch 810: Training Loss = 23.987364444544912\n",
      "Epoch 820: Training Loss = 23.98808319831337\n",
      "Epoch 830: Training Loss = 23.98791188774158\n",
      "Epoch 840: Training Loss = 23.98774855254436\n",
      "Epoch 850: Training Loss = 23.98762820736554\n",
      "Epoch 860: Training Loss = 23.987605473895535\n",
      "Epoch 870: Training Loss = 23.987873674716436\n",
      "Epoch 880: Training Loss = 23.987447363675795\n",
      "Epoch 890: Training Loss = 23.987555733375515\n",
      "Epoch 900: Training Loss = 23.987181275383264\n",
      "Epoch 910: Training Loss = 23.987570524714464\n",
      "Epoch 920: Training Loss = 23.98738277492132\n",
      "Epoch 930: Training Loss = 23.98765024548986\n",
      "Epoch 940: Training Loss = 23.98749913551855\n",
      "Epoch 950: Training Loss = 23.98697363339965\n",
      "Epoch 960: Training Loss = 23.98707547782683\n",
      "Epoch 970: Training Loss = 23.986944126411327\n",
      "Epoch 980: Training Loss = 23.986930313454955\n",
      "Epoch 990: Training Loss = 23.98700258760131\n",
      "Epoch 1000: Training Loss = 23.98695102410312\n",
      "Epoch 1010: Training Loss = 23.98700970719275\n",
      "Epoch 1020: Training Loss = 23.987284790666514\n",
      "Epoch 1030: Training Loss = 23.986932243780245\n",
      "Epoch 1040: Training Loss = 23.986828252875295\n",
      "Epoch 1050: Training Loss = 23.986910268456384\n",
      "Epoch 1060: Training Loss = 23.98673587367397\n",
      "Epoch 1070: Training Loss = 23.986813988198442\n",
      "Epoch 1080: Training Loss = 23.987188489911095\n",
      "Epoch 1090: Training Loss = 23.986775713576055\n",
      "Epoch 1100: Training Loss = 23.986872429159924\n",
      "Epoch 1110: Training Loss = 23.986830418183533\n",
      "Epoch 1120: Training Loss = 23.98652535924337\n",
      "Epoch 1130: Training Loss = 23.98655045779742\n",
      "Epoch 1140: Training Loss = 23.98680058268176\n",
      "Epoch 1150: Training Loss = 23.98657303357909\n",
      "Epoch 1160: Training Loss = 23.986420244644485\n",
      "Epoch 1170: Training Loss = 23.986755465028693\n",
      "Epoch 1180: Training Loss = 23.98626892888025\n",
      "Epoch 1190: Training Loss = 23.98667422696568\n",
      "Epoch 1200: Training Loss = 23.986664411168427\n",
      "Epoch 1210: Training Loss = 23.986755838073815\n",
      "Epoch 1220: Training Loss = 23.98623122008302\n",
      "Epoch 1230: Training Loss = 23.986388232663277\n",
      "Epoch 1240: Training Loss = 23.986695654238993\n",
      "Epoch 1250: Training Loss = 23.98647830068597\n",
      "Epoch 1260: Training Loss = 23.986314903832856\n",
      "Epoch 1270: Training Loss = 23.986658311857887\n",
      "Epoch 1280: Training Loss = 23.98643952592679\n",
      "Epoch 1290: Training Loss = 23.986424191728904\n",
      "Epoch 1300: Training Loss = 23.986329743644173\n",
      "Epoch 1310: Training Loss = 23.986293504358965\n",
      "Epoch 1320: Training Loss = 23.98653602265522\n",
      "Epoch 1330: Training Loss = 23.98648861962418\n",
      "Epoch 1340: Training Loss = 23.986403984653688\n",
      "Epoch 1350: Training Loss = 23.98623408845267\n",
      "Epoch 1360: Training Loss = 23.986182185403884\n",
      "Epoch 1370: Training Loss = 23.986297648403166\n",
      "Epoch 1380: Training Loss = 23.986212520134995\n",
      "Epoch 1390: Training Loss = 23.98627589606063\n",
      "Epoch 1400: Training Loss = 23.98648860878482\n",
      "Epoch 1410: Training Loss = 23.986303030947504\n",
      "Epoch 1420: Training Loss = 23.986263319363022\n",
      "Epoch 1430: Training Loss = 23.986101946326265\n",
      "Epoch 1440: Training Loss = 23.986091726683636\n",
      "Epoch 1450: Training Loss = 23.986058536912523\n",
      "Epoch 1460: Training Loss = 23.985991612568252\n",
      "Epoch 1470: Training Loss = 23.986015925467687\n",
      "Epoch 1480: Training Loss = 23.986115957004156\n",
      "Epoch 1490: Training Loss = 23.98630902088383\n",
      "Epoch 1500: Training Loss = 23.986081665131195\n",
      "Epoch 1510: Training Loss = 23.985981775658843\n",
      "Epoch 1520: Training Loss = 23.986136415128648\n",
      "Epoch 1530: Training Loss = 23.985755192665813\n",
      "Epoch 1540: Training Loss = 23.98611397577706\n",
      "Epoch 1550: Training Loss = 23.98587556543497\n",
      "Epoch 1560: Training Loss = 23.98601373206647\n",
      "Epoch 1570: Training Loss = 23.98583845428421\n",
      "Epoch 1580: Training Loss = 23.985904150193683\n",
      "Epoch 1590: Training Loss = 23.985922798079624\n",
      "Epoch 1600: Training Loss = 23.98587700727647\n",
      "Epoch 1610: Training Loss = 23.9859312629401\n",
      "Epoch 1620: Training Loss = 23.985785795149845\n",
      "Epoch 1630: Training Loss = 23.98558031740931\n",
      "Epoch 1640: Training Loss = 23.985883209576997\n",
      "Epoch 1650: Training Loss = 23.985737586929663\n",
      "Epoch 1660: Training Loss = 23.985957491638995\n",
      "Epoch 1670: Training Loss = 23.985902371821858\n",
      "Epoch 1680: Training Loss = 23.985791708471098\n",
      "Epoch 1690: Training Loss = 23.985621706378648\n",
      "Epoch 1700: Training Loss = 23.985737673223685\n",
      "Epoch 1710: Training Loss = 23.985587388872034\n",
      "Epoch 1720: Training Loss = 23.98580428694684\n",
      "Epoch 1730: Training Loss = 23.985503104708677\n",
      "Epoch 1740: Training Loss = 23.985684978433298\n",
      "Epoch 1750: Training Loss = 23.985603081255192\n",
      "Epoch 1760: Training Loss = 23.985903772257966\n",
      "Epoch 1770: Training Loss = 23.985888858100722\n",
      "Epoch 1780: Training Loss = 23.985741079985303\n",
      "Epoch 1790: Training Loss = 23.985666432160976\n",
      "Epoch 1800: Training Loss = 23.98576629173126\n",
      "Epoch 1810: Training Loss = 23.985671273209334\n",
      "Epoch 1820: Training Loss = 23.985750720928834\n",
      "Epoch 1830: Training Loss = 23.98545972080742\n",
      "Epoch 1840: Training Loss = 23.98578109466669\n",
      "Epoch 1850: Training Loss = 23.98560940723397\n",
      "Epoch 1860: Training Loss = 23.985611154159503\n",
      "Epoch 1870: Training Loss = 23.985702817518167\n",
      "Epoch 1880: Training Loss = 23.985594006006018\n",
      "Epoch 1890: Training Loss = 23.985704253221428\n",
      "Epoch 1900: Training Loss = 23.98566125818428\n",
      "Epoch 1910: Training Loss = 23.985677961453575\n",
      "Epoch 1920: Training Loss = 23.98556288034615\n",
      "Epoch 1930: Training Loss = 23.98550974620511\n",
      "Epoch 1940: Training Loss = 23.985664792065588\n",
      "Epoch 1950: Training Loss = 23.98551863832479\n",
      "Epoch 1960: Training Loss = 23.985567199844382\n",
      "Epoch 1970: Training Loss = 23.985803576700988\n",
      "Epoch 1980: Training Loss = 23.985644054886613\n",
      "Epoch 1990: Training Loss = 23.9854710974654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#W&B part:\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(project=\"SAMI-Ass-3-2\", entity='aryamarda')\n",
    "\n",
    "for learningRate in [0.005 , 0.001]:\n",
    "    for hiddenLayers in [[6], [6,3], [6,5,7] ]:\n",
    "            for batch_size in [0, 32, 50]:\n",
    "                \n",
    "                obj = Multi_layer_perceptron_gd(data = X_train , labels=y_train, hiddenLayers=hiddenLayers, learningRate=learningRate, activation='sigmoid', OutputLayer=6)\n",
    "                obj.fit(num_epochs=2000, batch_size=batch_size)\n",
    "                y_true =  y_test.values.T[0]\n",
    "                y_pred = obj.predict(X_test)\n",
    "\n",
    "                accuracy = accuracy_score(y_true, y_pred)\n",
    "                f1 = f1_score(y_true, y_pred, average='weighted')  # Use 'weighted' if you have multiple classes\n",
    "                precision = precision_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "                recall = recall_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "                \n",
    "                wandb.log({\"Accuracy\": accuracy})\n",
    "                wandb.log({\"F1 score\": f1})\n",
    "                wandb.log({\"Precision\": precision})\n",
    "                wandb.log({\"Recall\": recall})\n",
    "              \n",
    "# obj.fit(num_epochs=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Section 2.4: Multi-Label Classification`\n",
    "Modified for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer,OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = pd.read_csv(\"advertisement.csv\")\n",
    "X = data.drop(columns=[\"labels\"])\n",
    "y = data[\"labels\"].str.split().apply(lambda labels: labels if isinstance(labels, list) else [])  # Convert labels to lists\n",
    "\n",
    "# MultiLableBinarizer encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y = mlb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding for categorical variables in X\n",
    "categorical_columns = ['gender','education','city', 'occupation', 'most bought item' ] \n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "X_encoded = encoder.fit_transform(X[categorical_columns])\n",
    "X_encoded = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "a = ['labels']\n",
    "sorted_labels = [','.join(sorted(labels)) for labels in[x.split(' ') for x in data['labels']]]\n",
    "data['labels'] = sorted_labels\n",
    "# print(sorted_labels)\n",
    "\n",
    "y_label = encoder.fit_transform(data[a])\n",
    "\n",
    "#Normalize Data X_normalized = (X - X_min) / (X_max - X_min)\n",
    "scaler = MinMaxScaler()\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(X_encoded), columns=X_encoded.columns)\n",
    "\n",
    "#Standardize Data X_standardized = (X - X_mean) / X_std\n",
    "scaler = StandardScaler()\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df_normalized), columns=df_normalized.columns)\n",
    "\n",
    "# First normalized the data and then standardized the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_standardized, y, test_size=0.2, random_state=42)\n",
    "X_train_powerset, X_test_powerset, y_train_powerset, y_test_powerset = train_test_split(X_encoded, y_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_layer_perceptron_gd_mc():\n",
    "    def __init__(self,data, labels,  hiddenLayers=[3], OutputLayer=8, learningRate = 0.001, activation = \"sigmoid\"):\n",
    "        self.labels = labels\n",
    "        # self.labels = labels\n",
    "        self.data = data\n",
    "        self.action = activation\n",
    "        self.inputLayer = data.shape[1]\n",
    "        self.learningRate = learningRate\n",
    "        self.outputLayer = OutputLayer\n",
    "        self.hiddenLayer = hiddenLayers\n",
    "        # print(self.inputLayer, self.outputLayer, self.hiddenLayer)\n",
    "        #required number of weight vector's = hiddenLayers.size()+1\n",
    "        self.we = [] \n",
    "        \n",
    "        W1 = np.random.randn(self.inputLayer, self.hiddenLayer[0])\n",
    "        self.we.append(W1)\n",
    "\n",
    "        self.numberOfHiddenLayers = len(hiddenLayers)\n",
    "\n",
    "        for i in range (1, self.numberOfHiddenLayers):\n",
    "            W1 = np.random.randn(self.hiddenLayer[i-1], self.hiddenLayer[i])\n",
    "            self.we.append(W1)\n",
    "\n",
    "        W2 = np.random.randn(self.hiddenLayer[-1], self.outputLayer)\n",
    "        self.we.append(W2)\n",
    "        # print(len(self.we))\n",
    "        # print(W1.shape,W2.shape)\n",
    "\n",
    "    def activation(self,z):\n",
    "        if(self.action == \"sigmoid\"):\n",
    "            return 1/(1+np.exp(-z))\n",
    "        if(self.action == \"tanh\"):\n",
    "            return np.tanh(z)\n",
    "        if(self.action == \"Relu\"):\n",
    "            # return z*(z>0)\n",
    "            return np.maximum(0,z)\n",
    "        \n",
    "    def derivative(self,z):\n",
    "        if(self.action == \"sigmoid\"):\n",
    "            return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "        if(self.action == \"tanh\"):\n",
    "            return 1 - z**2\n",
    "        if(self.action == \"Relu\"):\n",
    "            # return 1*(z>0)\n",
    "            return np.where(z>0 , 1 , 0)\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def soft_max_each(self,X):\n",
    "        return np.exp(X) / np.sum(np.exp(X))\n",
    "    \n",
    "    def soft_max(self, pre_score):\n",
    "   \n",
    "        for i in range(len(pre_score)):\n",
    "            pre_score[i] = self.soft_max_each(pre_score[i])\n",
    "        return pre_score\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #number z's and a's will be self.numberOfHiddenLayers+1\n",
    "        self.a = []\n",
    "        self.z = []\n",
    "        \n",
    "        self.z.append( np.dot(X, self.we[0]) )\n",
    "        self.a.append( self.activation(self.z[0] ))\n",
    "\n",
    "        for i in range(1, self.numberOfHiddenLayers+1):\n",
    "            self.z.append(  np.dot(self.a[i-1], self.we[i])  ) \n",
    "            self.a.append(  self.activation(self.z[i]) )\n",
    "\n",
    "        # return self.soft_max(self.a[-1])\n",
    "    \n",
    "    def backward(self, data_batch, labels_batch):\n",
    "        djdw = []\n",
    "\n",
    "        # Forward pass\n",
    "        self.forward(data_batch)\n",
    "        self.a[-1] = self.sigmoid(self.a[-1])\n",
    "        # Compute output layer error\n",
    "        delta = np.multiply(-(labels_batch - self.a[-1]), self.derivative(self.z[-1]))\n",
    "\n",
    "        # Backpropagation through hidden layers\n",
    "        for i in range(self.numberOfHiddenLayers):\n",
    "            curr_idx = self.numberOfHiddenLayers - i - 1\n",
    "            jw = np.dot(self.a[curr_idx].T, delta)\n",
    "            djdw.insert(0, jw)\n",
    "            delta = np.dot(delta, self.we[curr_idx + 1].T) * self.derivative(self.z[curr_idx])\n",
    "\n",
    "        # Compute the gradient for the input layer\n",
    "        jw = np.dot(data_batch.T, delta)\n",
    "        djdw.insert(0, jw)\n",
    "\n",
    "        return djdw\n",
    "\n",
    "    def gradient_descent(self, djdw):\n",
    "        n = len(self.data)\n",
    "        for i in range(len(self.we)):\n",
    "            self.we[i] -= (self.learningRate) * djdw[i]\n",
    "\n",
    "    def CELoss(self):\n",
    "        #prediction set : self.a[-1]\n",
    "        #label's : self.labels\n",
    "        L = 0 # Define loss\n",
    "        for i in range(self.a[-1].shape[0]): # Iterate through X\n",
    "            L -= np.vdot( np.array(self.labels[i]),np.log(np.array(self.a[-1][i]))) # Cross Entropy Loss\n",
    "        return L\n",
    "    \n",
    "\n",
    "    def fit(self, batch_size=0, num_epochs=2000):\n",
    "        num_samples = len(self.data)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            if batch_size == 0:  # Full Batch Gradient Descent\n",
    "                data_batch, labels_batch = self.data, self.labels\n",
    "                djdw = self.backward(data_batch, labels_batch)\n",
    "                self.gradient_descent(djdw)\n",
    "            else:  # Mini-Batch Gradient Descent or Stochastic Gradient Descent\n",
    "                indices = np.arange(num_samples)\n",
    "                np.random.shuffle(indices)\n",
    "\n",
    "                for i in range(0, num_samples, batch_size):\n",
    "                    batch_indices = indices[i:i + batch_size]\n",
    "                    data_batch = self.data[batch_indices]\n",
    "                    labels_batch = self.labels[batch_indices]\n",
    "                    djdw = self.backward(data_batch, labels_batch)\n",
    "                    self.gradient_descent(djdw)\n",
    "\n",
    "            error = self.CELoss()\n",
    "            wandb.log({\"Learning_Rate\": self.learningRate, \"batch_size\": batch_size})\n",
    "            wandb.log({\"CELoss\": error})\n",
    "            if(epoch%10 == 0):\n",
    "                print(\"Epoch {}: Training Loss = {}\".format(epoch, error))\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.forward(X)\n",
    "        probs = self.sigmoid(self.a[-1])\n",
    "        print(probs)\n",
    "        output_matrix = np.where(probs <= 0.5, 0, 1)\n",
    "        return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:sbobrw4x) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td></td></tr><tr><td>CELoss</td><td></td></tr><tr><td>F1 score</td><td></td></tr><tr><td>Learning_Rate</td><td></td></tr><tr><td>Precision</td><td></td></tr><tr><td>Recall</td><td></td></tr><tr><td>batch_size</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy</td><td>0.32937</td></tr><tr><td>CELoss</td><td>86.55185</td></tr><tr><td>F1 score</td><td>0.49651</td></tr><tr><td>Learning_Rate</td><td>0.005</td></tr><tr><td>Precision</td><td>0.33052</td></tr><tr><td>Recall</td><td>1.0</td></tr><tr><td>batch_size</td><td>50</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-darkness-15</strong> at: <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/sbobrw4x' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/sbobrw4x</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231022_215013-sbobrw4x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:sbobrw4x). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/arya/Desktop/SAMI/Ass/Ass3/wandb/run-20231022_220510-bjhet9ul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/bjhet9ul' target=\"_blank\">pleasant-lion-16</a></strong> to <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/bjhet9ul' target=\"_blank\">https://wandb.ai/aryamarda/SAMI-Ass-3-2/runs/bjhet9ul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Loss = 1108.6240858002811\n",
      "Epoch 10: Training Loss = 1466.6429176600964\n",
      "Epoch 20: Training Loss = 1509.4843099193731\n",
      "Epoch 30: Training Loss = 1518.2305695062107\n",
      "Epoch 40: Training Loss = 1521.5447504050137\n",
      "Epoch 50: Training Loss = 1522.9761600948723\n",
      "Epoch 60: Training Loss = 1523.6096371831588\n",
      "Epoch 70: Training Loss = 1523.6862144140366\n",
      "Epoch 80: Training Loss = 1523.4125531020018\n",
      "Epoch 90: Training Loss = 1522.9505799422575\n",
      "Epoch 100: Training Loss = 1522.3544601191923\n",
      "Epoch 110: Training Loss = 1521.6099617257244\n",
      "Epoch 120: Training Loss = 1520.653329684424\n",
      "Epoch 130: Training Loss = 1519.602938484244\n",
      "Epoch 140: Training Loss = 1518.6664993408162\n",
      "Epoch 150: Training Loss = 1517.648270459136\n",
      "Epoch 160: Training Loss = 1516.4556120165262\n",
      "Epoch 170: Training Loss = 1515.1135278046443\n",
      "Epoch 180: Training Loss = 1513.689871992786\n",
      "Epoch 190: Training Loss = 1512.251008042736\n",
      "Epoch 200: Training Loss = 1510.8699476303766\n",
      "Epoch 210: Training Loss = 1509.4966640032396\n",
      "Epoch 220: Training Loss = 1508.086428479932\n",
      "Epoch 230: Training Loss = 1506.6540199122348\n",
      "Epoch 240: Training Loss = 1505.1749297144247\n",
      "Epoch 250: Training Loss = 1503.5717367194425\n",
      "Epoch 260: Training Loss = 1501.880236653018\n",
      "Epoch 270: Training Loss = 1500.1541996431918\n",
      "Epoch 280: Training Loss = 1498.3574988005803\n",
      "Epoch 290: Training Loss = 1496.574729923327\n",
      "Epoch 300: Training Loss = 1494.7763270209723\n",
      "Epoch 310: Training Loss = 1492.9850460275832\n",
      "Epoch 320: Training Loss = 1491.2665268400865\n",
      "Epoch 330: Training Loss = 1489.5931096058716\n",
      "Epoch 340: Training Loss = 1487.9355731359692\n",
      "Epoch 350: Training Loss = 1486.301070965063\n",
      "Epoch 360: Training Loss = 1484.5622826783667\n",
      "Epoch 370: Training Loss = 1482.835912910593\n",
      "Epoch 380: Training Loss = 1481.0525330074165\n",
      "Epoch 390: Training Loss = 1479.4300733261562\n",
      "Epoch 400: Training Loss = 1477.950436041741\n",
      "Epoch 410: Training Loss = 1476.4782138583369\n",
      "Epoch 420: Training Loss = 1474.8900085102862\n",
      "Epoch 430: Training Loss = 1473.3082565138864\n",
      "Epoch 440: Training Loss = 1471.8516585397253\n",
      "Epoch 450: Training Loss = 1470.3449730526993\n",
      "Epoch 460: Training Loss = 1469.006777743726\n",
      "Epoch 470: Training Loss = 1467.6028123081342\n",
      "Epoch 480: Training Loss = 1466.1571667849607\n",
      "Epoch 490: Training Loss = 1464.5152750323691\n",
      "Epoch 500: Training Loss = 1462.7595017162553\n",
      "Epoch 510: Training Loss = 1461.0210009775271\n",
      "Epoch 520: Training Loss = 1459.390577402315\n",
      "Epoch 530: Training Loss = 1457.7776623138695\n",
      "Epoch 540: Training Loss = 1456.2369225266293\n",
      "Epoch 550: Training Loss = 1454.709740127385\n",
      "Epoch 560: Training Loss = 1453.0383130746186\n",
      "Epoch 570: Training Loss = 1451.367800838601\n",
      "Epoch 580: Training Loss = 1449.9455856656011\n",
      "Epoch 590: Training Loss = 1448.798854761364\n",
      "Epoch 600: Training Loss = 1447.6201727013352\n",
      "Epoch 610: Training Loss = 1446.4378380859966\n",
      "Epoch 620: Training Loss = 1445.2899632418892\n",
      "Epoch 630: Training Loss = 1443.9932226179637\n",
      "Epoch 640: Training Loss = 1442.7405694671518\n",
      "Epoch 650: Training Loss = 1441.5162491466199\n",
      "Epoch 660: Training Loss = 1440.186878782989\n",
      "Epoch 670: Training Loss = 1439.0994257776201\n",
      "Epoch 680: Training Loss = 1438.077923306907\n",
      "Epoch 690: Training Loss = 1436.8360791546665\n",
      "Epoch 700: Training Loss = 1435.6328460682994\n",
      "Epoch 710: Training Loss = 1434.40602691549\n",
      "Epoch 720: Training Loss = 1433.1797433863746\n",
      "Epoch 730: Training Loss = 1431.9176999164115\n",
      "Epoch 740: Training Loss = 1430.8207367932757\n",
      "Epoch 750: Training Loss = 1429.781576524403\n",
      "Epoch 760: Training Loss = 1428.7341617295804\n",
      "Epoch 770: Training Loss = 1427.6077917671976\n",
      "Epoch 780: Training Loss = 1426.4869893899725\n",
      "Epoch 790: Training Loss = 1425.3320609958364\n",
      "Epoch 800: Training Loss = 1424.210505545817\n",
      "Epoch 810: Training Loss = 1423.188585761299\n",
      "Epoch 820: Training Loss = 1422.1399020780666\n",
      "Epoch 830: Training Loss = 1420.9415610227447\n",
      "Epoch 840: Training Loss = 1419.5443258850266\n",
      "Epoch 850: Training Loss = 1418.2286406186424\n",
      "Epoch 860: Training Loss = 1417.149754955185\n",
      "Epoch 870: Training Loss = 1416.13934314738\n",
      "Epoch 880: Training Loss = 1415.0221128046858\n",
      "Epoch 890: Training Loss = 1414.0210666554824\n",
      "Epoch 900: Training Loss = 1413.1248119162422\n",
      "Epoch 910: Training Loss = 1412.2142909220206\n",
      "Epoch 920: Training Loss = 1411.3502724935347\n",
      "Epoch 930: Training Loss = 1410.4800860710877\n",
      "Epoch 940: Training Loss = 1409.5140493841086\n",
      "Epoch 950: Training Loss = 1408.5575132139545\n",
      "Epoch 960: Training Loss = 1407.6915768038343\n",
      "Epoch 970: Training Loss = 1406.6833691509526\n",
      "Epoch 980: Training Loss = 1405.82109612172\n",
      "Epoch 990: Training Loss = 1404.9627370981862\n",
      "Epoch 1000: Training Loss = 1403.902397041816\n",
      "Epoch 1010: Training Loss = 1402.837392665215\n",
      "Epoch 1020: Training Loss = 1402.0443485290357\n",
      "Epoch 1030: Training Loss = 1401.2267106907327\n",
      "Epoch 1040: Training Loss = 1400.299814648722\n",
      "Epoch 1050: Training Loss = 1399.5396777228455\n",
      "Epoch 1060: Training Loss = 1398.8312798635873\n",
      "Epoch 1070: Training Loss = 1398.0651062873897\n",
      "Epoch 1080: Training Loss = 1397.2965442104107\n",
      "Epoch 1090: Training Loss = 1396.4510827234164\n",
      "Epoch 1100: Training Loss = 1395.580367822032\n",
      "Epoch 1110: Training Loss = 1394.5879055790183\n",
      "Epoch 1120: Training Loss = 1393.5149680291174\n",
      "Epoch 1130: Training Loss = 1392.6574953789677\n",
      "Epoch 1140: Training Loss = 1391.6726958004986\n",
      "Epoch 1150: Training Loss = 1390.6344356023449\n",
      "Epoch 1160: Training Loss = 1389.7225122238976\n",
      "Epoch 1170: Training Loss = 1388.8702608092958\n",
      "Epoch 1180: Training Loss = 1388.0626405604803\n",
      "Epoch 1190: Training Loss = 1387.2671879884197\n",
      "Epoch 1200: Training Loss = 1386.4885210694524\n",
      "Epoch 1210: Training Loss = 1385.8009460896842\n",
      "Epoch 1220: Training Loss = 1385.035091589888\n",
      "Epoch 1230: Training Loss = 1384.2667695457328\n",
      "Epoch 1240: Training Loss = 1383.5656053857458\n",
      "Epoch 1250: Training Loss = 1382.8172428905516\n",
      "Epoch 1260: Training Loss = 1382.1247718042239\n",
      "Epoch 1270: Training Loss = 1381.5279595593256\n",
      "Epoch 1280: Training Loss = 1380.9684700842445\n",
      "Epoch 1290: Training Loss = 1380.3287995599753\n",
      "Epoch 1300: Training Loss = 1379.723056489184\n",
      "Epoch 1310: Training Loss = 1379.2534364771293\n",
      "Epoch 1320: Training Loss = 1378.7583996446226\n",
      "Epoch 1330: Training Loss = 1378.0931283162115\n",
      "Epoch 1340: Training Loss = 1377.4468427869178\n",
      "Epoch 1350: Training Loss = 1376.6824037939773\n",
      "Epoch 1360: Training Loss = 1376.1699020131864\n",
      "Epoch 1370: Training Loss = 1375.5971363076055\n",
      "Epoch 1380: Training Loss = 1374.7403280623707\n",
      "Epoch 1390: Training Loss = 1373.9456185785104\n",
      "Epoch 1400: Training Loss = 1373.2440195064637\n",
      "Epoch 1410: Training Loss = 1372.52513338615\n",
      "Epoch 1420: Training Loss = 1372.0258611698005\n",
      "Epoch 1430: Training Loss = 1371.5843623973965\n",
      "Epoch 1440: Training Loss = 1371.1531143208074\n",
      "Epoch 1450: Training Loss = 1370.6866482346422\n",
      "Epoch 1460: Training Loss = 1370.1082718197888\n",
      "Epoch 1470: Training Loss = 1369.6101496860583\n",
      "Epoch 1480: Training Loss = 1369.1325988154033\n",
      "Epoch 1490: Training Loss = 1368.6234011876586\n",
      "Epoch 1500: Training Loss = 1368.0673667917438\n",
      "Epoch 1510: Training Loss = 1367.6132302219435\n",
      "Epoch 1520: Training Loss = 1367.1563891254689\n",
      "Epoch 1530: Training Loss = 1366.703296977578\n",
      "Epoch 1540: Training Loss = 1366.2257942238891\n",
      "Epoch 1550: Training Loss = 1365.3630854447995\n",
      "Epoch 1560: Training Loss = 1364.572928311666\n",
      "Epoch 1570: Training Loss = 1363.8234240225147\n",
      "Epoch 1580: Training Loss = 1363.1873716479663\n",
      "Epoch 1590: Training Loss = 1362.5674508849565\n",
      "Epoch 1600: Training Loss = 1362.082653634467\n",
      "Epoch 1610: Training Loss = 1361.6300225936398\n",
      "Epoch 1620: Training Loss = 1361.1478947110231\n",
      "Epoch 1630: Training Loss = 1360.5911410364217\n",
      "Epoch 1640: Training Loss = 1360.1030853691454\n",
      "Epoch 1650: Training Loss = 1359.6047210013226\n",
      "Epoch 1660: Training Loss = 1359.0830322510144\n",
      "Epoch 1670: Training Loss = 1358.5440067265952\n",
      "Epoch 1680: Training Loss = 1358.1431686489018\n",
      "Epoch 1690: Training Loss = 1357.6940978260202\n",
      "Epoch 1700: Training Loss = 1357.2202745496847\n",
      "Epoch 1710: Training Loss = 1356.66510955817\n",
      "Epoch 1720: Training Loss = 1356.1704419986368\n",
      "Epoch 1730: Training Loss = 1355.6273075064828\n",
      "Epoch 1740: Training Loss = 1355.1079780536531\n",
      "Epoch 1750: Training Loss = 1354.5324212446073\n",
      "Epoch 1760: Training Loss = 1354.0012444206463\n",
      "Epoch 1770: Training Loss = 1353.481507913142\n",
      "Epoch 1780: Training Loss = 1352.9961613573241\n",
      "Epoch 1790: Training Loss = 1352.3743054028132\n",
      "Epoch 1800: Training Loss = 1351.7459653057879\n",
      "Epoch 1810: Training Loss = 1351.244793800925\n",
      "Epoch 1820: Training Loss = 1350.7522440110579\n",
      "Epoch 1830: Training Loss = 1350.2779739925204\n",
      "Epoch 1840: Training Loss = 1349.6164894623266\n",
      "Epoch 1850: Training Loss = 1348.8842864941519\n",
      "Epoch 1860: Training Loss = 1348.1713794725454\n",
      "Epoch 1870: Training Loss = 1347.786439691722\n",
      "Epoch 1880: Training Loss = 1347.416397865037\n",
      "Epoch 1890: Training Loss = 1347.0495521415917\n",
      "Epoch 1900: Training Loss = 1346.6590228257735\n",
      "Epoch 1910: Training Loss = 1346.1661346646795\n",
      "Epoch 1920: Training Loss = 1345.5878622804553\n",
      "Epoch 1930: Training Loss = 1345.072478204004\n",
      "Epoch 1940: Training Loss = 1344.6794053006602\n",
      "Epoch 1950: Training Loss = 1344.3392082350827\n",
      "Epoch 1960: Training Loss = 1343.9823261548015\n",
      "Epoch 1970: Training Loss = 1343.4959755372704\n",
      "Epoch 1980: Training Loss = 1343.1245558273552\n",
      "Epoch 1990: Training Loss = 1342.5628215565118\n",
      "Epoch 2000: Training Loss = 1342.1603186724142\n",
      "Epoch 2010: Training Loss = 1341.775342764107\n",
      "Epoch 2020: Training Loss = 1341.4254786016377\n",
      "Epoch 2030: Training Loss = 1341.0474724762698\n",
      "Epoch 2040: Training Loss = 1340.634320248904\n",
      "Epoch 2050: Training Loss = 1340.1443715363732\n",
      "Epoch 2060: Training Loss = 1339.822127178067\n",
      "Epoch 2070: Training Loss = 1339.5377430693397\n",
      "Epoch 2080: Training Loss = 1339.2655373432096\n",
      "Epoch 2090: Training Loss = 1339.006785437096\n",
      "Epoch 2100: Training Loss = 1338.757579939275\n",
      "Epoch 2110: Training Loss = 1338.5113090593263\n",
      "Epoch 2120: Training Loss = 1338.257812223867\n",
      "Epoch 2130: Training Loss = 1337.9902985759918\n",
      "Epoch 2140: Training Loss = 1337.6677951914169\n",
      "Epoch 2150: Training Loss = 1337.3536359762916\n",
      "Epoch 2160: Training Loss = 1336.9975561132437\n",
      "Epoch 2170: Training Loss = 1336.6810884887586\n",
      "Epoch 2180: Training Loss = 1336.43218896579\n",
      "Epoch 2190: Training Loss = 1336.0309192988736\n",
      "Epoch 2200: Training Loss = 1335.6996664621201\n",
      "Epoch 2210: Training Loss = 1335.116321626436\n",
      "Epoch 2220: Training Loss = 1334.609728905262\n",
      "Epoch 2230: Training Loss = 1334.2972157701495\n",
      "Epoch 2240: Training Loss = 1334.1121773740488\n",
      "Epoch 2250: Training Loss = 1333.8883681728546\n",
      "Epoch 2260: Training Loss = 1333.6824909448442\n",
      "Epoch 2270: Training Loss = 1333.5029887317576\n",
      "Epoch 2280: Training Loss = 1333.3266627763046\n",
      "Epoch 2290: Training Loss = 1333.1192940501319\n",
      "Epoch 2300: Training Loss = 1332.822013108289\n",
      "Epoch 2310: Training Loss = 1332.461575026822\n",
      "Epoch 2320: Training Loss = 1332.0246763034067\n",
      "Epoch 2330: Training Loss = 1331.6756519393764\n",
      "Epoch 2340: Training Loss = 1331.4437514948877\n",
      "Epoch 2350: Training Loss = 1331.2533980735102\n",
      "Epoch 2360: Training Loss = 1331.0725029397593\n",
      "Epoch 2370: Training Loss = 1330.8853294855878\n",
      "Epoch 2380: Training Loss = 1330.6386284357484\n",
      "Epoch 2390: Training Loss = 1330.32070873416\n",
      "Epoch 2400: Training Loss = 1330.1181634436286\n",
      "Epoch 2410: Training Loss = 1329.9221292991153\n",
      "Epoch 2420: Training Loss = 1329.6574469062791\n",
      "Epoch 2430: Training Loss = 1329.3233957178784\n",
      "Epoch 2440: Training Loss = 1329.1269203694078\n",
      "Epoch 2450: Training Loss = 1328.9428459812402\n",
      "Epoch 2460: Training Loss = 1328.69427013364\n",
      "Epoch 2470: Training Loss = 1328.5043780697226\n",
      "Epoch 2480: Training Loss = 1328.3711054909252\n",
      "Epoch 2490: Training Loss = 1328.242440082304\n",
      "Epoch 2500: Training Loss = 1328.1197520923672\n",
      "Epoch 2510: Training Loss = 1328.0001729831956\n",
      "Epoch 2520: Training Loss = 1327.8782312892893\n",
      "Epoch 2530: Training Loss = 1327.7508701220559\n",
      "Epoch 2540: Training Loss = 1327.6251536684965\n",
      "Epoch 2550: Training Loss = 1327.5002578902006\n",
      "Epoch 2560: Training Loss = 1327.3568334800098\n",
      "Epoch 2570: Training Loss = 1327.1677461759068\n",
      "Epoch 2580: Training Loss = 1327.035771495129\n",
      "Epoch 2590: Training Loss = 1326.9017378535675\n",
      "Epoch 2600: Training Loss = 1326.6525991942524\n",
      "Epoch 2610: Training Loss = 1326.4734886208776\n",
      "Epoch 2620: Training Loss = 1326.3494130165436\n",
      "Epoch 2630: Training Loss = 1326.2008548142874\n",
      "Epoch 2640: Training Loss = 1326.0439161753352\n",
      "Epoch 2650: Training Loss = 1325.85064615153\n",
      "Epoch 2660: Training Loss = 1325.560263947161\n",
      "Epoch 2670: Training Loss = 1325.3542298294153\n",
      "Epoch 2680: Training Loss = 1325.1301701689554\n",
      "Epoch 2690: Training Loss = 1324.6190355150725\n",
      "Epoch 2700: Training Loss = 1324.1198528358236\n",
      "Epoch 2710: Training Loss = 1323.8636743287805\n",
      "Epoch 2720: Training Loss = 1323.6907923149158\n",
      "Epoch 2730: Training Loss = 1323.536500867674\n",
      "Epoch 2740: Training Loss = 1323.3857846649967\n",
      "Epoch 2750: Training Loss = 1323.2474078169707\n",
      "Epoch 2760: Training Loss = 1323.1189005467643\n",
      "Epoch 2770: Training Loss = 1322.9937012892615\n",
      "Epoch 2780: Training Loss = 1322.8652892095097\n",
      "Epoch 2790: Training Loss = 1322.7294606659125\n",
      "Epoch 2800: Training Loss = 1322.5941582137386\n",
      "Epoch 2810: Training Loss = 1322.4613981045247\n",
      "Epoch 2820: Training Loss = 1322.280383458708\n",
      "Epoch 2830: Training Loss = 1322.043120719384\n",
      "Epoch 2840: Training Loss = 1321.870364882916\n",
      "Epoch 2850: Training Loss = 1321.6645863741264\n",
      "Epoch 2860: Training Loss = 1321.3788039434055\n",
      "Epoch 2870: Training Loss = 1321.0715863553587\n",
      "Epoch 2880: Training Loss = 1320.843002862158\n",
      "Epoch 2890: Training Loss = 1320.684733952829\n",
      "Epoch 2900: Training Loss = 1320.533602558287\n",
      "Epoch 2910: Training Loss = 1320.3652307069078\n",
      "Epoch 2920: Training Loss = 1320.2286074794138\n",
      "Epoch 2930: Training Loss = 1320.1249586114945\n",
      "Epoch 2940: Training Loss = 1320.0185624454666\n",
      "Epoch 2950: Training Loss = 1319.8972508991073\n",
      "Epoch 2960: Training Loss = 1319.7198813280756\n",
      "Epoch 2970: Training Loss = 1319.4708813072214\n",
      "Epoch 2980: Training Loss = 1319.3406054014604\n",
      "Epoch 2990: Training Loss = 1319.2184084960027\n",
      "[[0.50026066 0.50000001 0.5000134  ... 0.5000552  0.5        0.50132505]\n",
      " [0.5        0.50020768 0.5        ... 0.63738327 0.50021906 0.50000004]\n",
      " [0.50000038 0.50000003 0.50000001 ... 0.50000871 0.50119427 0.50000002]\n",
      " ...\n",
      " [0.70903217 0.50006437 0.51199848 ... 0.50000094 0.5        0.72899934]\n",
      " [0.50000346 0.50040849 0.50844591 ... 0.50000289 0.50044927 0.5030198 ]\n",
      " [0.5        0.50174974 0.5        ... 0.50024382 0.50005394 0.5       ]]\n",
      "Epoch 0: Training Loss = 44.57723598239274\n",
      "Epoch 10: Training Loss = 54.2637773886165\n",
      "Epoch 20: Training Loss = 54.83814435182558\n",
      "Epoch 30: Training Loss = 53.96404780853655\n",
      "Epoch 40: Training Loss = 54.719439796705515\n",
      "Epoch 50: Training Loss = 54.4421207421556\n",
      "Epoch 60: Training Loss = 54.918964925343616\n",
      "Epoch 70: Training Loss = 54.55625472738005\n",
      "Epoch 80: Training Loss = 54.861460781882236\n",
      "Epoch 90: Training Loss = 55.23873466104685\n",
      "Epoch 100: Training Loss = 54.2339589667383\n",
      "Epoch 110: Training Loss = 54.877587076390064\n",
      "Epoch 120: Training Loss = 54.095694394745195\n",
      "Epoch 130: Training Loss = 54.38336476091284\n",
      "Epoch 140: Training Loss = 54.34177280331675\n",
      "Epoch 150: Training Loss = 54.70496566358184\n",
      "Epoch 160: Training Loss = 54.710596532600626\n",
      "Epoch 170: Training Loss = 54.73311209611048\n",
      "Epoch 180: Training Loss = 53.666115246722356\n",
      "Epoch 190: Training Loss = 54.61918074089685\n",
      "Epoch 200: Training Loss = 53.93864872708962\n",
      "Epoch 210: Training Loss = 53.52229183920793\n",
      "Epoch 220: Training Loss = 54.313758527219875\n",
      "Epoch 230: Training Loss = 54.027668197389474\n",
      "Epoch 240: Training Loss = 54.57622793525963\n",
      "Epoch 250: Training Loss = 54.06584258144489\n",
      "Epoch 260: Training Loss = 54.23952935461436\n",
      "Epoch 270: Training Loss = 54.385717817014466\n",
      "Epoch 280: Training Loss = 53.47503686394879\n",
      "Epoch 290: Training Loss = 53.40916837270594\n",
      "Epoch 300: Training Loss = 54.46558919659908\n",
      "Epoch 310: Training Loss = 53.47964403606491\n",
      "Epoch 320: Training Loss = 53.96065080322461\n",
      "Epoch 330: Training Loss = 54.127976078454296\n",
      "Epoch 340: Training Loss = 53.637497191197696\n",
      "Epoch 350: Training Loss = 54.66311127368806\n",
      "Epoch 360: Training Loss = 53.69007980178406\n",
      "Epoch 370: Training Loss = 54.14520605494197\n",
      "Epoch 380: Training Loss = 53.531790611121814\n",
      "Epoch 390: Training Loss = 53.5777775160793\n",
      "Epoch 400: Training Loss = 53.013651459909276\n",
      "Epoch 410: Training Loss = 53.89357011168118\n",
      "Epoch 420: Training Loss = 52.97371896594355\n",
      "Epoch 430: Training Loss = 54.502516806072336\n",
      "Epoch 440: Training Loss = 53.08501430820319\n",
      "Epoch 450: Training Loss = 53.22048117805736\n",
      "Epoch 460: Training Loss = 53.710622864403774\n",
      "Epoch 470: Training Loss = 54.317274054827955\n",
      "Epoch 480: Training Loss = 53.420926352418896\n",
      "Epoch 490: Training Loss = 52.60518506338606\n",
      "Epoch 500: Training Loss = 54.15639083383778\n",
      "Epoch 510: Training Loss = 52.92858864969669\n",
      "Epoch 520: Training Loss = 52.31746257875171\n",
      "Epoch 530: Training Loss = 53.61251055822721\n",
      "Epoch 540: Training Loss = 53.82326339948251\n",
      "Epoch 550: Training Loss = 52.98185210259907\n",
      "Epoch 560: Training Loss = 53.714557866497046\n",
      "Epoch 570: Training Loss = 53.67833680942546\n",
      "Epoch 580: Training Loss = 52.54778754112709\n",
      "Epoch 590: Training Loss = 52.87994771010614\n",
      "Epoch 600: Training Loss = 53.15520902032803\n",
      "Epoch 610: Training Loss = 53.335438306609966\n",
      "Epoch 620: Training Loss = 52.57370295131737\n",
      "Epoch 630: Training Loss = 52.58098073769551\n",
      "Epoch 640: Training Loss = 53.56983365680112\n",
      "Epoch 650: Training Loss = 53.19954070474944\n",
      "Epoch 660: Training Loss = 52.69011052056744\n",
      "Epoch 670: Training Loss = 54.48515941979752\n",
      "Epoch 680: Training Loss = 53.15486606293059\n",
      "Epoch 690: Training Loss = 53.08637268162421\n",
      "Epoch 700: Training Loss = 53.40912183160938\n",
      "Epoch 710: Training Loss = 53.43372861875201\n",
      "Epoch 720: Training Loss = 53.47673937060147\n",
      "Epoch 730: Training Loss = 53.32504442113908\n",
      "Epoch 740: Training Loss = 52.20864028529085\n",
      "Epoch 750: Training Loss = 53.30032514695635\n",
      "Epoch 760: Training Loss = 51.43686390462076\n",
      "Epoch 770: Training Loss = 52.29126683836021\n",
      "Epoch 780: Training Loss = 53.81058391912759\n",
      "Epoch 790: Training Loss = 53.528610466568395\n",
      "Epoch 800: Training Loss = 51.61812063033544\n",
      "Epoch 810: Training Loss = 52.376304420961034\n",
      "Epoch 820: Training Loss = 53.42881313511265\n",
      "Epoch 830: Training Loss = 53.03876413474811\n",
      "Epoch 840: Training Loss = 52.18764218091282\n",
      "Epoch 850: Training Loss = 53.973089863648205\n",
      "Epoch 860: Training Loss = 50.634587233087856\n",
      "Epoch 870: Training Loss = 53.03137800681555\n",
      "Epoch 880: Training Loss = 52.19663038603367\n",
      "Epoch 890: Training Loss = 52.480534543695654\n",
      "Epoch 900: Training Loss = 54.080322253553625\n",
      "Epoch 910: Training Loss = 51.030959649979934\n",
      "Epoch 920: Training Loss = 52.63394278636704\n",
      "Epoch 930: Training Loss = 52.1596752004026\n",
      "Epoch 940: Training Loss = 52.49717847691719\n",
      "Epoch 950: Training Loss = 52.84920057206168\n",
      "Epoch 960: Training Loss = 52.787878159680695\n",
      "Epoch 970: Training Loss = 52.54982187401417\n",
      "Epoch 980: Training Loss = 52.932296802131525\n",
      "Epoch 990: Training Loss = 52.3892014050689\n",
      "Epoch 1000: Training Loss = 52.964137077251095\n",
      "Epoch 1010: Training Loss = 50.38178078820644\n",
      "Epoch 1020: Training Loss = 52.69850586168023\n",
      "Epoch 1030: Training Loss = 52.614004991961735\n",
      "Epoch 1040: Training Loss = 52.36419938378382\n",
      "Epoch 1050: Training Loss = 53.09583158809115\n",
      "Epoch 1060: Training Loss = 51.164196134304\n",
      "Epoch 1070: Training Loss = 52.77503291692804\n",
      "Epoch 1080: Training Loss = 51.789451186517724\n",
      "Epoch 1090: Training Loss = 51.52071655403818\n",
      "Epoch 1100: Training Loss = 52.428230209118745\n",
      "Epoch 1110: Training Loss = 52.11359568136998\n",
      "Epoch 1120: Training Loss = 51.61500543811343\n",
      "Epoch 1130: Training Loss = 52.202410756844856\n",
      "Epoch 1140: Training Loss = 52.08305364476924\n",
      "Epoch 1150: Training Loss = 53.478405950396194\n",
      "Epoch 1160: Training Loss = 52.19428799402839\n",
      "Epoch 1170: Training Loss = 51.916969538598195\n",
      "Epoch 1180: Training Loss = 53.278814590139895\n",
      "Epoch 1190: Training Loss = 52.212956847424444\n",
      "Epoch 1200: Training Loss = 52.324245809220265\n",
      "Epoch 1210: Training Loss = 52.76766371138024\n",
      "Epoch 1220: Training Loss = 52.93282287514516\n",
      "Epoch 1230: Training Loss = 52.87621116152654\n",
      "Epoch 1240: Training Loss = 52.99590634965067\n",
      "Epoch 1250: Training Loss = 51.78891588978451\n",
      "Epoch 1260: Training Loss = 50.18484139260786\n",
      "Epoch 1270: Training Loss = 52.848491549693165\n",
      "Epoch 1280: Training Loss = 53.37016975469821\n",
      "Epoch 1290: Training Loss = 52.305178925998185\n",
      "Epoch 1300: Training Loss = 52.28973432743451\n",
      "Epoch 1310: Training Loss = 50.858344387286266\n",
      "Epoch 1320: Training Loss = 51.51106008072263\n",
      "Epoch 1330: Training Loss = 52.88687730843195\n",
      "Epoch 1340: Training Loss = 53.003403933420515\n",
      "Epoch 1350: Training Loss = 51.0444013958582\n",
      "Epoch 1360: Training Loss = 51.327492151919344\n",
      "Epoch 1370: Training Loss = 52.95976832519924\n",
      "Epoch 1380: Training Loss = 52.361848522785536\n",
      "Epoch 1390: Training Loss = 51.85243292201664\n",
      "Epoch 1400: Training Loss = 52.722088142296066\n",
      "Epoch 1410: Training Loss = 52.231321613575496\n",
      "Epoch 1420: Training Loss = 53.109678590302735\n",
      "Epoch 1430: Training Loss = 52.48726694705138\n",
      "Epoch 1440: Training Loss = 51.71246184781412\n",
      "Epoch 1450: Training Loss = 52.7776807580368\n",
      "Epoch 1460: Training Loss = 50.93256733350682\n",
      "Epoch 1470: Training Loss = 51.27530335150537\n",
      "Epoch 1480: Training Loss = 50.379962848179545\n",
      "Epoch 1490: Training Loss = 52.63715574329724\n",
      "Epoch 1500: Training Loss = 50.97119987276645\n",
      "Epoch 1510: Training Loss = 52.499461165751754\n",
      "Epoch 1520: Training Loss = 52.159635812627414\n",
      "Epoch 1530: Training Loss = 52.269894738526915\n",
      "Epoch 1540: Training Loss = 51.63393608468588\n",
      "Epoch 1550: Training Loss = 51.341048709642614\n",
      "Epoch 1560: Training Loss = 51.57667832656778\n",
      "Epoch 1570: Training Loss = 51.695650766308525\n",
      "Epoch 1580: Training Loss = 52.53848909352189\n",
      "Epoch 1590: Training Loss = 53.11216834560855\n",
      "Epoch 1600: Training Loss = 51.96871632834821\n",
      "Epoch 1610: Training Loss = 53.00455999850242\n",
      "Epoch 1620: Training Loss = 52.211329277179665\n",
      "Epoch 1630: Training Loss = 52.412353405989194\n",
      "Epoch 1640: Training Loss = 50.672408694861\n",
      "Epoch 1650: Training Loss = 51.94997864541512\n",
      "Epoch 1660: Training Loss = 52.34713921330553\n",
      "Epoch 1670: Training Loss = 52.85846842539138\n",
      "Epoch 1680: Training Loss = 52.10026504440308\n",
      "Epoch 1690: Training Loss = 52.9810655352978\n",
      "Epoch 1700: Training Loss = 52.381206664088275\n",
      "Epoch 1710: Training Loss = 52.129472606834305\n",
      "Epoch 1720: Training Loss = 51.80394842700758\n",
      "Epoch 1730: Training Loss = 51.93350715331648\n",
      "Epoch 1740: Training Loss = 52.68388402978247\n",
      "Epoch 1750: Training Loss = 52.412995487887976\n",
      "Epoch 1760: Training Loss = 50.89073941749753\n",
      "Epoch 1770: Training Loss = 51.868121274968125\n",
      "Epoch 1780: Training Loss = 52.50294211454448\n",
      "Epoch 1790: Training Loss = 51.97943211396154\n",
      "Epoch 1800: Training Loss = 54.460172855631264\n",
      "Epoch 1810: Training Loss = 51.81762418047845\n",
      "Epoch 1820: Training Loss = 53.68292292439449\n",
      "Epoch 1830: Training Loss = 50.427198076255884\n",
      "Epoch 1840: Training Loss = 51.027383336261366\n",
      "Epoch 1850: Training Loss = 51.44473833454317\n",
      "Epoch 1860: Training Loss = 51.70624287972018\n",
      "Epoch 1870: Training Loss = 51.23902421143884\n",
      "Epoch 1880: Training Loss = 52.48931376812909\n",
      "Epoch 1890: Training Loss = 51.08542426713598\n",
      "Epoch 1900: Training Loss = 52.458775008652935\n",
      "Epoch 1910: Training Loss = 52.84077867781708\n",
      "Epoch 1920: Training Loss = 52.10057426998155\n",
      "Epoch 1930: Training Loss = 50.217659843728676\n",
      "Epoch 1940: Training Loss = 52.44043458866003\n",
      "Epoch 1950: Training Loss = 52.364475089768966\n",
      "Epoch 1960: Training Loss = 52.69258074918088\n",
      "Epoch 1970: Training Loss = 51.501398976545566\n",
      "Epoch 1980: Training Loss = 54.3246069065767\n",
      "Epoch 1990: Training Loss = 51.66236089927573\n",
      "Epoch 2000: Training Loss = 51.033974470815224\n",
      "Epoch 2010: Training Loss = 51.99928044747766\n",
      "Epoch 2020: Training Loss = 52.28431117930285\n",
      "Epoch 2030: Training Loss = 51.9154827453704\n",
      "Epoch 2040: Training Loss = 50.84910815244794\n",
      "Epoch 2050: Training Loss = 51.982130883980076\n",
      "Epoch 2060: Training Loss = 52.28429795835212\n",
      "Epoch 2070: Training Loss = 49.977000157191284\n",
      "Epoch 2080: Training Loss = 52.32431055022144\n",
      "Epoch 2090: Training Loss = 52.53548460712674\n",
      "Epoch 2100: Training Loss = 51.844683432990536\n",
      "Epoch 2110: Training Loss = 50.75639747251131\n",
      "Epoch 2120: Training Loss = 50.71588510191646\n",
      "Epoch 2130: Training Loss = 50.027935354012705\n",
      "Epoch 2140: Training Loss = 52.0871912132162\n",
      "Epoch 2150: Training Loss = 52.033673499045506\n",
      "Epoch 2160: Training Loss = 51.685597837415706\n",
      "Epoch 2170: Training Loss = 51.39036794633827\n",
      "Epoch 2180: Training Loss = 52.08200342768996\n",
      "Epoch 2190: Training Loss = 54.29092861816031\n",
      "Epoch 2200: Training Loss = 51.74109475184989\n",
      "Epoch 2210: Training Loss = 52.52543701707033\n",
      "Epoch 2220: Training Loss = 50.53531140440735\n",
      "Epoch 2230: Training Loss = 52.67757453113866\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_size \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m50\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     obj \u001b[39m=\u001b[39m Multi_layer_perceptron_gd_mc(data \u001b[39m=\u001b[39m X_train , labels\u001b[39m=\u001b[39my_train, hiddenLayers\u001b[39m=\u001b[39mhiddenLayers, learningRate\u001b[39m=\u001b[39mlearningRate, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m, OutputLayer\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     obj\u001b[39m.\u001b[39;49mfit(num_epochs\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     y_true \u001b[39m=\u001b[39m  y_test\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m         data_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[batch_indices]\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m         labels_batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[batch_indices]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m         djdw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackward(data_batch, labels_batch)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_descent(djdw)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=127'>128</a>\u001b[0m error \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCELoss()\n",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m djdw \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(data_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# Compute output layer error\u001b[39;00m\n",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz\u001b[39m.\u001b[39mappend( np\u001b[39m.\u001b[39;49mdot(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwe[\u001b[39m0\u001b[39;49m]) )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\u001b[39m.\u001b[39mappend( \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz[\u001b[39m0\u001b[39m] ))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/2.ipynb#X41sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumberOfHiddenLayers\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "wandb.init(project=\"SAMI-Ass-3-2\", entity='aryamarda')\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "for learningRate in [0.005 , 0.001]:\n",
    "    for hiddenLayers in [[26], [8,4], [6,5,7] ]:\n",
    "            for batch_size in [0, 32, 50]:\n",
    "                \n",
    "                obj = Multi_layer_perceptron_gd_mc(data = X_train , labels=y_train, hiddenLayers=hiddenLayers, learningRate=learningRate, activation='sigmoid', OutputLayer=8)\n",
    "                obj.fit(num_epochs=3000, batch_size=batch_size)\n",
    "                y_true =  y_test\n",
    "                y_pred = obj.predict(X_test)\n",
    "\n",
    "                accuracy=1-hamming_loss(np.array(y_pred),y_true)\n",
    "                f1 = f1_score(y_true, y_pred, average='weighted')  # Use 'weighted' if you have multiple classes\n",
    "                precision = precision_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "                recall = recall_score(y_true, y_pred, average='weighted')  # Use 'weighted' for multiple classes\n",
    "                \n",
    "                wandb.log({\"Accuracy\": accuracy})\n",
    "                wandb.log({\"F1 score\": f1})\n",
    "                wandb.log({\"Precision\": precision})\n",
    "                wandb.log({\"Recall\": recall})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.26737689965494\n",
      "54.52552998295346\n",
      "51.3716466532338\n",
      "54.662978799918726\n",
      "53.57163544230851\n",
      "51.832546713361324\n",
      "46.014441494629224\n",
      "49.394151070591406\n",
      "59.57226958545256\n",
      "51.26156104487028\n",
      "[[0.5        0.73105818 0.50000086 ... 0.73105666 0.5        0.50000001]\n",
      " [0.5        0.50002646 0.50431565 ... 0.5        0.5        0.5       ]\n",
      " [0.513393   0.5        0.50000276 ... 0.5        0.50001593 0.5       ]\n",
      " ...\n",
      " [0.73005022 0.5        0.5        ... 0.50000002 0.50000119 0.7310098 ]\n",
      " [0.50142964 0.50000105 0.50231466 ... 0.73103445 0.5        0.50000125]\n",
      " [0.73094705 0.5        0.5        ... 0.50081192 0.50000024 0.5       ]]\n",
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 0 ... 1 1 1]]\n",
      "[[0 1 0 ... 1 0 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [1 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score,hamming_loss\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "obj = Multi_layer_perceptron_gd_mc(data=X_train, labels=y_train,  hiddenLayers=[26], OutputLayer=8, learningRate =0.06, activation = \"sigmoid\")\n",
    "obj.fit(num_epochs=1000, batch_size=32) \n",
    "predict=obj.predict(X_train)\n",
    "\n",
    "print(predict)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "a=[[1,1,0],\n",
    "   [1,1,0]]\n",
    "b=[[1,0,0],\n",
    "   [1,0,0]]\n",
    "a=np.array(a)\n",
    "b=np.array(b)\n",
    "predict=np.array(predict)\n",
    "y_train=np.array(y_train)\n",
    "print(1-hamming_loss(predict,y_train))\n",
    "# # loss=hamming_loss(predict,y_train)\n",
    "# print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
