{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.9327079057693481\n",
      "Epoch [2/10] - Loss: 0.9290419816970825\n",
      "Epoch [3/10] - Loss: 0.9268123507499695\n",
      "Epoch [4/10] - Loss: 0.8592595458030701\n",
      "Epoch [5/10] - Loss: 0.9776791334152222\n",
      "Epoch [6/10] - Loss: 1.0828932523727417\n",
      "Epoch [7/10] - Loss: 0.8985559344291687\n",
      "Epoch [8/10] - Loss: 1.0228029489517212\n",
      "Epoch [9/10] - Loss: 0.9193823933601379\n",
      "Epoch [10/10] - Loss: 0.8246817588806152\n",
      "Validation Accuracy: 63.32%\n"
     ]
    }
   ],
   "source": [
    "### Implement and train an MLP model on the MultiMNIST dataset\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = './double_mnist_seed_123_image_size_64_64'\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Define the dataset\n",
    "dataset = ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# Define the DataLoader for the dataset\n",
    "batch_size = 64\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "input_size = 64 * 64 * 3  # Corrected input size for your dataset\n",
    "hidden_size = 128\n",
    "num_classes = 10  # Assuming you want to classify individual digits\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = MLP(input_size, hidden_size, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "\n",
    "# # Evaluation on the test set\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on the test set\n",
    "#### Evalution on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy: 64.09%\n",
      "Train set Accuracy: 64.07375%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test set Accuracy: {accuracy}%')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train set Accuracy: {accuracy}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: Adjust the number of hidden layers and the number of neurons within each layer to optimize performance and find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.9755915403366089\n",
      "Epoch [2/10] - Loss: 0.8356947898864746\n",
      "Epoch [3/10] - Loss: 0.8324364423751831\n",
      "Epoch [4/10] - Loss: 0.9502490758895874\n",
      "Epoch [5/10] - Loss: 0.8857095241546631\n",
      "Epoch [6/10] - Loss: 1.0500184297561646\n",
      "Epoch [7/10] - Loss: 1.0570441484451294\n",
      "Epoch [8/10] - Loss: 0.9917820692062378\n",
      "Epoch [9/10] - Loss: 1.0315135717391968\n",
      "Epoch [10/10] - Loss: 0.8267143368721008\n",
      "Validation Accuracy: 63.32%\n",
      "Epoch [1/10] - Loss: 0.8746269941329956\n",
      "Epoch [2/10] - Loss: 0.9590107798576355\n",
      "Epoch [3/10] - Loss: 0.8966859579086304\n",
      "Epoch [4/10] - Loss: 0.8422229290008545\n",
      "Epoch [5/10] - Loss: 0.819499671459198\n",
      "Epoch [6/10] - Loss: 1.0187956094741821\n",
      "Epoch [7/10] - Loss: 0.9927339553833008\n",
      "Epoch [8/10] - Loss: 0.8483586311340332\n",
      "Epoch [9/10] - Loss: 0.8398551940917969\n",
      "Epoch [10/10] - Loss: 0.9233770370483398\n",
      "Validation Accuracy: 63.32%\n",
      "Epoch [1/10] - Loss: 0.7588663697242737\n",
      "Epoch [2/10] - Loss: 0.902991533279419\n",
      "Epoch [3/10] - Loss: 1.0225439071655273\n",
      "Epoch [4/10] - Loss: 0.8683803081512451\n",
      "Epoch [5/10] - Loss: 0.8345160484313965\n",
      "Epoch [6/10] - Loss: 0.9536965489387512\n",
      "Epoch [7/10] - Loss: 0.8774786591529846\n",
      "Epoch [8/10] - Loss: 0.9734683036804199\n",
      "Epoch [9/10] - Loss: 0.9027912020683289\n",
      "Epoch [10/10] - Loss: 0.8700070381164551\n",
      "Validation Accuracy: 63.32%\n",
      "Epoch [1/10] - Loss: 0.8673276305198669\n",
      "Epoch [2/10] - Loss: 0.9182831645011902\n",
      "Epoch [3/10] - Loss: 0.7934929132461548\n",
      "Epoch [4/10] - Loss: 0.9947680234909058\n",
      "Epoch [5/10] - Loss: 0.9658160209655762\n",
      "Epoch [6/10] - Loss: 0.9448099732398987\n",
      "Epoch [7/10] - Loss: 0.7819707989692688\n",
      "Epoch [8/10] - Loss: 0.8516843914985657\n",
      "Epoch [9/10] - Loss: 1.1192727088928223\n",
      "Epoch [10/10] - Loss: 0.9290044903755188\n",
      "Validation Accuracy: 63.32%\n",
      "Epoch [1/10] - Loss: 1.0615206956863403\n",
      "Epoch [2/10] - Loss: 0.9513676166534424\n",
      "Epoch [3/10] - Loss: 1.0545449256896973\n",
      "Epoch [4/10] - Loss: 0.7565531730651855\n",
      "Epoch [5/10] - Loss: 0.8969540596008301\n",
      "Epoch [6/10] - Loss: 0.9212307929992676\n",
      "Epoch [7/10] - Loss: 0.9013833999633789\n",
      "Epoch [8/10] - Loss: 0.8910892605781555\n",
      "Epoch [9/10] - Loss: 0.8655309677124023\n",
      "Epoch [10/10] - Loss: 0.9432631731033325\n",
      "Validation Accuracy: 63.32%\n",
      "Epoch [1/10] - Loss: 0.8768522143363953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb Cell 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/arya/Desktop/SAMI/Ass/Ass3/5.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "hidden_layer_sizes = [1, 2, 3]  # Number of hidden layers\n",
    "neurons_per_layer = [64, 128, 256]  # Number of neurons in each layer\n",
    "\n",
    "best_model = None\n",
    "best_accuracy = 0.0\n",
    "\n",
    "# Grid search through the hyperparameter space\n",
    "for num_layers, num_neurons in itertools.product(hidden_layer_sizes, neurons_per_layer):\n",
    "    # Define and train the model with the current hyperparameters\n",
    "    model = MLP(input_size, num_neurons, num_classes)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ##################\n",
    "\n",
    "    # train\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item()}')\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Validation Accuracy: {accuracy}%')\n",
    "    ############\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best Accuracy: {best_accuracy}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN FOR THE MULTI -DIGIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Loss: 0.9264047741889954\n",
      "Epoch [2/10] - Loss: 0.5912719368934631\n",
      "Epoch [3/10] - Loss: 0.4435708820819855\n",
      "Epoch [4/10] - Loss: 0.35280418395996094\n",
      "Epoch [5/10] - Loss: 0.18789555132389069\n",
      "Epoch [6/10] - Loss: 0.23441602289676666\n",
      "Epoch [7/10] - Loss: 0.08885250985622406\n",
      "Epoch [8/10] - Loss: 0.024477843195199966\n",
      "Epoch [9/10] - Loss: 0.20695814490318298\n",
      "Epoch [10/10] - Loss: 0.012288366444408894\n",
      "Validation Accuracy: 84.03%\n",
      "Test Accuracy: 84.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = './double_mnist_seed_123_image_size_64_64'\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64 pixels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize for 3 color channels\n",
    "])\n",
    "\n",
    "# Define the dataset\n",
    "dataset = ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# Define the DataLoader for the dataset\n",
    "batch_size = 64\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_classes = 10  # Assuming you want to classify individual digits\n",
    "\n",
    "# Initialize the CNN model and optimizer\n",
    "model = CNN(num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - Loss: {loss.item()}')\n",
    "\n",
    "# Evaluation on the validation set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy}%')\n",
    "\n",
    "# Evaluation on the test set\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m      6\u001b[0m     _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 48\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[1;32m     49\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool(x)\n\u001b[1;32m     50\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Train Accuracy: {accuracy}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: Experiment with different learning rates, kernel sizes, and dropout rates to determine the optimal configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of hyperparameters to search over\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "kernel_sizes = [(3, 3), (5, 5)]\n",
    "dropout_rates = [0.0, 0.2, 0.5]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for kernel_size in kernel_sizes:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            # Define the model with the current hyperparameters\n",
    "            model = CNN(num_classes, kernel_size=kernel_size, dropout_rate=dropout_rate)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Training loop\n",
    "            num_epochs = 10\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                for images, labels in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Evaluation on the validation set\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for images, labels in val_loader:\n",
    "                    outputs = model(images)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                accuracy = 100 * correct / total\n",
    "            \n",
    "            print(f'Learning Rate: {lr}, Kernel Size: {kernel_size}, Dropout Rate: {dropout_rate}, Validation Accuracy: {accuracy}%')\n",
    "            \n",
    "            # Update the best hyperparameters if a better configuration is found\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_hyperparameters = {\n",
    "                    'learning_rate': lr,\n",
    "                    'kernel_size': kernel_size,\n",
    "                    'dropout_rate': dropout_rate\n",
    "                }\n",
    "\n",
    "print(f'Best Hyperparameters: {best_hyperparameters}, Best Validation Accuracy: {best_accuracy}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing on Single digit MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize for a single grayscale channel\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset = MNIST(root='./mnist_dataset', train=True, download=True, transform=mnist_transform)\n",
    "\n",
    "# Create a DataLoader for the MNIST dataset\n",
    "mnist_loader = DataLoader(mnist_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Assuming you have already trained the CNN model and it's stored in the 'model' variable\n",
    "model.eval()\n",
    "\n",
    "# Evaluation on the MNIST dataset\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in mnist_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy on MNIST Single-Digit Images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28) (10000, 28, 28) (12000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# To load the dataset, you can use np.load\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.load(\"permuted_mnist/train_images.npy\")\n",
    "y_train = np.load(\"permuted_mnist/train_labels.npy\")\n",
    "x_test = np.load(\"permuted_mnist/test_images.npy\")\n",
    "y_test = np.load(\"permuted_mnist/test_labels.npy\")\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "val_split = 0.2  # You can adjust the split ratio\n",
    "num_samples = len(x_train)\n",
    "split_idx = int(num_samples * val_split)\n",
    "\n",
    "x_val = x_train[:split_idx]\n",
    "y_val = y_train[:split_idx]\n",
    "\n",
    "x_train = x_train[split_idx:]\n",
    "y_train = y_train[split_idx:]\n",
    "\n",
    "print(np.shape(x_train),np.shape(x_test),np.shape(x_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to PyTorch tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float).clone().detach()\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).clone().detach()\n",
    "\n",
    "x_val = torch.tensor(x_val, dtype=torch.float).clone().detach()\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).clone().detach()\n",
    "\n",
    "x_test = torch.tensor(x_test, dtype=torch.float).clone().detach()\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).clone().detach()\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "# Create DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP on Permuted-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m \u001b[39m# Load your data (x_train, y_train, x_val, y_val, x_test, y_test)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m# Convert the data to PyTorch tensors\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m x_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x_train, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     12\u001b[0m y_train \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(y_train, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     14\u001b[0m x_val \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x_val, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = MLP()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 64.8324, Val Accuracy: 96.34%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the val set\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in val_loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Val Loss: {test_loss:.4f}, Val Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 94.7850, Train Accuracy: 98.10%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the train set\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Train Loss: {test_loss:.4f}, Train Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning: Adjust the number of hidden layers and the number of neurons within each layer to optimize performance and find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubantu/.local/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming x_train, y_train, x_test, and y_test are loaded and 3D\n",
    "# Flatten the input data\n",
    "x_train_flattened = x_train.reshape(x_train.shape[0], -1)\n",
    "x_test_flattened = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(128,), (128, 64), (64, 32, 16)],\n",
    "    'max_iter': [100, 200, 300],\n",
    "}\n",
    "\n",
    "# Create an MLP model\n",
    "mlp = MLPClassifier(solver='adam')\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(x_train_flattened, y_train)\n",
    "\n",
    "# Get the best model configuration\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Flatten the test data\n",
    "x_test_flattened = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "# Evaluate the best model on the test dataset\n",
    "test_accuracy = best_model.score(x_test_flattened, y_test)\n",
    "print(f'Best Model Test Accuracy: {test_accuracy:.2f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN on Permuted-MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 90.1840, Validation Accuracy: 92.47%\n",
      "Epoch 2/10, Validation Loss: 73.1474, Validation Accuracy: 93.94%\n",
      "Epoch 3/10, Validation Loss: 77.5608, Validation Accuracy: 93.85%\n",
      "Epoch 4/10, Validation Loss: 83.6219, Validation Accuracy: 93.69%\n",
      "Epoch 5/10, Validation Loss: 77.1314, Validation Accuracy: 94.77%\n",
      "Epoch 6/10, Validation Loss: 81.7339, Validation Accuracy: 94.27%\n",
      "Epoch 7/10, Validation Loss: 76.6616, Validation Accuracy: 95.03%\n",
      "Epoch 8/10, Validation Loss: 87.1500, Validation Accuracy: 94.65%\n",
      "Epoch 9/10, Validation Loss: 106.9055, Validation Accuracy: 94.14%\n",
      "Epoch 10/10, Validation Loss: 95.6192, Validation Accuracy: 95.18%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 79.8134, Test Accuracy: 94.84%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data (x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "\n",
    "# Create DataLoader for training and validation data\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "model = CNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x.unsqueeze(1))  # Add a channel dimension\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x.unsqueeze(1))  # Add a channel dimension\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs = model(batch_x.unsqueeze(1))  # Add a channel dimension\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 69.6466, Train Accuracy: 98.46%\n",
      "Validation Loss: 165.2657, Validation Accuracy: 95.18%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the Train set\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        outputs = model(batch_x.unsqueeze(1))  # Add a channel dimension\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Train Loss: {test_loss:.4f}, Train Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_x, batch_y in val_loader:\n",
    "        outputs = model(batch_x.unsqueeze(1))  # Add a channel dimension\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Validation Loss: {test_loss:.4f}, Validation Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning: Experiment with different learning rates, kernel sizes, and dropout rates to determine the optimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.001, 'kernel_size': 3, 'dropout_rate': 0.2}\n",
      "Best Validation Accuracy: 95.18333333333334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import itertools\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, learning_rate, kernel_size, dropout_rate):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define a function for model training and evaluation\n",
    "def train_and_evaluate(learning_rate, kernel_size, dropout_rate):\n",
    "    model = CNN(learning_rate, kernel_size, dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop (similar to previous code)\n",
    "    \n",
    "    # Return validation accuracy\n",
    "    return val_accuracy\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "kernel_sizes = [3, 5, 7]\n",
    "dropout_rates = [0.2, 0.5, 0.7]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Grid search\n",
    "for lr, kernel_size, dropout_rate in itertools.product(learning_rates, kernel_sizes, dropout_rates):\n",
    "    val_accuracy = train_and_evaluate(lr, kernel_size, dropout_rate)\n",
    "    \n",
    "    # Check if this configuration is better than the previous best\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_hyperparameters = {\n",
    "            'learning_rate': lr,\n",
    "            'kernel_size': kernel_size,\n",
    "            'dropout_rate': dropout_rate\n",
    "        }\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "print(\"Best Validation Accuracy:\", best_accuracy)\n",
    "\n",
    "# Train the final model with the best hyperparameters and evaluate on the test set\n",
    "final_model = CNN(best_hyperparameters['learning_rate'], best_hyperparameters['kernel_size'], best_hyperparameters['dropout_rate'])\n",
    "# Training loop for the final model\n",
    "# Evaluate on the test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
